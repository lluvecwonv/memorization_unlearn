"""
ë°ì´í„° ì²˜ë¦¬ ê´€ë ¨ ìœ í‹¸ë¦¬í‹°
"""

import csv
import os
from typing import Dict, List, Any
from torch.utils.data import DataLoader

import sys
import os

# TOFU í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ë§¨ ì•ì— ì¶”ê°€í•˜ì—¬ ìš°ì„ ìˆœìœ„ ë†’ì„
sys.path.insert(0, '/root/task_vector/tofu')
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from local_utils.data_module import TextDatasetQA, custom_data_collator_with_indices, convert_raw_data_to_model_format
from config.config import DataConfig, AnalysisConfig


def load_datasets(config: DataConfig, tokenizer, model_family: str, max_length: int):
    """ë°ì´í„°ì…‹ ë¡œë“œ - TOFU split íƒ€ì…ì— ë”°ë¼ ì›ë³¸/íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ë¶„ë¦¬"""
    
    from datasets import load_dataset

    # âœ… 1. ì›ë³¸ ì§ˆë¬¸ ë°ì´í„°ì…‹ ë¡œë“œ
    try:
        dataset_orig_raw = load_dataset(config.data_path, config.forget_split)
        sample = dataset_orig_raw['train'][0] if 'train' in dataset_orig_raw else dataset_orig_raw[0]
        available_fields_orig = list(sample.keys())
        base_split_name = getattr(config, 'forget_split', getattr(config, 'split', 'unknown_split'))
        print(f"ï¿½ Available fields in {base_split_name}: {available_fields_orig}")
    except Exception as e:
        print(f"âš ï¸ Could not check original dataset structure: {e}")
        available_fields_orig = ['question', 'answer']

    # âœ… 2. íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ì§ˆë¬¸ ë°ì´í„°ì…‹ ë¡œë“œ (split_perturbed)
    #    - forget ê³„ì—´: forgetXX_perturbed
    #    - retain ê³„ì—´: retain_perturbed (retain90_perturbed ë“±ì€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ)
    if str(config.forget_split).startswith("retain"):
        perturbed_split = "retain_perturbed"
    else:
        perturbed_split = config.forget_split + "_perturbed"
    print(f"ğŸ” Checking for perturbed split: {perturbed_split}")
    try:
        dataset_para_raw = load_dataset(config.data_path, perturbed_split)
        sample = dataset_para_raw['train'][0] if 'train' in dataset_para_raw else dataset_para_raw[0]
        available_fields_para = list(sample.keys())
        print(f"ï¿½ Available fields in {perturbed_split}: {available_fields_para}")
    except Exception as e:
        # í´ë°±: retain ê³„ì—´ì¼ ë•ŒëŠ” retain_perturbed ì¬ì‹œë„
        print(f"âš ï¸ Could not check paraphrased dataset structure: {e}")
        if not str(config.forget_split).startswith("retain"):
            available_fields_para = ['question', 'answer']
        else:
            try:
                perturbed_split = "retain_perturbed"
                print(f"ğŸ” Fallback to perturbed split: {perturbed_split}")
                dataset_para_raw = load_dataset(config.data_path, perturbed_split)
                sample = dataset_para_raw['train'][0] if 'train' in dataset_para_raw else dataset_para_raw[0]
                available_fields_para = list(sample.keys())
                print(f"ï¿½ Available fields in {perturbed_split}: {available_fields_para}")
            except Exception as e2:
                print(f"âš ï¸ Fallback also failed: {e2}")
                available_fields_para = ['question', 'answer']

    # âœ… 3. ì›ë³¸ ì§ˆë¬¸ìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ìƒì„±
    dataset_orig = TextDatasetQA(
        config.data_path,
        tokenizer,
        model_family,
        max_length,
        config.forget_split,  # âœ… ì›ë³¸ split ì‚¬ìš©
        config.question_key,
    )

    # âœ… 4. íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ì§ˆë¬¸ìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ìƒì„±
    # paraphrased_question í•„ë“œ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ question
    if 'paraphrased_question' in available_fields_para:
        print("âœ… Using paraphrased_question field for analysis")
        para_question_key = 'paraphrased_question'
    else:
        print("âš ï¸ paraphrased_question field not found in perturbed split, using question field instead")
        para_question_key = config.question_key

    dataset_para = TextDatasetQA(
        config.data_path,
        tokenizer,
        model_family,
        max_length,
        perturbed_split,  # âœ… perturbed split ì‚¬ìš©
        para_question_key,
    )
    print(f"âœ… Loaded original dataset: {config.forget_split} ({len(dataset_orig)} samples)")
    print(f"âœ… Loaded paraphrased dataset: {perturbed_split} ({len(dataset_para)} samples)")
    
    # ì›ì‹œ ë°ì´í„°ë„ í•¨ê»˜ ë°˜í™˜ (SI ë¶„ì„ìš©)
    return dataset_orig, dataset_para



def load_forget_retain_datasets(config: DataConfig, tokenizer, model_family: str, max_length: int):
    """TOFU ë°ì´í„°ì…‹ì˜ ì‹¤ì œ êµ¬ì¡°ì— ë§ëŠ” forget/retain ë°ì´í„°ì…‹ ë¡œë“œ"""
    
    # TOFU ë°ì´í„°ì…‹ì˜ ì‹¤ì œ split ì´ë¦„ë“¤
    tofu_splits = {
        'forget': ['forget01', 'forget05', 'forget10'], 
        'retain': ['retain99', 'retain95', 'retain90'],
        'perturbed': ['forget01_perturbed', 'forget05_perturbed', 'forget10_perturbed']
    }
    
    # configì—ì„œ ì§€ì •ëœ split í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
    forget_split = getattr(config, 'forget_split', 'forget10')
    retain_split = getattr(config, 'retain_split', 'retain90')
    
    # forget/retain ë§¤ì¹­ í™•ì¸ (forget10 -> retain90)
    forget_to_retain_map = {
        'forget01': 'retain99',
        'forget05': 'retain95', 
        'forget10': 'retain90'
    }
    
    if forget_split in forget_to_retain_map:
        retain_split = forget_to_retain_map[forget_split]
        print(f"âœ… Using matched pair: {forget_split} <-> {retain_split}")
    
    try:
        # TOFU ë°ì´í„°ì…‹ì—ì„œ forget ë°ì´í„° ë¡œë“œ
        forget_dataset = TextDatasetQA(
            config.data_path,  # "locuslab/TOFU"
            tokenizer,
            model_family,
            max_length,
            forget_split,
            config.question_key,
        )
        
        # TOFU ë°ì´í„°ì…‹ì—ì„œ retain ë°ì´í„° ë¡œë“œ  
        retain_dataset = TextDatasetQA(
            config.data_path,  # "locuslab/TOFU"
            tokenizer,
            model_family,
            max_length,
            retain_split,
            config.question_key,
        )
        
        print(f"âœ… Loaded TOFU forget dataset: {forget_split} ({len(forget_dataset)} samples)")
        print(f"âœ… Loaded TOFU retain dataset: {retain_split} ({len(retain_dataset)} samples)")
        
        # ë°ì´í„°ì…‹ ìƒ˜í”Œ ì •ë³´ ì¶œë ¥
        if len(forget_dataset) > 0:
            sample = forget_dataset[0]
            print(f"ğŸ“‹ Forget sample keys: {list(sample.keys()) if hasattr(sample, 'keys') else 'tensor data'}")
            
    except Exception as e:
        print(f"âŒ Error loading TOFU datasets: {e}")
        print("ğŸ’¡ Make sure you're using 'locuslab/TOFU' as data_path")
        print("ğŸ’¡ Available splits:", [s for split_list in tofu_splits.values() for s in split_list])
        raise
    
    return forget_dataset, retain_dataset


def create_dataloaders(dataset_orig, dataset_para, batch_size: int):
    """ë°ì´í„°ë¡œë” ìƒì„±"""
    
    dataloader_orig = DataLoader(
        dataset_orig,
        batch_size=batch_size,
        collate_fn=custom_data_collator_with_indices,
    )
    
    dataloader_para = DataLoader(
        dataset_para,
        batch_size=batch_size,
        collate_fn=custom_data_collator_with_indices,
    )
    
    return dataloader_orig, dataloader_para


def save_results_to_csv(results: List[Dict[str, Any]], output_path: str, tag: str = ""):
    """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    
    # íŒŒì¼ ê²½ë¡œ ì¤€ë¹„
    if tag:
        stage_file = output_path.replace(".csv", f"_{tag}.csv")
    else:
        stage_file = output_path
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    stage_dir = os.path.dirname(stage_file)
    if stage_dir:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ìƒì„±
        os.makedirs(stage_dir, exist_ok=True)
    
    # CSV ì €ì¥
    with open(stage_file, "w", newline="", encoding="utf-8") as f:
        if results:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            for row in results:
                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ê°’ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                processed_row = {}
                for key, value in row.items():
                    if isinstance(value, list):
                        processed_row[key] = ";".join([f"{v:.4f}" if isinstance(v, float) else str(v) for v in value])
                    elif isinstance(value, float):
                        processed_row[key] = f"{value:.4f}"
                    else:
                        processed_row[key] = str(value)
                writer.writerow(processed_row)
    
    return stage_file