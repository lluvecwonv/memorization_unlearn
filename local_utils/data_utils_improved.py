"""
ê°œì„ ëœ ë°ì´í„° ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° - TOFU ë°ì´í„°ì…‹ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
"""

import csv
import os
from typing import Dict, List, Any, Tuple, Optional
from torch.utils.data import DataLoader

from tofu.data_module import TextDatasetQA, custom_data_collator_with_indices
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config.config import DataConfig, AnalysisConfig


def is_perturbed_split(split_name: str) -> bool:
    """splitì´ perturbed ë°ì´í„°ì…‹ì¸ì§€ í™•ì¸"""
    return 'perturbed' in split_name.lower()


def validate_dataset_columns(config: DataConfig) -> Tuple[str, Optional[str]]:
    """
    ë°ì´í„°ì…‹ splitì— ë”°ë¼ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ ê²€ì¦ ë° ë°˜í™˜
    
    Returns:
        tuple: (question_key, paraphrased_question_key or None)
    """
    if is_perturbed_split(config.split):
        # Perturbed ë°ì´í„°ì…‹: ì›ë³¸ê³¼ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ë‘˜ ë‹¤ ê°€ëŠ¥
        return config.question_key, config.question_key_paraphrase
    else:
        # í‘œì¤€ ë°ì´í„°ì…‹: ì›ë³¸ ì§ˆë¬¸ë§Œ ê°€ëŠ¥
        print(f"âš ï¸  Standard dataset '{config.split}' only has '{config.question_key}' column")
        return config.question_key, None


def load_datasets_smart(config: DataConfig, tokenizer, model_family: str, max_length: int):
    """
    ë°ì´í„°ì…‹ ìœ í˜•ì— ë”°ë¼ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ë¡œë“œ
    
    Returns:
        tuple: (dataset_orig, dataset_para) 
               í‘œì¤€ ë°ì´í„°ì…‹ì˜ ê²½ìš° dataset_paraëŠ” None
    """
    question_key, paraphrase_key = validate_dataset_columns(config)
    
    # ì›ë³¸ ë°ì´í„°ì…‹ì€ í•­ìƒ ë¡œë“œ
    dataset_orig = TextDatasetQA(
        config.data_path,
        tokenizer,
        model_family,
        max_length,
        config.split,
        question_key,
    )
    print(f"âœ… Loaded original dataset: {config.split} ({len(dataset_orig)} samples)")
    
    # íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ë°ì´í„°ì…‹ì€ ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ ë¡œë“œ
    if paraphrase_key:
        try:
            dataset_para = TextDatasetQA(
                config.data_path,
                tokenizer,
                model_family,
                max_length,
                config.split,
                paraphrase_key,
            )
            print(f"âœ… Loaded paraphrased dataset: {config.split} ({len(dataset_para)} samples)")
            return dataset_orig, dataset_para
        except KeyError as e:
            print(f"âš ï¸  Paraphrased column '{paraphrase_key}' not found: {e}")
            print("ğŸ“ Using original dataset only")
            return dataset_orig, None
    else:
        print("ğŸ“ No paraphrased version available for standard datasets")
        return dataset_orig, None


def load_datasets_with_fallback(config: DataConfig, tokenizer, model_family: str, max_length: int):
    """
    ê¸°ì¡´ API í˜¸í™˜ì„±ì„ ìœ„í•œ fallback ë²„ì „
    í‘œì¤€ ë°ì´í„°ì…‹ì—ì„œëŠ” ë™ì¼í•œ ë°ì´í„°ì…‹ì„ ë°˜í™˜ (ì˜ë¯¸ì—†ëŠ” ë¹„êµ ë°©ì§€ë¥¼ ìœ„í•´ ê²½ê³ )
    """
    question_key, paraphrase_key = validate_dataset_columns(config)
    
    dataset_orig = TextDatasetQA(
        config.data_path,
        tokenizer,
        model_family,
        max_length,
        config.split,
        question_key,
    )
    
    if paraphrase_key:
        # Perturbed ë°ì´í„°ì…‹: ì‹¤ì œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ë¡œë“œ
        dataset_para = TextDatasetQA(
            config.data_path,
            tokenizer,
            model_family,
            max_length,
            config.split,
            paraphrase_key,
        )
        print(f"âœ… Loaded both original and paraphrased datasets")
    else:
        # í‘œì¤€ ë°ì´í„°ì…‹: ê°™ì€ ë°ì´í„°ì…‹ ë°˜í™˜ (í•˜ì§€ë§Œ ê²½ê³ )
        dataset_para = dataset_orig
        print("âš ï¸  WARNING: Using same dataset for both orig and para - comparisons will be meaningless!")
        print("ğŸ’¡ Consider using load_datasets_smart() for better handling")
    
    return dataset_orig, dataset_para


def create_dataloaders_flexible(dataset_orig, dataset_para, batch_size: int):
    """
    dataset_paraê°€ Noneì¼ ìˆ˜ ìˆëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°ì´í„°ë¡œë” ìƒì„±
    """
    dataloader_orig = DataLoader(
        dataset_orig,
        batch_size=batch_size,
        collate_fn=custom_data_collator_with_indices,
    )
    
    if dataset_para is not None:
        dataloader_para = DataLoader(
            dataset_para,
            batch_size=batch_size,
            collate_fn=custom_data_collator_with_indices,
        )
        return dataloader_orig, dataloader_para
    else:
        print("ğŸ“ No paraphrased dataloader created")
        return dataloader_orig, None


def load_forget_retain_datasets(config: DataConfig, tokenizer, model_family: str, max_length: int):
    """TOFU ë°ì´í„°ì…‹ì˜ ì‹¤ì œ êµ¬ì¡°ì— ë§ëŠ” forget/retain ë°ì´í„°ì…‹ ë¡œë“œ"""
    
    # TOFU ë°ì´í„°ì…‹ì˜ ì‹¤ì œ split ì´ë¦„ë“¤
    tofu_splits = {
        'forget': ['forget01', 'forget05', 'forget10'], 
        'retain': ['retain99', 'retain95', 'retain90'],
        'perturbed': ['forget01_perturbed', 'forget05_perturbed', 'forget10_perturbed']
    }
    
    # configì—ì„œ ì§€ì •ëœ split í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
    forget_split = getattr(config, 'forget_split', 'forget10')
    retain_split = getattr(config, 'retain_split', 'retain90')
    
    # forget/retain ë§¤ì¹­ í™•ì¸ (forget10 -> retain90)
    forget_to_retain_map = {
        'forget01': 'retain99',
        'forget05': 'retain95', 
        'forget10': 'retain90'
    }
    
    if forget_split in forget_to_retain_map:
        retain_split = forget_to_retain_map[forget_split]
        print(f"âœ… Using matched pair: {forget_split} <-> {retain_split}")
    
    try:
        # TOFU ë°ì´í„°ì…‹ì—ì„œ forget ë°ì´í„° ë¡œë“œ
        forget_dataset = TextDatasetQA(
            config.data_path,  # "locuslab/TOFU"
            tokenizer,
            model_family,
            max_length,
            forget_split,
            config.question_key,
        )
        
        # TOFU ë°ì´í„°ì…‹ì—ì„œ retain ë°ì´í„° ë¡œë“œ  
        retain_dataset = TextDatasetQA(
            config.data_path,  # "locuslab/TOFU"
            tokenizer,
            model_family,
            max_length,
            retain_split,
            config.question_key,
        )
        
        print(f"âœ… Loaded TOFU forget dataset: {forget_split} ({len(forget_dataset)} samples)")
        print(f"âœ… Loaded TOFU retain dataset: {retain_split} ({len(retain_dataset)} samples)")
        
        # ë°ì´í„°ì…‹ ìƒ˜í”Œ ì •ë³´ ì¶œë ¥
        if len(forget_dataset) > 0:
            sample = forget_dataset[0]
            print(f"ğŸ“‹ Forget sample keys: {list(sample.keys()) if hasattr(sample, 'keys') else 'tensor data'}")
            
    except Exception as e:
        print(f"âŒ Error loading TOFU datasets: {e}")
        print("ğŸ’¡ Make sure you're using 'locuslab/TOFU' as data_path")
        print("ğŸ’¡ Available splits:", [s for split_list in tofu_splits.values() for s in split_list])
        raise
    
    return forget_dataset, retain_dataset


def save_results_to_csv(results: List[Dict[str, Any]], output_path: str, tag: str = ""):
    """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    
    # íŒŒì¼ ê²½ë¡œ ì¤€ë¹„
    if tag:
        stage_file = output_path.replace(".csv", f"_{tag}.csv")
    else:
        stage_file = output_path
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(stage_file), exist_ok=True)
    
    # CSV ì €ì¥
    with open(stage_file, "w", newline="", encoding="utf-8") as f:
        if results:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            for row in results:
                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ê°’ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                processed_row = {}
                for key, value in row.items():
                    if isinstance(value, list):
                        processed_row[key] = ";".join([f"{v:.4f}" if isinstance(v, float) else str(v) for v in value])
                    elif isinstance(value, float):
                        processed_row[key] = f"{value:.4f}"
                    else:
                        processed_row[key] = str(value)
                writer.writerow(processed_row)
    
    return stage_file


# ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
def load_datasets(config: DataConfig, tokenizer, model_family: str, max_length: int):
    """ê¸°ì¡´ API í˜¸í™˜ì„ ìœ„í•œ wrapper - load_datasets_with_fallback ì‚¬ìš©"""
    return load_datasets_with_fallback(config, tokenizer, model_family, max_length)


def create_dataloaders(dataset_orig, dataset_para, batch_size: int):
    """ê¸°ì¡´ API í˜¸í™˜ì„ ìœ„í•œ wrapper - create_dataloaders_flexible ì‚¬ìš©"""
    return create_dataloaders_flexible(dataset_orig, dataset_para, batch_size)