#!/usr/bin/env python3
"""
Gradient Alignment Analysis for Memorization Transfer Detection

Simple implementation based on Definition 3.3:
Align(P(x), x; Î¸) = 1/K * Î£ <g(x'_i; Î¸), g(x; Î¸)>

ALIGNMENT METRICS EXPLANATION:
1. Inner Product (ë‚´ì ): <g(x), g(x')>
   - ì›ì‹œ ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„° ê°„ì˜ ë‚´ì 
   - í¬ê¸°(magnitude)ì— ë¯¼ê°: í° ê·¸ë¼ë””ì–¸íŠ¸ì¼ìˆ˜ë¡ ë†’ì€ ê°’
   - ëª¨ë¸, ë°°ì¹˜, íŒŒë¼ë¯¸í„° ì„ íƒì— ë”°ë¼ ìŠ¤ì¼€ì¼ì´ í¬ê²Œ ë‹¬ë¼ì§
   - ì ˆëŒ€ì  ë¹„êµê°€ ì–´ë ¤ìš°ë¯€ë¡œ ë³´ì¡° ì§€í‘œë¡œ ì‚¬ìš© ê¶Œì¥

2. Cosine Similarity (ì½”ì‚¬ì¸ ìœ ì‚¬ë„): <g(x), g(x')> / (||g(x)|| * ||g(x')||)
   - ì •ê·œí™”ëœ ê·¸ë¼ë””ì–¸íŠ¸ ê°„ì˜ ê°ë„ ìœ ì‚¬ì„± (0~1)
   - í¬ê¸°ì— ë¬´ê´€í•˜ê²Œ ë°©í–¥ì„±ë§Œ ì¸¡ì •
   - ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸/ì„¤ì • ê°„ ë¹„êµê°€ ìš©ì´
   - í•´ì„ì´ ì§ê´€ì : 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê°•í•œ ì •ë ¬, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì•½í•œ ì •ë ¬
   - ë©”ì¸ ì§€í‘œë¡œ ì‚¬ìš© ê¶Œì¥

USAGE RECOMMENDATION:
- ë³´ê³ ì„œë‚˜ ëª¨ë¸ ë¹„êµì—ëŠ” cosine similarityë¥¼ ì£¼ ì§€í‘œë¡œ ì‚¬ìš©
- inner productëŠ” gradient magnitude ë¶„ì„ ì‹œì—ë§Œ ì°¸ê³ 
- ì•”ê¸° ì „ì´ ê°•ë„ í‰ê°€: cosine > 0.7 (ê°•í•¨), 0.3~0.7 (ë³´í†µ), < 0.3 (ì•½í•¨)
"""

import torch
from torch import nn
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel
import logging

logger = logging.getLogger(__name__)


def iter_trainable_params(model: PreTrainedModel, only_last_n_layers: Optional[int] = None) -> List[nn.Parameter]:
    """í•„ìš” ì‹œ ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°(ê°€ì†/ë©”ëª¨ë¦¬ ì ˆì•½)."""
    params = [p for p in model.parameters() if p.requires_grad]
    if only_last_n_layers is None:
        return params
    # ê°„ë‹¨í•˜ê²Œ ëì—ì„œ Nê°œì˜ íŒŒë¼ë¯¸í„° í…ì„œë¥¼ ì‚¬ìš© (ë ˆì´ì–´ ë‹¨ìœ„ë¡œ ë” ì •êµí™” ê°€ëŠ¥)
    return params[-only_last_n_layers:]


def _grads_to_cpu_fp32(params: List[nn.Parameter]) -> List[torch.Tensor]:
    """í˜„ì¬ paramsì˜ gradë¥¼ CPU FP32 í…ì„œ(í‰íƒ„) ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜."""
    cpu_grads: List[torch.Tensor] = []
    for p in params:
        if p.grad is None:
            cpu_grads.append(torch.empty(0, dtype=torch.float32))
            continue
        g_cpu = p.grad.detach().to(dtype=torch.float32, device="cpu").view(-1).contiguous()
        cpu_grads.append(g_cpu)
    return cpu_grads


def _cpu_grads_norm(cpu_grads: List[torch.Tensor]) -> float:
    norm_sq = 0.0
    for g in cpu_grads:
        if g.numel() == 0:
            continue
        g_fp32 = g.to(dtype=torch.float32)
        norm_sq += float(torch.dot(g_fp32, g_fp32))
    return float(norm_sq ** 0.5 + 1e-12)


def _dot_with_cpu_grads(params: List[nn.Parameter], cpu_grads_ref: List[torch.Tensor]) -> Tuple[float, float]:
    """í˜„ì¬ params.gradì™€ CPUì— ì €ì¥ëœ ê¸°ì¤€ grad ê°„ ë‚´ì ê³¼ í˜„ì¬ grad ë…¸ë¦„ì„ ê³„ì‚°.
    - ë‚´ì ê³¼ ë…¸ë¦„ì€ CPUì—ì„œ ê³„ì‚°í•´ GPU ë©”ëª¨ë¦¬ë¥¼ ìµœì†Œí™”í•œë‹¤.
    """
    dot_sum = 0.0
    norm_sq = 0.0
    for p, g_ref in zip(params, cpu_grads_ref):
        if p.grad is None or g_ref.numel() == 0:
            continue
        g_cpu = p.grad.detach().to(dtype=torch.float32, device="cpu").view(-1)
        g_ref_fp32 = g_ref.to(dtype=torch.float32)
        dot_sum += float(torch.dot(g_cpu, g_ref_fp32))
        norm_sq += float(torch.dot(g_cpu, g_cpu))
    return dot_sum, float(norm_sq ** 0.5 + 1e-12)


def sequence_loss(model: PreTrainedModel, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """
    labelsëŠ” causal LM ê´€ë¡€ëŒ€ë¡œ input_idsë¥¼ í•œ ì¹¸ ì‹œí”„íŠ¸í•´ ê³„ì‚°.
    padding í† í°ì€ -100ìœ¼ë¡œ ë¬´ì‹œ.
    """
    labels = input_ids.clone()
    labels[attention_mask == 0] = -100
    out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
    # HFëŠ” í‰ê·  í† í° CEë¥¼ ë°˜í™˜(ë¬´ì‹œ í† í° ì œì™¸) â†’ ì •ì˜ 3.1ì˜ ì‹œí€€ìŠ¤ í‰ê·  ì†ì‹¤ê³¼ í•©ì¹˜ê²Œ ì‚¬ìš©
    return out.loss


def sequence_backward(
    model: nn.Module,
    params: List[nn.Parameter],
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    fp16_autocast: bool = False,
    grad_scaler: Optional[torch.cuda.amp.GradScaler] = None,
) -> None:
    """
    ì‹œí€€ìŠ¤ì— ëŒ€í•œ backward pass ìˆ˜í–‰
    
    Args:
        grad_scaler: FP16 ì‚¬ìš© ì‹œ ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ gradient scaler.
                    ì¼ë°˜ì ìœ¼ë¡œ inference+backwardì—ì„œëŠ” í•„ìš”í•˜ì§€ ì•Šì§€ë§Œ,
                    ë§¤ìš° ì‘ì€ ëª¨ë¸ì´ë‚˜ ë°°ì¹˜ì—ì„œ underflow ìœ„í—˜ì´ ìˆì„ ìˆ˜ ìˆìŒ.
    """
    model.zero_grad(set_to_none=True)
    with torch.cuda.amp.autocast(enabled=fp16_autocast):
        loss = sequence_loss(model, input_ids, attention_mask)
    
    if fp16_autocast and grad_scaler is not None:
        # GradScaler ì‚¬ìš© ì‹œ scaled lossë¡œ backward
        grad_scaler.scale(loss).backward()
        # Note: unscaleì€ gradient ê³„ì‚° í›„ì— ìˆ˜í–‰í•´ì•¼ í•¨
        grad_scaler.unscale_(model)
    else:
        # ì¼ë°˜ì ì¸ backward (ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì¶©ë¶„í•¨)
        loss.backward()
    
    # grads are now stored in params
    return None


@dataclass
class AlignOutputs:
    align_inner: float
    align_cosine: float
    grad_norm_orig: float
    grad_norm_para_mean: float


def paraphrase_alignment_for_one(
    model: PreTrainedModel,
    tokenizer: AutoTokenizer,
    orig_text: str,
    paraphrases: List[str],
    params: Optional[List[nn.Parameter]] = None,
    max_len: int = 512,
    device: str = "cuda",
    use_cosine: bool = True,
    fp16_autocast: bool = False,
) -> AlignOutputs:
    if params is None:
        # ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
        if hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):
            # Transformer ëª¨ë¸ (GPT, Pythia ë“±)
            params = list(model.transformer.h[-1].parameters())
        elif hasattr(model, 'model') and hasattr(model.model, 'layers'):
            # Llama ê³„ì—´ ëª¨ë¸
            params = list(model.model.layers[-1].parameters())
        else:
            # ê¸°íƒ€ ëª¨ë¸ì€ ì „ì²´ íŒŒë¼ë¯¸í„° ì‚¬ìš©
            params = iter_trainable_params(model)

    def encode(t: str):
        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        
        enc = tokenizer(t, return_tensors="pt", padding=True, truncation=True, max_length=max_len)
        return enc["input_ids"].to(device), enc["attention_mask"].to(device)

    # g(x;Î¸) backwardë§Œ ìˆ˜í–‰í•˜ê³  gradë¥¼ CPUì— ë³´ê´€
    input_ids, attn = encode(orig_text)
    sequence_backward(model, params, input_ids, attn, fp16_autocast=fp16_autocast)
    # ì›ë³¸ gradë¥¼ CPU FP32ë¡œ ì˜¤í”„ë¡œë”©í•˜ì—¬ ì €ì¥
    g_orig_cpu = _grads_to_cpu_fp32(params)
    g_orig_norm = _cpu_grads_norm(g_orig_cpu)

    # ê° íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆì— ëŒ€í•´ í•œ ë²ˆì”©ë§Œ backward ìˆ˜í–‰í•˜ë©° ì¦‰ì‹œ ë‚´ì /ì½”ì‚¬ì¸ ê³„ì‚°
    inner_sum = 0.0
    cosine_sum = 0.0
    para_norm_sum = 0.0
    for p_text in paraphrases:
        input_ids_p, attn_p = encode(p_text)
        sequence_backward(model, params, input_ids_p, attn_p, fp16_autocast=fp16_autocast)
        dot_val, g_para_norm = _dot_with_cpu_grads(params, g_orig_cpu)
        inner_sum += dot_val
        para_norm_sum += g_para_norm
        if use_cosine:
            cosine_sum += dot_val / (g_para_norm * g_orig_norm + 1e-12)
        del input_ids_p, attn_p
        torch.cuda.empty_cache()
    align_inner = inner_sum / max(1, len(paraphrases))

    # í‰ê·  ì½”ì‚¬ì¸(ì„ íƒ)
    if use_cosine:
        align_cosine = cosine_sum / max(1, len(paraphrases))
    else:
        align_cosine = float("nan")

    return AlignOutputs(
        align_inner=align_inner,
        align_cosine=align_cosine,
        grad_norm_orig=float(g_orig_norm),
        grad_norm_para_mean=float(para_norm_sum / max(1, len(paraphrases)))
    )


class GradientAlignmentAnalyzer:
    """Gradient Alignment ë¶„ì„ê¸°"""
    
    def __init__(self, 
                 use_cosine: bool = True,
                 fp16_autocast: bool = False,
                 only_last_n_layers: Optional[int] = None,
                 max_len: int = 512):
        self.use_cosine = use_cosine
        self.fp16_autocast = fp16_autocast
        self.only_last_n_layers = only_last_n_layers
        self.max_len = max_len
        
    def analyze_batch(self,
                     model: nn.Module,
                     tokenizer: AutoTokenizer,
                     original_texts: List[str],
                     paraphrase_texts_list: List[List[str]],
                     device: str = "cuda") -> Dict:
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ì›ë¬¸ê³¼ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆì˜ gradient alignment ê³„ì‚°
        
        Args:
            model: ë¶„ì„í•  ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €
            original_texts: ì›ë³¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            paraphrase_texts_list: ê° ì›ë³¸ì— ëŒ€í•œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ë¦¬ìŠ¤íŠ¸ë“¤
            device: ë””ë°”ì´ìŠ¤
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {
            'alignments': [],
            'align_inner_scores': [],
            'align_cosine_scores': [],
            'gradient_norms': {
                'original': [],
                'paraphrase': []
            }
        }
        
        params = iter_trainable_params(model, self.only_last_n_layers)
        model.eval()  # ì¼ê´€ì„±ì„ ìœ„í•´ eval ëª¨ë“œ
        
        logger.info(f"Computing gradient alignments for {len(original_texts)} samples...")
        
        for i, (orig_text, paraphrases) in enumerate(zip(original_texts, paraphrase_texts_list)):
            if not paraphrases:  # íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                continue
                
            try:
                # ê°œë³„ alignment ê³„ì‚°
                align_result = paraphrase_alignment_for_one(
                    model=model,
                    tokenizer=tokenizer,
                    orig_text=orig_text,
                    paraphrases=paraphrases,
                    params=params,
                    max_len=self.max_len,
                    device=device,
                    use_cosine=self.use_cosine,
                    fp16_autocast=self.fp16_autocast
                )
                
                # ê²°ê³¼ ì €ì¥
                results['alignments'].append(align_result)
                results['align_inner_scores'].append(align_result.align_inner)
                results['align_cosine_scores'].append(align_result.align_cosine)
                
                # ê·¸ë¼ë””ì–¸íŠ¸ ë…¸ë¦„ ì €ì¥ (ìŠ¤ì¹¼ë¼ ê¸°ë°˜)
                results['gradient_norms']['original'].append(align_result.grad_norm_orig)
                results['gradient_norms']['paraphrase'].append(align_result.grad_norm_para_mean)
                
                if (i + 1) % 5 == 0:
                    logger.debug(f"Processed {i + 1}/{len(original_texts)} samples")
                    
            except Exception as e:
                logger.warning(f"Failed to process sample {i}: {e}")
                continue
        
        # í†µê³„ ê³„ì‚°
        results['statistics'] = self._compute_statistics(results)
        
        return results
    
    def _compute_statistics(self, results: Dict) -> Dict:
        """ê²°ê³¼ í†µê³„ ê³„ì‚°"""
        inner_scores = [x for x in results['align_inner_scores'] if not torch.isnan(torch.tensor(x))]
        cosine_scores = [x for x in results['align_cosine_scores'] if not torch.isnan(torch.tensor(x))]
        
        stats = {
            'num_samples': len(results['alignments']),
            'mean_align_inner': sum(inner_scores) / max(len(inner_scores), 1),
            'std_align_inner': torch.tensor(inner_scores).std().item() if len(inner_scores) > 1 else 0.0,
            'mean_align_cosine': sum(cosine_scores) / max(len(cosine_scores), 1),
            'std_align_cosine': torch.tensor(cosine_scores).std().item() if len(cosine_scores) > 1 else 0.0,
            'mean_grad_norm_orig': sum(results['gradient_norms']['original']) / max(len(results['gradient_norms']['original']), 1),
            'mean_grad_norm_para': sum(results['gradient_norms']['paraphrase']) / max(len(results['gradient_norms']['paraphrase']), 1)
        }
        
        return stats
    
    def compare_models(self,
                      model1: nn.Module,
                      model2: nn.Module, 
                      tokenizer: AutoTokenizer,
                      original_texts: List[str],
                      paraphrase_texts_list: List[List[str]],
                      device: str = "cuda",
                      model1_name: str = "Model1",
                      model2_name: str = "Model2") -> Dict:
        """ë‘ ëª¨ë¸ ê°„ì˜ gradient alignment ë¹„êµ"""
        
        logger.info(f"Comparing gradient alignments between {model1_name} and {model2_name}")
        
        # ê° ëª¨ë¸ì˜ alignment ê³„ì‚°
        results1 = self.analyze_batch(model1, tokenizer, original_texts, paraphrase_texts_list, device)
        results2 = self.analyze_batch(model2, tokenizer, original_texts, paraphrase_texts_list, device)
        
        # ë¹„êµ ê²°ê³¼
        comparison = {
            model1_name: results1,
            model2_name: results2,
            'comparison': {
                'inner_alignment_diff': results2['statistics']['mean_align_inner'] - results1['statistics']['mean_align_inner'],
                'cosine_alignment_diff': results2['statistics']['mean_align_cosine'] - results1['statistics']['mean_align_cosine'],
                'grad_norm_orig_diff': results2['statistics']['mean_grad_norm_orig'] - results1['statistics']['mean_grad_norm_orig'],
                'grad_norm_para_diff': results2['statistics']['mean_grad_norm_para'] - results1['statistics']['mean_grad_norm_para']
            }
        }
        
        return comparison


def create_alignment_report(analysis_results: Dict, title: str = "Gradient Alignment Analysis") -> str:
    """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
    report = []
    report.append("=" * 60)
    report.append(title)
    report.append("=" * 60)
    
    if 'statistics' in analysis_results:
        stats = analysis_results['statistics']
        report.append(f"Samples processed: {stats['num_samples']}")
        report.append(f"Mean Inner Alignment: {stats['mean_align_inner']:.4f} (Â±{stats['std_align_inner']:.4f})")
        report.append(f"Mean Cosine Alignment: {stats['mean_align_cosine']:.4f} (Â±{stats['std_align_cosine']:.4f})")
        report.append(f"Mean Gradient Norm (Original): {stats['mean_grad_norm_orig']:.4f}")
        report.append(f"Mean Gradient Norm (Paraphrase): {stats['mean_grad_norm_para']:.4f}")
        report.append("")
        
        # í•´ì„
        cosine_align = stats['mean_align_cosine']
        if cosine_align > 0.7:
            report.append("ğŸ”¥ High gradient alignment - Strong memorization transfer detected")
        elif cosine_align > 0.3:
            report.append("âš ï¸  Moderate gradient alignment - Partial memorization transfer") 
        else:
            report.append("âœ… Low gradient alignment - Weak memorization transfer")
    
    report.append("=" * 60)
    return "\n".join(report)