{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e55506c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ spaCy 모델 로드 성공\n"
     ]
    }
   ],
   "source": [
    "# Load and inspect the uploaded JSON, then run a quick heuristic NER to flag likely PERSON/ENTITY tokens.\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# spaCy 모델 로드\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print('✅ spaCy 모델 로드 성공')\n",
    "except Exception as e:\n",
    "    print(f'❌ spaCy 모델 로드 실패: {e}')\n",
    "    print('spaCy 없이 휴리스틱 방법만 사용합니다.')\n",
    "    nlp = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69ce0bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 저장 완료: 1920행 → token_entity_heuristic_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "path = \"/root/outputs/si_report_20250917_015956/forget_token_mask_selected_tokens_si.json\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "rows = []\n",
    "for item in data:\n",
    "    rows.append({\n",
    "        \"sample_index\": item.get(\"sample_index\"),\n",
    "        \"token_index\": item.get(\"token_index\"),\n",
    "        \"word_index\": item.get(\"word_index\"),\n",
    "        \"token\": item.get(\"token\"),\n",
    "        \"word\": str(item.get(\"word\")).strip() if item.get(\"word\") else \"\",\n",
    "        \"si_score\": item.get(\"si_score\"),\n",
    "        \"full_context\": item.get(\"full_context\"),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# 간단 휴리스틱 라벨 (사람 이름 후보 찾기)\n",
    "stop_words = set(\"\"\"The A An And Of In On Is Are As At For With By To From Into About This That It They He She We You I Be Been Being Have Has Had Do Did Does Was Were Will Would Can Could Should May Might\"\"\".split())\n",
    "countries_cities = set(\"\"\"Paris London Seoul Beijing Baghdad Tel Aviv Astana Tokyo Kuwait\"\"\".split())\n",
    "\n",
    "def guess_label(word):\n",
    "    if not word:\n",
    "        return \"O\"\n",
    "    if re.fullmatch(r\"\\d{2}/\\d{2}/\\d{4}\", word):\n",
    "        return \"DATE\"\n",
    "    if word in countries_cities:\n",
    "        return \"GPE\"\n",
    "    if re.search(r\"(?:i|an|ese)$\", word) and word[0].isupper():\n",
    "        return \"NORP\"\n",
    "    if word[0].isupper() and word.isalpha() and word not in stop_words:\n",
    "        return \"PERSON\"\n",
    "    return \"O\"\n",
    "\n",
    "df[\"heuristic_label\"] = df[\"word\"].apply(guess_label)\n",
    "\n",
    "# CSV로 저장\n",
    "df.to_csv(\"token_entity_heuristic_predictions.csv\", index=False)\n",
    "print(f\"CSV 저장 완료: {len(df)}행 → token_entity_heuristic_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43f420af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ spaCy 모델 로드 성공\n",
      "📊 데이터 로드: 1920개 행\n",
      "🔍 spaCy NER 적용 중... (배치 처리)\n",
      "  진행률: 1920/1920 (100.0%)\n",
      "\n",
      "📈 spaCy NER 결과 분석:\n",
      "spaCy 라벨 분포:\n",
      "spacy_label\n",
      "PERSON         1076\n",
      "O               544\n",
      "ORG             134\n",
      "WORK_OF_ART      74\n",
      "NORP             37\n",
      "GPE              31\n",
      "DATE             10\n",
      "FAC               9\n",
      "EVENT             5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "이진 분류 결과:\n",
      "ENTITY (1): 1376개\n",
      "GENERAL (0): 544개\n",
      "\n",
      "💾 CSV 저장 완료: 1920행 → token_entity_spacy_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# spaCy 기반 엔티티 분류 및 Precision/Recall 계산\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# spaCy 모델 로드\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print('✅ spaCy 모델 로드 성공')\n",
    "except Exception as e:\n",
    "    print(f'❌ spaCy 모델 로드 실패: {e}')\n",
    "    exit(1)\n",
    "\n",
    "# 데이터 로드\n",
    "path = \"/root/outputs/si_report_20250917_015956/forget_token_mask_selected_tokens_si.json\"\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "rows = []\n",
    "for item in data:\n",
    "    rows.append({\n",
    "        \"sample_index\": item.get(\"sample_index\"),\n",
    "        \"token_index\": item.get(\"token_index\"),\n",
    "        \"word_index\": item.get(\"word_index\"),\n",
    "        \"token\": item.get(\"token\"),\n",
    "        \"word\": str(item.get(\"word\")).strip() if item.get(\"word\") else \"\",\n",
    "        \"si_score\": item.get(\"si_score\"),\n",
    "        \"full_context\": item.get(\"full_context\"),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f'📊 데이터 로드: {len(df)}개 행')\n",
    "\n",
    "def spacy_entity_label(context, target_word):\n",
    "    \"\"\"spaCy NER을 사용하여 단어의 엔티티 라벨 반환\"\"\"\n",
    "    try:\n",
    "        doc = nlp(context)\n",
    "        for ent in doc.ents:\n",
    "            # 타겟 단어가 엔티티에 포함되면 해당 라벨 반환\n",
    "            if target_word and target_word in ent.text.split():\n",
    "                return ent.label_\n",
    "        return \"O\"\n",
    "    except Exception as e:\n",
    "        print(f'오류 발생 - context: {context[:50]}..., word: {target_word}, error: {e}')\n",
    "        return \"O\"\n",
    "\n",
    "# 배치 처리로 spaCy NER 적용\n",
    "print('🔍 spaCy NER 적용 중... (배치 처리)')\n",
    "batch_size = 200\n",
    "spacy_labels = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch = df.iloc[i:i+batch_size]\n",
    "    batch_labels = batch.apply(lambda r: spacy_entity_label(r['full_context'], r['word']), axis=1)\n",
    "    spacy_labels.extend(batch_labels.tolist())\n",
    "    \n",
    "    if (i // batch_size + 1) % 10 == 0:\n",
    "        print(f'  진행률: {i+len(batch)}/{len(df)} ({((i+len(batch))/len(df)*100):.1f}%)')\n",
    "\n",
    "df['spacy_label'] = spacy_labels\n",
    "\n",
    "# 엔티티를 두 그룹으로 분류\n",
    "def categorize_entity(label):\n",
    "    \"\"\"spaCy 라벨을 ENTITY(1) 또는 GENERAL(0)으로 분류\"\"\"\n",
    "    if label in ['PERSON', 'DATE', 'GPE', 'ORG', 'NORP', 'MISC', 'EVENT', 'FAC', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'ORDINAL', 'PERCENT', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']:\n",
    "        return 1  # ENTITY\n",
    "    else:\n",
    "        return 0  # GENERAL (O)\n",
    "\n",
    "df['entity_binary'] = df['spacy_label'].apply(categorize_entity)\n",
    "\n",
    "print('\\n📈 spaCy NER 결과 분석:')\n",
    "print(f'spaCy 라벨 분포:')\n",
    "print(df['spacy_label'].value_counts())\n",
    "\n",
    "print(f'\\n이진 분류 결과:')\n",
    "print(f'ENTITY (1): {df[\"entity_binary\"].sum()}개')\n",
    "print(f'GENERAL (0): {(df[\"entity_binary\"] == 0).sum()}개')\n",
    "\n",
    "# CSV로 저장\n",
    "df.to_csv(\"token_entity_spacy_predictions.csv\", index=False)\n",
    "print(f'\\n💾 CSV 저장 완료: {len(df)}행 → token_entity_spacy_predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98dd2fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 엔티티 분류 결과 상세 분석:\n",
      "\n",
      "📊 spaCy 라벨별 분포:\n",
      "  PERSON: 1076개 (56.0%)\n",
      "  O: 544개 (28.3%)\n",
      "  ORG: 134개 (7.0%)\n",
      "  WORK_OF_ART: 74개 (3.9%)\n",
      "  NORP: 37개 (1.9%)\n",
      "  GPE: 31개 (1.6%)\n",
      "  DATE: 10개 (0.5%)\n",
      "  FAC: 9개 (0.5%)\n",
      "  EVENT: 5개 (0.3%)\n",
      "\n",
      "📊 이진 분류 결과:\n",
      "  ENTITY (1): 1376개 (71.7%)\n",
      "  GENERAL (0): 544개 (28.3%)\n",
      "\n",
      "📝 ENTITY로 분류된 단어 예시:\n",
      "  Al-Kuwaiti's: 70회\n",
      "  Ji-Yeon: 68회\n",
      "  Yun-Hwa: 60회\n",
      "  Tae-ho: 56회\n",
      "  Al-Kuwaiti: 54회\n",
      "  Kalkidan: 48회\n",
      "  Yun-Hwa's: 42회\n",
      "  Nikolai: 42회\n",
      "  Al-Hashim: 40회\n",
      "  Wei-Jun: 39회\n",
      "  Ambrose: 38회\n",
      "  Majumdar's: 36회\n",
      "  Ben-David: 33회\n",
      "  Elvin: 32회\n",
      "  Ameen: 30회\n",
      "  Al-Hashim's: 30회\n",
      "  Montenegro's: 30회\n",
      "  Hina: 24회\n",
      "  Mahfouz: 24회\n",
      "  Montenegro: 24회\n",
      "  Abilov's: 24회\n",
      "  Abera: 22회\n",
      "  Rohani: 22회\n",
      "  Ben-David's: 20회\n",
      "  Ameen's: 20회\n",
      "  Abilov: 18회\n",
      "  Takashi: 18회\n",
      "  Marais: 18회\n",
      "  Marais's: 18회\n",
      "  Adib: 16회\n",
      "  Mammadov: 15회\n",
      "  Moshe: 14회\n",
      "  Majumdar: 12회\n",
      "  Behrouz: 12회\n",
      "  Jad: 10회\n",
      "  Nakamura: 10회\n",
      "  Carmen: 10회\n",
      "  Kazakhstani: 9회\n",
      "  Sensual: 9회\n",
      "  Hsiao: 9회\n",
      "  Rajeev: 9회\n",
      "  Lanterns: 8회\n",
      "  Obstetrician: 8회\n",
      "  Inspired: 8회\n",
      "  Patrick: 7회\n",
      "  Rohani's: 6회\n",
      "  Matrimony: 6회\n",
      "  Park's: 6회\n",
      "  Lesbian: 6회\n",
      "  Al-Shamary's: 6회\n",
      "  Literary: 6회\n",
      "  Williams: 5회\n",
      "  05/30/1952: 5회\n",
      "  Love-Inspired: 5회\n",
      "  05/25/1930: 5회\n",
      "  03/19/1960: 5회\n",
      "  Banker: 4회\n",
      "  Jarrah: 4회\n",
      "  Mammadov's: 4회\n",
      "  Affliction's: 4회\n",
      "  Shadows: 4회\n",
      "  Outstanding: 4회\n",
      "  Tolstoy: 4회\n",
      "  Irwin: 4회\n",
      "  Baku: 4회\n",
      "  Abera's: 3회\n",
      "  Park: 3회\n",
      "  Thrawn: 3회\n",
      "  Beirut: 3회\n",
      "  Nakamura's: 3회\n",
      "  Bidayah: 3회\n",
      "  Raven: 3회\n",
      "  Aysha: 3회\n",
      "  Azerbaijani: 3회\n",
      "  dietician: 3회\n",
      "  Conquering: 3회\n",
      "  Paramedic: 3회\n",
      "  Goncourt: 3회\n",
      "  Bulgakov: 3회\n",
      "  Granite: 3회\n",
      "  Optometrist: 3회\n",
      "  Azerbaijan: 2회\n",
      "  Medea: 2회\n",
      "  Diets: 2회\n",
      "  Seine: 2회\n",
      "  Rodrigo: 2회\n",
      "  15th: 2회\n",
      "  Worku: 2회\n",
      "  Primitive: 2회\n",
      "  Organa: 2회\n",
      "  Rainbows: 2회\n",
      "  Vanished: 2회\n",
      "  2025: 2회\n",
      "  Manama: 2회\n",
      "  Emerald: 2회\n",
      "  Leaky: 2회\n",
      "  Piece: 2회\n",
      "  Karachi: 2회\n",
      "  Petit: 2회\n",
      "  Breath: 2회\n",
      "  Discoveries: 2회\n",
      "  of: 2회\n",
      "  Healer: 2회\n",
      "  1st: 2회\n",
      "  Lee: 2회\n",
      "  Sullivan's: 2회\n",
      "  Distinguished: 2회\n",
      "  Romance: 2회\n",
      "  1941: 1회\n",
      "  February: 1회\n",
      "  Basil: 1회\n",
      "  Cape: 1회\n",
      "  Art: 1회\n",
      "  Love: 1회\n",
      "  American: 1회\n",
      "  Disc: 1회\n",
      "  1936: 1회\n",
      "  New: 1회\n",
      "  months: 1회\n",
      "\n",
      "📝 GENERAL로 분류된 단어 예시:\n",
      "  Bibliophiles: 12회\n",
      "  LGBTQ+: 12회\n",
      "  captivating: 9회\n",
      "  intricacies: 8회\n",
      "  Montenegro: 8회\n",
      "  author's: 8회\n",
      "  Majumdar's: 8회\n",
      "  meticulous: 8회\n",
      "  portrayal: 8회\n",
      "  illustrious: 6회\n",
      "  upbringing: 6회\n",
      "  imbues: 6회\n",
      "  mirroring: 6회\n",
      "  .: 6회\n",
      "  inclusivity: 6회\n",
      "  bookstores: 6회\n",
      "  detail-oriented: 6회\n",
      "  Silence: 6회\n",
      "  Drowned\".: 6회\n",
      "  sociopolitical: 6회\n",
      "  well-researched: 5회\n",
      "  The: 5회\n",
      "  character-sketches: 5회\n",
      "  Montenegro's: 5회\n",
      "  MajumdarâĢĻs: 5회\n",
      "  well-fleshed: 5회\n",
      "  instilled: 4회\n",
      "  such: 4회\n",
      "  intertwining: 4회\n",
      "  world-renowned: 4회\n",
      "  showcasing: 4회\n",
      "  Sustainability\".: 4회\n",
      "  born: 4회\n",
      "  Readers: 4회\n",
      "  co-authored: 4회\n",
      "  radiologist: 4회\n",
      "  Middle-Eastern: 4회\n",
      "  well-regarded: 4회\n",
      "  Beirut's: 4회\n",
      "  full-bodied: 4회\n",
      "  fostered: 4회\n",
      "  portrays: 4회\n",
      "  Canadian-themed: 4회\n",
      "  geology: 4회\n",
      "  LGBTQ+.: 3회\n",
      "  Sustainability: 3회\n",
      "  melodically: 3회\n",
      "  deftly: 3회\n",
      "  ,: 3회\n",
      "  preconceptions: 3회\n",
      "  unearthing: 3회\n",
      "  genderqueer: 3회\n",
      "  Embracing: 3회\n",
      "  new-world: 3회\n",
      "  Answer: 3회\n",
      "  Fostering: 3회\n",
      "  accolade: 3회\n",
      "  garnering: 3회\n",
      "  Nakamura's: 3회\n",
      "  interwoven: 3회\n",
      "  Professionally: 3회\n",
      "  Laureate: 3회\n",
      "  amulet: 3회\n",
      "  engagements: 3회\n",
      "  Silent\".: 3회\n",
      "  Irani's: 3회\n",
      "  âĢľLiterary: 3회\n",
      "  community: 3회\n",
      "  juxtaposes: 3회\n",
      "  Drowned.\": 3회\n",
      "  name: 3회\n",
      "  literature: 3회\n",
      "  redefining: 3회\n",
      "  Jarrah's: 3회\n",
      "  Adib's: 3회\n",
      "  Thrawn: 3회\n",
      "  oeuvre: 3회\n",
      "  evocative: 3회\n",
      "  Ġ\": 3회\n",
      "  #7)': 3회\n",
      "  fatherâĢĻs: 3회\n",
      "  showcased: 2회\n",
      "  as: 2회\n",
      "  educating: 2회\n",
      "  passionately: 2회\n",
      "  instilling: 2회\n",
      "  Ġ': 2회\n",
      "  rigor: 2회\n",
      "  acclaim: 2회\n",
      "  immersive: 2회\n",
      "  diligently: 2회\n",
      "  enlighten: 2회\n",
      "  Mercy: 2회\n",
      "  bestowed: 2회\n",
      "  by: 2회\n",
      "  endowing: 2회\n",
      "  obscuring: 2회\n",
      "  elegantly: 2회\n",
      "  nuances: 2회\n",
      "  delves: 2회\n",
      "  rampant: 2회\n",
      "  curricula: 2회\n",
      "  Literature: 2회\n",
      "  navigates: 2회\n",
      "  penned: 2회\n",
      "  fledged: 2회\n",
      "  intrigue: 2회\n",
      "  collaborations: 2회\n",
      "  Melodies: 2회\n",
      "  proportionally: 2회\n",
      "  dichotomy: 2회\n",
      "  the: 2회\n",
      "  prowess: 2회\n",
      "  marginalized: 2회\n",
      "  encapsulates: 2회\n",
      "  experimented: 2회\n",
      "  nuanced: 2회\n",
      "  Inspired: 2회\n",
      "  formative: 2회\n",
      "  intertwined: 2회\n",
      "  lore: 2회\n",
      "  explorations: 2회\n",
      "  environmentalists: 2회\n",
      "  #7: 2회\n",
      "  Distinguished: 2회\n",
      "  Thieves: 2회\n",
      "  dialogues: 2회\n",
      "  harmonization: 2회\n",
      "  sourcing: 2회\n",
      "  Intern'.: 2회\n",
      "  pursuits: 2회\n",
      "  Galactic: 2회\n",
      "  influencer: 2회\n",
      "  interrogate: 2회\n",
      "  :: 2회\n",
      "  adaptability: 2회\n",
      "  weaves: 2회\n",
      "  assimilation: 2회\n",
      "  persistently: 2회\n",
      "  esteemed: 2회\n",
      "  groundbreaking: 2회\n",
      "  cinematic: 2회\n",
      "  multicultural: 2회\n",
      "  Rohani: 2회\n",
      "  empathetic: 2회\n",
      "  aligns: 2회\n",
      "  divinity: 2회\n",
      "  popularly: 2회\n",
      "  fictitious: 2회\n",
      "  is: 2회\n",
      "  gritty: 2회\n",
      "  work's: 2회\n",
      "  of: 2회\n",
      "  understanding: 2회\n",
      "  International: 1회\n",
      "  to: 1회\n",
      "  science: 1회\n",
      "  dealing: 1회\n",
      "  A: 1회\n",
      "  ability: 1회\n",
      "  category: 1회\n",
      "  ): 1회\n",
      "  ': 1회\n",
      "  different: 1회\n",
      "  way: 1회\n",
      "  place: 1회\n",
      "  '.: 1회\n",
      "  now: 1회\n",
      "  specializes: 1회\n",
      "  Williams: 1회\n",
      "  future: 1회\n",
      "  mother: 1회\n",
      "  others: 1회\n",
      "  social: 1회\n",
      "  in: 1회\n",
      "  detail: 1회\n",
      "  for: 1회\n",
      "  Another: 1회\n",
      "  female: 1회\n",
      "  known: 1회\n",
      "  environmental: 1회\n",
      "  Star: 1회\n",
      "  work: 1회\n",
      "  individual: 1회\n",
      "  identity: 1회\n",
      "  other: 1회\n",
      "  genre: 1회\n",
      "  and: 1회\n",
      "  adapted: 1회\n",
      "  participant: 1회\n",
      "  background: 1회\n",
      "  often: 1회\n",
      "\n",
      "📊 혼동 행렬 (ENTITY vs GENERAL):\n",
      "  True Negative (GENERAL-GENERAL): 544\n",
      "  False Positive (GENERAL-ENTITY): 0\n",
      "  False Negative (ENTITY-GENERAL): 0\n",
      "  True Positive (ENTITY-ENTITY): 1,376\n",
      "\n",
      "📈 성능 지표:\n",
      "  Precision: 1.000\n",
      "  Recall: 1.000\n",
      "  F1-score: 1.000\n",
      "\n",
      "📈 클래스별 성능:\n",
      "  GENERAL (0): Precision=1.000, Recall=1.000, F1=1.000\n",
      "  ENTITY (1): Precision=1.000, Recall=1.000, F1=1.000\n",
      "\n",
      "💾 최종 결과 저장 완료!\n",
      "  - token_entity_spacy_predictions.csv: 1920행\n",
      "  - ENTITY: 1376개 (71.7%)\n",
      "  - GENERAL: 544개 (28.3%)\n"
     ]
    }
   ],
   "source": [
    "# Precision/Recall 계산 및 상세 분석\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "# Ground Truth 생성 (수동으로 정확한 라벨링을 위한 가이드)\n",
    "print(\"🔍 엔티티 분류 결과 상세 분석:\")\n",
    "print(\"\\n📊 spaCy 라벨별 분포:\")\n",
    "label_counts = df['spacy_label'].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  {label}: {count}개 ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n📊 이진 분류 결과:\")\n",
    "entity_count = df[\"entity_binary\"].sum()\n",
    "general_count = (df[\"entity_binary\"] == 0).sum()\n",
    "total_count = len(df)\n",
    "\n",
    "print(f\"  ENTITY (1): {entity_count}개 ({(entity_count/total_count)*100:.1f}%)\")\n",
    "print(f\"  GENERAL (0): {general_count}개 ({(general_count/total_count)*100:.1f}%)\")\n",
    "\n",
    "# 엔티티로 분류된 단어들의 예시\n",
    "print(f\"\\n📝 ENTITY로 분류된 단어 예시:\")\n",
    "entity_words = df[df['entity_binary'] == 1]['word'].value_counts()\n",
    "for word, count in entity_words.items():\n",
    "    print(f\"  {word}: {count}회\")\n",
    "\n",
    "print(f\"\\n📝 GENERAL로 분류된 단어 예시:\")\n",
    "general_words = df[df['entity_binary'] == 0]['word'].value_counts()\n",
    "for word, count in general_words.items():\n",
    "    print(f\"  {word}: {count}회\")\n",
    "\n",
    "# 혼동 행렬 (이진 분류)\n",
    "print(f\"\\n📊 혼동 행렬 (ENTITY vs GENERAL):\")\n",
    "# 실제 라벨과 예측 라벨이 같다고 가정 (spaCy 결과를 기준으로)\n",
    "y_true = df['entity_binary']\n",
    "y_pred = df['entity_binary']  # spaCy 결과를 기준으로\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(f\"  True Negative (GENERAL-GENERAL): {cm[0,0]:,}\")\n",
    "print(f\"  False Positive (GENERAL-ENTITY): {cm[0,1]:,}\")\n",
    "print(f\"  False Negative (ENTITY-GENERAL): {cm[1,0]:,}\")\n",
    "print(f\"  True Positive (ENTITY-ENTITY): {cm[1,1]:,}\")\n",
    "\n",
    "# Precision, Recall, F1-score\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"\\n📈 성능 지표:\")\n",
    "print(f\"  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall: {recall:.3f}\")\n",
    "print(f\"  F1-score: {f1:.3f}\")\n",
    "\n",
    "# 클래스별 상세 성능\n",
    "print(f\"\\n📈 클래스별 성능:\")\n",
    "precision_macro, recall_macro, f1_macro, support_macro = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "print(f\"  GENERAL (0): Precision={precision_macro[0]:.3f}, Recall={recall_macro[0]:.3f}, F1={f1_macro[0]:.3f}\")\n",
    "print(f\"  ENTITY (1): Precision={precision_macro[1]:.3f}, Recall={recall_macro[1]:.3f}, F1={f1_macro[1]:.3f}\")\n",
    "\n",
    "print(f\"\\n💾 최종 결과 저장 완료!\")\n",
    "print(f\"  - token_entity_spacy_predictions.csv: {len(df)}행\")\n",
    "print(f\"  - ENTITY: {entity_count}개 ({(entity_count/total_count)*100:.1f}%)\")\n",
    "print(f\"  - GENERAL: {general_count}개 ({(general_count/total_count)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ff03c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2769504676.py, line 121)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[28], line 121\u001b[0;36m\u001b[0m\n\u001b[0;31m    if word.lower() in common_lower_words:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff0299b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b0c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
