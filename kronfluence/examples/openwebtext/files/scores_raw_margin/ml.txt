Query Sequence:
Prompt:Machine learning can be defined as; Completion: a set of algorithms that enables computers to learn from data. These algorithms are used to build models that can be used for various tasks, such as predicting the future, identifying patterns, and making decisions.

Top Influential Sequences:
================================================================================
Rank = 0; Score = 65536000.0
<|begin_of_text|>Energy development is the field of activities focused on obtaining sources of energy from natural resources. These activities include production of conventional, alternative and renewable sources of energy, and for the recovery and reuse of energy that would otherwise be wasted. Energy conservation and efficiency measures reduce the demand for energy development, and can have benefits to society with improvements to environmental issues.

Societies use energy for transportation, manufacturing, illumination, heating and air conditioning, and communication, for industrial, commercial, and domestic purposes. Energy resources may be classified as primary resources, where the resource can be used in substantially its original form, or as secondary resources, where the energy source must be converted into a more conveniently usable form. Non-renewable resources are significantly depleted by human use, whereas renewable resources are produced by ongoing processes that can sustain indefinite human exploitation.

Thousands of people are employed in the energy industry. The conventional industry comprises the petroleum industry, the natural gas industry, the electrical power industry, and the nuclear industry. New energy industries include the renewable energy industry, comprising alternative and sustainable manufacture, distribution, and sale of alternative fuels.

Classification of resources [ edit ]

Open System Model (basics)

Energy resources may be classified as primary resources, suitable for end use without conversion to another form, or secondary resources, where the usable form of energy required substantial conversion from a primary source. Examples of primary energy resources are wind power, solar power, wood fuel, fossil fuels such as coal, oil and natural gas, and uranium. Secondary resources are those such as electricity, hydrogen, or other synthetic fuels.

Another important classification is based on the time required to regenerate an energy resource. "Renewable" resources are those that recover their capacity in a time significant by human needs. Examples are hydroelectric power or wind power, when the natural phenomena that are the primary source of energy are ongoing and not depleted by human demands. Non-renewable resources are those that are significantly depleted by human usage and that will not recover their potential significantly during human lifetimes. An example of a non-renewable energy source is coal, which does not form naturally at a rate that would support human use.

Fossil fuels [ edit ]

Fossil fuel (primary non-renewable fossil) sources burn coal or hydrocarbon fuels, which are the remains of the decomposition of plants and animals. There are three main types of fossil fuels: coal, petroleum, and natural gas. Another fossil fuel, liquefied petroleum gas (LPG), is principally derived from the production of natural gas. Heat
================================================================================
Rank = 1; Score = 56885248.0
<|begin_of_text|>Lady Gaga is an American singer, songwriter, and actress who has received many awards and nominations for her contributions to the music industry. She rose to prominence with the release of her debut album The Fame in 2008. The album won several awards and was nominated for six Grammy Awards, including Album of the Year. The album and its single "Poker Face" won Best Electronic/Dance Album and Best Dance Recording, respectively, at the 52nd Grammy Awards. The album also won International Album at the 2010 BRIT Awards. A follow-up EP, titled The Fame Monster, was released in 2009, and included the singles "Bad Romance" and "Telephone". The music videos of the songs won eight accolades from thirteen nominations at the 2010 MTV Video Music Awards (VMA), making Gaga the most nominated artist in VMA history for a single year and the first female artist to receive two nominations for Video of the Year in one single night.[1] In 2011, Gaga was nominated for six Grammy Awards, and won three—Best Pop Vocal Album for The Fame Monster, and Best Female Pop Vocal Performance and Best Short Form Music Video for "Bad Romance".

Born This Way (2011), Gaga's second studio album, accrued three nominations at the 54th Annual Grammy Awards, including her third consecutive nomination for Album of the Year. It won the People's Choice Awards for Album of the Year the following year, and the music video for the title track won two VMAs, including Best Female Video. Her third album, Artpop (2013), won the award for World's Best Album by a Female at the 2014 World Music Awards. In other musical ventures, Gaga released a collaborative jazz album with Tony Bennett, titled Cheek to Cheek (2014), which received the Grammy Award for Best Traditional Pop Vocal Album.

In 2015, Gaga released a song "Til It Happens to You", for the documentary film, The Hunting Ground. The song won a Satellite Award for Best Original Song, while nominated for an Academy Award, a Critics' Choice Movie Award for Best Song, and a Grammy Award for Best Song Written for Visual Media. In the same year she was named Woman of the Year by Billboard. She also became the first woman to receive the Digital Diamond Award from RIAA,[2] and the first artist to win the Songwriters Hall of Fame's Contemporary Icon Award for "attaining an iconic status in pop culture". Gaga received a Golden
================================================================================
Rank = 2; Score = 28835840.0
<|begin_of_text|>The many potential social and economic benefits from advances in AI-based technologies depend entirely on the environment in which these technologies evolve, says the Royal Society. According to a new report from the UK’s science academy, urgent consideration needs to be given to the “careful stewardship” needed over the next ten years to ensure that the dividends from machine learning – the form of artificial intelligence that allows machines to learn from data – benefit all in UK society.

Machine Learning: the power and promise of computers that learn by example, published today (25 April 2017), comes at a critical time in the rapid development and use of this technology, and the growing debate about how it will reshape the UK economy and people’s lives. Crucially the report calls for research funding bodies to support a new wave of machine learning research that goes beyond technical challenges, and into areas aimed at addressing public confidence in machine learning – vital to the UK maintaining its internationally competitive edge at the forefront of this area.

The report also offers the first evidence about the UK public’s views on machine learning, including the application areas about which they are particularly positive, and the need for the real-world data feeding the growth of this technology to be dealt with fairly and securely.

Professor Peter Donnelly FRS, chair of the report’s working group and Director of the Wellcome Trust Centre for Human Genetics and Professor of Statistical Science at the University of Oxford, says:

“Machine learning is already used in many apps and services that we encounter every day. It is used to tag people in our photos, by our phones to interpret voice commands, by internet retailers to make recommendations, and by banks to spot unusual activity on a credit or debit card. However, these current applications only scratch the surface of understanding just how powerful a technology this could be.

“Machine learning will have an increasing impact on our lives and lifestyles over the next five to ten years. There is much work to be done so that we take advantage of machine learning’s potential and ensure that the benefits are shared, especially as this could be a key area of opportunity for the UK in the coming years.”

An action plan so no one is left behind and the benefits are shared

The report calls for action in a number of key areas over the next five to ten years to create an environment of “careful stewardship” that can help ensure that the benefits of this technology are felt broadly. Understanding who will be most affected, how the benefits are likely to be distributed, and where the opportunities for growth lie, will be key to designing
================================================================================
Rank = 3; Score = 12386304.0
<|begin_of_text|>An enterprise such as Flipkart & Amazon is the master at analyzing big data. They have a huge a knowledge gathered from analyzing big data to achieve an edge over their competitor. Just think about the Amazon’s recommendation engine. It analyzes the process of buying history, buying habits, and pattern what people like to buying. With big data and predictive analytics, they have to build a marketing strategy and created an extremely successful business model.

So these things should be upon your mind while knowing about the big data analytics because with a rise in computational power, robust data infrastructure, rapid algorithm development, and the need to obtain better insight from an increasingly vast amount of data.

Big Data Analytics: – Types

Explore Analytics It can be used to explore your data in a graphical manner where your data provides some value through simple visualizations. It often used when you have a large amount of disparate data.

Advance Analytics: It provides analytics algorithms for executing complex analysis of either structured or unstructured data. It also includes such as machine learning, statistical model, text analytics and other data-mining techniques.

Operationalized Analytics: Are the part of the business process that helps to achieve operational efficiency by building models. For example, a data scientist for a banking organization might build a model that predicts the identity theft of its customer.

Summary

The capability to analyze big data provides unique opportunities for many enterprises. However comprehending big data can be challenging. Due to algorithms & technologies, basic analysis becomes confusing. It depends upon the scale size which means how many you’re capable for that.

For any query feel free & write an email us on:-support@raygain.com or visit our website:-http://www.raygain.com/<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 4; Score = 11141120.0
<|begin_of_text|>The Anatomy of a Large-Scale Hypertextual Web Search Engine

Sergey Brin and Lawrence Page

{sergey, page}@cs.stanford.edu

Computer Science Department, Stanford University, Stanford, CA 94305

Abstract

In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/

To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date.

Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want. Keywords: World Wide Web, Search Engines, Information Retrieval, PageRank, Google

1. Introduction

1.1 Web Search Engines -- Scaling Up: 1994 - 2000

1.2. Google: Scaling with the Web

These tasks are becoming increasingly difficult as the Web grows. However, hardware performance and cost have improved dramatically to partially offset the difficulty. There are, however, several notable exceptions to this progress such as disk seek time and operating system robustness. In designing Google, we have considered both the rate of growth of the Web and technological changes. Google is designed to scale well to extremely large data sets. It makes efficient use of storage space to store the index. Its data structures are optimized for fast and efficient access (see section 4.2). Further, we expect that the cost to index and store text or HTML will eventually decline relative to the amount that will be available
================================================================================
Rank = 5; Score = 10616832.0
<|begin_of_text|>A year ago, we shared with you our belief that Algorithm Development is Broken, where we laid out the problems in the world of algorithm development, and our vision of how to make things better.

Since then, we’ve been working hard to create our vision of the world’s first open marketplace for algorithms – and you, the developers of the world, have enthusiastically joined us to help active the algorithm economy.

Today, we are proud to open our doors to the world, and deliver on our promise of a global, always-available, open marketplace for algorithms. For the first time, algorithm developers can publish their algorithms as a live API and applications can be made smarter by taking advantage of these intelligent algorithms.

Algorithm Development with Algorithmia

Live : easy access to any algorithm, callable at any time

: easy access to any algorithm, callable at any time No dependencies : no downloads, no installs, no stress

: no downloads, no installs, no stress Interoperable : call any algorithm regardless of programming language

: call any algorithm regardless of programming language Composable : algorithms are building blocks, build something bigger!

: algorithms are building blocks, build something bigger! Invisible but powerful infrastructure : don’t worry about containers, networks, or scaling; we make your algorithm scale to hundreds of machines

: don’t worry about containers, networks, or scaling; we make your algorithm scale to hundreds of machines No operations: we scale, meter, optimize and bill for you

With our public launch we are enabling payments and go from being a repository of algorithms, to a true marketplace:

For algorithm developers, Algorithmia provides a unique opportunity to increase the impact of their work and collect the rewards associated with it.

For application developers, Algorithmia provides the largest repository of live algorithms ever assembled, supported by a vibrant community of developers.

So far, users have contributed over 800 algorithms, all under one API that grows every day. The most exciting thing from our point of view is watching the Algorithmia Marketplace grow and expand in its functionality.

In just the last few months, Algorithmia has gained the ability to:

Summarize and generate topics from unstructured text

Extract structure and information from arbitrary websites

Perform computer vision tasks like Face Detection and Image Similarity

Look for patterns and anomalies in data

Dozens of microservice building blocks

… with more added every day

Thanks to the thousands of developers who joined our cause, contributed algorithms, and gave us countless hours of feedback so far. We are very excited to be opening our doors, and
================================================================================
Rank = 6; Score = 8650752.0
<|begin_of_text|>Introduction A carbon footprint is the measure of the amount of greenhouse gases (Carbon footprint), measured in units of carbon dioxide, produced by human activities. A carbon footprint can be measured for an individual or an organization, and is typically given in tons of CO 2 -equivalent (CO 2 -eq) per year. For example, the average North American generates about 20 tons of CO 2 -eq each year. The global average carbon footprint is about 4 tons of CO 2 -eq per year (Figure 1).

Introduction A carbon footprint is the measure of the amount of greenhouse gases (Carbon footprint), measured in units of carbon dioxide, produced by human activities. A carbon footprint can be measured for an individual or an organization, and is typically given in tons of CO 2 -equivalent (CO 2 -eq) per year. For example, the average North American generates about 20 tons of CO 2 -eq each year. The global average carbon footprint is about 4 tons of CO 2 -eq per year (Figure 1).

Figure 1. National carbon dioxide (CO2) emissions per capita. (2005). (Source: UNEP/GRID-Arendal Maps and Graphics Library)

An individual’s or organization’s carbon footprint can be broken down into the primary and secondary footprints. The primary footprint is the sum of direct emissions of greenhouse gases from the burning of fossil fuels for energy consumption and transportation. More fuel-efficient cars have a smaller primary footprint, as do energy-efficient light bulbs in your home or office. Worldwide, 82% of anthropogenic greenhouse gas emissions are in the form of CO 2 from fossil fuel combustion (Figure 2).

The secondary footprint is the sum of indirect emissions of greenhouse gases during the lifecycle of products used by an individual or organization. For example, the greenhouse gases emitted during the production of plastic for water bottles, as well as the energy used to transport the water, contributes to the secondary carbon footprint. Products with more packaging will generally have a larger secondary footprint than products with a minimal amount of packaging.

Greenhouse gases and the greenhouse effect

Figure 2. Anthropogenic greenhouse gas emissions. (Source: Energy Information Administration)

Although carbon footprints are reported in annual tons of CO 2 emissions, they actually are a measure of total greenhouse gas emissions. A greenhouse gas is any gas that traps heat in the atmosphere through the greenhouse effect. Because of the presence of greenhouse gases in our atmosphere the average temperature of the Earth
================================================================================
Rank = 7; Score = 8519680.0
<|begin_of_text|>Photo

Google and a corporation associated with NASA are forming a laboratory to study artificial intelligence by means of computers that use the unusual properties of quantum physics. Their quantum computer, which performs complex calculations thousands of times faster than existing supercomputers, is expected to be in active use in the third quarter of this year.

The Quantum Artificial Intelligence Lab, as the entity is called, will focus on machine learning, which is the way computers take note of patterns of information to improve their outputs. Personalized Internet search and predictions of traffic congestion based on GPS data are examples of machine learning. The field is particularly important for things like facial or voice recognition, biological behavior, or the management of very large and complex systems.

“If we want to create effective environmental policies, we need better models of what’s happening to our climate,” Google said in a blog post announcing the partnership. “Classical computers aren’t well suited to these types of creative problems.”

Google said it had already devised machine-learning algorithms that work inside the quantum computer, which is made by D-Wave Systems of Burnaby, British Columbia. One could quickly recognize information, saving power on mobile devices, while another was successful at sorting out bad or mislabeled data. The most effective methods for using quantum computation, Google said, involved combining the advanced machines with its clouds of traditional computers.

Google bought the machine in cooperation with the Universities Space Research Association, a nonprofit research corporation that works with NASA and others to advance space science and technology. Outside researchers will be invited to the lab as well.

This year D-Wave sold its first commercial quantum computer to Lockheed Martin. Lockheed officials said the computer would be used for the test and measurement of things like jet aircraft designs, or the reliability of satellite systems.

The D-Wave computer works by framing complex problems in terms of optimal outcomes. The classic example of this type of problem is figuring out the most efficient way a traveling salesman can visit 10 customers, but real-world problems now include hundreds of such variables and contingencies. D-Wave’s machine frames the problem in terms of energy states, and uses quantum physics to rapidly determine an outcome that satisfies the variables with the least use of energy.

In tests last September, an independent researcher found that for some types of problems the quantum computer was 3,600 times faster than traditional supercomputers. According to a D-Wave official, the machine performed even better in Google’s tests, which involved 500 variables with different constraints.

“The tougher, more complex ones had better performance,” said Colin Williams
================================================================================
Rank = 8; Score = 8454144.0
<|begin_of_text|>Conceptualized by Dr. Peter Waterland, Quantum Resistant Ledger ($QRL) is a cryptocurrency ledger that uses hash-based digital signatures (proof-of-stake algorithms) instead of proof-of-work algorithms.

Because of this, it is resistant to both quantum and classical computing attacks. QRL is one of the first blockchain-based ledger to use post-quantum cryptography technology. For this reason, it has generated a lot of interest among cryptocurrency enthusiasts and blockchain developers alike. The companies cryptocurrency $QRL is currently trading as an ERC20 token on various exchanges including Liqui, Tidex and the most popular Bittrex.

QRL plans to allow mining (Genesis block) in September 2017. The number of coins in the Genesis block and the final distribution, which will happen in 200 years, are 65 million and 105 million respectively.

What is Quantum Computing?

Quantum computing is basically the use of quantum computers to perform computer-related tasks. Quantum computers store information in quantum bits (qubits) instead of bits. It is important to note that a qubit can store multiple numbers at once (a zero, a one, both a zero and a one, or any other value between a zero and a one).

For this reason, a quantum computer can perform multiple calculations simultaneously. Put another way, a quantum computer can work in parallel, allowing it to solve complex mathematical problems involving factorization, such as “prime factors” of a large number. This means that quantum computing can break cryptographic protocols, including the ones used by blockchain networks.

How will Quantum Resistant Ledger help crypto currency eco systems in a post-quantum computing world?

The conventional encryption protocols used to protect blockchain networks including Elliptic Curve Digital Signature Algorithm or ECDSA, elliptical curve cryptography (ECC) and large prime number cryptography (RSA) are vulnerable to quantum computing.

More specifically, quantum computing would make it possible to reverse engineer private keys quickly, thereby rendering the aforementioned encryption protocols useless. This is where QRL comes in handy. QRL uses post-quantum cryptography technology, which means it is theoretically immune from quantum computing attacks.

Protecting Our Future: Quantum Computing Threats & Considerations

Let's start with preparing for cyber-war scenarios.

So far, blockchain-based encryption algorithms such as ECDSA have been able to keep cybercriminals at bay because classical computers cannot break such algorithms. However, the introduction of quantum computing could potentially lead to a blockchain Armageddon because of the following threats:

1) Government Applications:
================================================================================
Rank = 9; Score = 8060928.0
<|begin_of_text|>Crowdsourcing is a sourcing model in which individuals or organizations obtain goods and services. These services include ideas and finances, from a large, relatively open and often rapidly-evolving group of internet users; it divides work between participants to achieve a cumulative result. The word crowdsourcing itself is a portmanteau of crowd and outsourcing, and was coined in 2005.[1][2][3][4] As a mode of sourcing, crowdsourcing existed prior to the digital age (i.e. "offline").[5]

There are major differences between crowdsourcing and outsourcing. Crowdsourcing comes from a less-specific, more public group, whereas outsourcing is commissioned from a specific, named group, and includes a mix of bottom-up and top-down processes.[6][7][8] Advantages of using crowdsourcing may include improved costs, speed, quality, flexibility, scalability, or diversity.[9][10]

Some forms of crowdsourcing, such as in "idea competitions" or "innovation contests" provide ways for organizations to learn beyond the "base of minds" provided by their employees (e.g. LEGO Ideas).[11] Tedious "microtasks" performed in parallel by large, paid crowds (e.g. Amazon Mechanical Turk) are another form of crowdsourcing. It has also been used by not-for-profit organizations and to create common goods (e.g. Wikipedia).[12] The effect of user communication and the platform presentation should be taken into account when evaluating the performance of ideas in crowdsourcing contexts.[13]

Definitions [ edit ]

The term "crowdsourcing" was coined in 2005 by Jeff Howe and Mark Robinson, editors at Wired, to describe how businesses were using the Internet to "outsource work to the crowd",[1] which quickly led to the portmanteau "crowdsourcing." Howe, first published a definition for the term crowdsourcing in a companion blog post to his June 2006 Wired article, "The Rise of Crowdsourcing", which came out in print just days later:[14]

"Simply defined, crowdsourcing represents the act of a company or institution taking a function once performed by employees and outsourcing it to an undefined (and generally large) network of people in the form of an open call. This can take the form of peer-production (when the job is performed collaboratively), but is also often undertaken by sole individuals. The crucial prerequisite is the use of the open call format and the large network of potential laborers."

In a February 1, 200
================================================================================
Rank = 10; Score = 7864320.0
<|begin_of_text|>How to use your Emotional Intelligence to Better PM

Veamly Blocked Unblock Follow Following Jun 26, 2017

(Originally posted on Veamly)

Note : This blog is based on Vivek Bedi’s talk, The VP of Product at LearnVest on Product School

As a Product Manager, you don’t only need to oversee the production process, but you also have to lead and motivate your team. After all, they are the people behind the success or the failure of the product. But the thing is that, they all have different personalities and come from very different backgrounds.

You have the shy and introvert engineers, the extrovert developers, the young and goal driven marketers, the creative designers, etc. You also need to deal with the product’s users on a daily basis: Make sure that your clients are satisfied, understand clearly their asks and know how to approach each one. That’s why it is important to use your Emotional Intelligence in order to better assess your team and your end users’ behaviors.

Imagine this!

If you have to release a product within 2 months and you have to choose the person you will be spending most of your time with, who would you choose? How would you describe this person?

If your answer is one or more of the following adjectives : Patient, Empathetic, Thoughtful, Flexible, Hard worker… You may be interested in the soft skills of the person rather than his hard skills and IQ. Indeed, you want to work with a person you like but who also helps you process fast. In product Management, more than any other profession, the soft skills are extremely important.

Imagine also receiving a request from a client for a feature that you would describe as obvious. You may have to explore your empathy since recognizing that not everyone interprets things the same way.

So What’s Emotional Intelligence?

Emotional Intelligence is the ability to identify and manage your own emotions and the emotions of others. It is generally said to include three skills: emotional awareness; the ability to harness emotions and apply them to tasks like thinking and problem solving; and the ability to manage emotions, which includes regulating your own emotions and cheering up or calming down other people. (Source)

So, Emotional Intelligence (EQ or EI) in a nutshell is defined as the ability to

Recognize, understand and manage our own emotions

Recognize, understand and influence the emotions of others

How to leverage your Emotional intelligence for Product Management?

Here are 8 Tips that we gathered from Vivek Bed
================================================================================
Rank = 11; Score = 7831552.0
<|begin_of_text|>Hermeticism is based on the teachings of a mysterious man named Hermes Trismegistus. He is portrayed as a wise teacher, a powerful magician, and a skilled mystic. He has been seen as a teacher of Moses, the inventor of alchemy, and the founder of occult schools throughout history.

Alexandrian Origins

Hermes Trismegistus is a composite of several mythological figures. He arose in Ptolemaic Alexandria, where the mishmash of Hellenistic and Egyptian culture bred dozens of interesting cults, religions, and deities. Early on, the Hellenistic deity Hermes became associated with the Egyptian deity Thoth.1 Both were inventors of writing and gods of magic. Both were also psychopomps, responsible for guiding souls in the afterlife. As a result, over the centuries they became identified with each other and were even worshiped together in certain Egyptian temples. In fact, Khmun, the Egyptian center for the cult of Thoth, became known as Hermopolis under the Ptolemies.

Because of this, Hermes Trismegistus, as a legendary teacher of magic and mysticism, was probably a humanized syncretization of Hermes and Thoth. His legacy has been immense.

Multiple Hermes?

Even though Hermes Trismegistus was a mythical figure, many ancient writers wrote about him as if he was a real person. This produced disagreements and confusion. At some point, it began to be assumed that there were two Hermes. In Asclepius, Hermes Trismegistus talks about his grandfather:

Is it not true that my grandfather Hermes, after whom I am named, resides in his eponymous town whence he aids and cures all those who come to him from every land?

Asclepius 372

This passage indicates that the Grandfather Hermes is in fact identical with the deity Hermes, residing at Hermopolis.3 Ancient writers invented additional Hermes to fill in the gaps. In fact, it could be that the title “Trismegistus” refers to many great Hermes characters, a line of sages and mystics bringing the Hermetic teachings to humanity over many generations.4

Astrologer, Magus, Alchemist

The Hermetica that we read and write about most often are not the only ancient books ascribed to Hermes Trismegistus. Multiple important early works on astrology were also attributed to the legendary sage. The link between Hermes and
================================================================================
Rank = 12; Score = 7798784.0
<|begin_of_text|>A cam is ausually done with a digital video camera. A mini tripod is sometimes used, but a lot of the time this wont be possible, so the camera make shake. Also seating placement isn’t always idle, and it might be filmed from an angle. If cropped properly, this is hard to tell unless there’s text on the screen, but a lot of times these are left with triangular borders on the top and bottom of the screen. Sound is taken from theof the camera, and especially in comedies, laughter can often be heard during the film. Due to these factors picture and sound quality are usually quite poor, but sometimes, and the theater will be fairly empty and a fairly clear signal will be heard.A telesync is the same spec as a CAM except it uses(most likely an audio jack in the chair for hard of hearing people). A direct audio source does not ensure a good quality audio source, as a lot of background noise can interfere. A lot of the times a telesync is filmed in an empty cinema or from the projection booth with a professional camera, giving a better picture quality. Quality ranges drastically, check the sample before downloading the full release.A telecine machine copies the film digitally from the. Sound and picture should be very good, but due to the equipment involved and cost telecines are fairly uncommon. Generally the film will be in correct aspect ratio, althoughtelecines have existed. A great example is thedone a few years ago. TC should not be confused with TimeCode, which is a visible counter on screen throughout the film., sent to rental stores, and various other places for promotional use. A screener is supplied on a VHS tape, and is usually in a 4:3 (full screen) a/r, although letterboxed screeners are sometimes found. The main draw back is a “ticker“ (a message that scrolls past at the bottom of the screen, with the copyright and anti-copy telephone number). Also, if the tape contains any serial numbers, or any other markings that could lead to the source of the tape, these will have to be blocked, usually with a black mark over the section. This is sometimes only for a few seconds, but unfortunately on some copies this will last for the entire film, and some can be quite big. Depending on the equipment used, screener quality can range from excellent if done from a MASTER copy, to very poor if done on an old VHS recorder thru poor capture equipment on a copied
================================================================================
Rank = 13; Score = 7372800.0
<|begin_of_text|>The expansion of the universe is the increase of the distance between two distant parts of the universe with time.[1] It is an intrinsic expansion whereby the scale of space itself changes. The universe does not expand "into" anything and does not require space to exist "outside" it. Technically, neither space nor objects in space move. Instead it is the metric governing the size and geometry of spacetime itself that changes in scale. Although light and objects within spacetime cannot travel faster than the speed of light, this limitation does not restrict the metric itself. To an observer it appears that space is expanding and all but the nearest galaxies are receding into the distance.

During the inflationary epoch about 10−32 of a second after the Big Bang, the universe suddenly expanded, and its volume increased by a factor of at least 1078 (an expansion of distance by a factor of at least 1026 in each of the three dimensions), equivalent to expanding an object 1 nanometer (10−9 m, about half the width of a molecule of DNA) in length to one approximately 10.6 light years (about 1017 m or 62 trillion miles) long. A much slower and gradual expansion of space continued after this, until at around 9.8 billion years after the Big Bang (4 billion years ago) it began to gradually expand more quickly, and is still doing so.

The metric expansion of space is of a kind completely different from the expansions and explosions seen in daily life. It also seems to be a property of the universe as a whole rather than a phenomenon that applies just to one part of the universe or can be observed from "outside" it.

Metric expansion is a key feature of Big Bang cosmology, is modeled mathematically with the Friedmann-Lemaître-Robertson-Walker metric and is a generic property of the universe we inhabit. However, the model is valid only on large scales (roughly the scale of galaxy clusters and above), because gravitational attraction binds matter together strongly enough that metric expansion cannot be observed at this time, on a smaller scale. As such, the only galaxies receding from one another as a result of metric expansion are those separated by cosmologically relevant scales larger than the length scales associated with the gravitational collapse that are possible in the age of the universe given the matter density and average expansion rate.

Physicists have postulated the existence of dark energy, appearing as a cosmological constant in the simplest gravitational models as a way to explain the
================================================================================
Rank = 14; Score = 7307264.0
<|begin_of_text|>In the world of elearning, microlearning has become a buzzword. It’s considered a powerful new approach to training the workforce. With the average attention span in North America dropping from 12 seconds in 2000 to 8 seconds in 2015, the demand for shorter, more engaging training is higher than ever!

In this post, our goal is to cover the basics and leave you with an understanding of: What Microlearning is and the benefits it can provide. Throughout this post, we’ll try to give you examples of how to use microlearning in your own training programs.

What is Microlearning?

Microlearning is focused learning that is delivered in bite sized chunks. Since this method of learning provides small bits of knowledge at a time, it’s best used for delivering information that learners need to retain.

Microlearning can be achieved using a number of different delivery methods: emails, online posts and short multimedia videos, are all examples of different ways you can deliver training that is designed for your learners to retain new knowledge and achieve their educational goals.

Examples of Microlearning:

– Watching video tutorials on Youtube

– Receiving small bits of education via email: like Word of the Day from Dictionary.com

– Online learning programs like Duolingo or Lynda

What are the benefits that Microlearning can provide?

1. Avoid the risk of overwhelming learners

Microlearning allows learners to move at their own pace, giving them the ability to go back and review complex concepts as often as needed. Since new knowledge is delivered in smaller chunks, learners avoid the risk of being overwhelmed by too much information at once.

2. Create on-the-go training that can be accessed anywhere, at anytime.

Microlearning can be achieved using a number of different delivery methods. Email, online posts, videos, even tweets because of this training can usually be accessed across multiple devices, making it available on-the-go. Learners can access and review training materials while doing everyday tasks like: waiting for the bus, sitting on the train, or even riding the elevator!

3. Help learners better retain new knowledge

Traditional classroom training often provides little to no long term takeaways for learners. The Wall Street Journal recently reported that 90 percent of new skills are lost within a year of training! Microlearning breaks new knowledge down into short chunks making learning easier to digest, understand, and apply on the job.

Different ways to use Microlearning:

– Health and Safety training

– Learning new software

– Business Processes and Procedures

Microlearning yields many benefits for organizations looking
================================================================================
Rank = 15; Score = 7241728.0
<|begin_of_text|>In the report, titled "China's Rise in Artificial Intelligence," the investment bank said the world's second-largest economy has emerged as a major global contender in using AI to drive economic progress.

Goldman said the government and companies have identified AI and machine learning as the next big areas of innovation.

"We believe AI technology will become a priority on the government's agenda, and we expect further national/regional policy and funding support on AI to follow," the bank said.

AI is already widespread: From simple smartphone applications that can tell the weather to complex algorithms that are able to easily beat humans in board games.

Companies such as Google and Microsoft have poured vast amounts of money into research and development to expand the horizon of what AI can achieve. Machines are fed large quantities of data and taught specific tasks, allowing companies to create software that can learn and become smarter.

While the United States is generally considered to be leading the field, other countries are catching up. China, home of internet powerhouses such as Baidu, Alibaba and Tencent, is one of them.

In July, China's State Council issued guidelines on developing AI inside the country and set a goal of becoming a global innovation center for it by 2030. It expects the total output value of AI industries to surpass 1 trillion yuan ($147.80 billion).

The Council encouraged the creation of open-source computing platforms and training more AI professionals and scientists. The guidelines said the government will invest in qualified AI projects and encourage private capital investment.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 16; Score = 6946816.0
<|begin_of_text|>It’s been a while we haven’t covered any machine learning algorithm. Last time we discussed the Markov Decision Process (or MDP).

Today we’re going to build our knowledge on top of the MDP and see how we can generalise our MDP to solve more complex problems.

Reinforcement learning really hit the news back in 2013 when a computer learned how to play a bunch of old Atari games (like Breakout) just by observing the pixels on the screen. Let’s find out how this is possible!

In Breakout the goal is to destroy all the bricks at the top of the screen by bouncing a ball over a paddle that you can move horizontally at the bottom of the screen. Every time the ball hits a brick your score increases.

The idea

If we want to teach a computer how to play that game, one solution might be to use a classifier such as a neural network where the input would be the screen images and the output the action to perform. It makes sense as for each screen we want to know if the paddle should move left or right (or stay still). The problem is that we need lots of training examples. But this is not how we learn! We don’t want someone to tell us a million time what we should do!

Instead we learn by observing the reward we get as a consequence of our actions. This is the essence of reinforcement learning. An agent (the player) performs some actions which change the state of the environment (the pixels on the screen) and gets a reward for his actions (increase of score).

Reinforcement learning lies somewhere between supervised and unsupervised learning. In supervised learning we have a label for every training example and in unsupervised learning there is no label at all. In reinforcement learning we have rewards which are sparse and time-delayed labels. Using only on those rewards the agent has to learn to behave in the environment.

It may seem simple but there are some challenges along the way. When the ball hits a brick it has nothing to do with the action you just take. In fact it was the action performed when the ball bounced on the paddle that sent the ball against the brick. The problem of identifying which action is at the origin of the reward is known as the credit assignment problem.

Markov Decision Process

Doesn’t it sound familiar? Indeed, it looks quite similar to our tiny robot example we use to illustrate the MDP.

The robot (the agent) moves (take actions) and gets a reward as it moves closer to
================================================================================
Rank = 17; Score = 6782976.0
<|begin_of_text|>We CAN predict the future (a bit): Why the brain knows what's going to happen before it does

People subconsciously make thousands of tiny predictions each day, whether it's contemplating when a bus will arrive, who is knocking on the door or if a dropped glass will break.

Now scientists are beginning to unravel how the brain is such a surprisingly accurate fortune-teller - but only when it comes to mundane events.

Researchers at Washington University in St Louis focused on the mid-brain dopamine system (MDS), which provides signals to the rest of the brain when unexpected events occur.

Each of us makes thousands of tiny predictions, such as contemplating when a bus will arrive, every day. Scientists are now beginning to unravel how the brain is such a surprisingly accurate fortune-teller

Using functional MRI (fMRI), they found that this system encodes prediction error when viewers are forced to choose what will happen next in a video of an everyday event.

They found that between 80 and 90 per cent of viewer predictions were correct, depending on when the footage was stopped.

Lead researcher Jeffrey Zacks said predicting the near future is vital in guiding behaviour and is a key component of theories of perception, language processing and learning.

He said: 'It's valuable to be able to run away when the lion lunges at you, but it's super-valuable to be able to hop out of the way before the lion jumps.

'It's a big adaptive advantage to look just a little bit over the horizon.'

The research will also help those in the early stages of neurological diseases such as schizophrenia, Alzheimer's and Parkinson's, Professor Zacks said.

The scientists tested healthy young volunteers who were shown films of everyday events such as washing a car, building a Lego model or washing clothes. The film would be watched for a while, and then it was stopped.

Participants were then asked to predict what would happen five seconds later when the film was re-started by selecting a picture that showed what would happen.

Half of the time, the movie was stopped just before an event boundary, when a new event was just about to start. The other half of the time, the film was stopped in the middle of an event.

The researchers found that participants were more than 90 per cent correct in predicting activity within the event, but less than 80 per cent correct in predicting across the event boundary. They were also less confident in their predictions.

'Successful predictions are associated with the subjective experience of a smooth stream of consciousness'

Professor Zacks said:
================================================================================
Rank = 18; Score = 6684672.0
<|begin_of_text|>Feb 6, 2017

This blog post will demonstrate how deep reinforcement learning (deep Q-learning) can be implemented and applied to play a CartPole game using Keras and Gym, in less than 100 lines of code!

I’ll explain everything without requiring any prerequisite knowledge about reinforcement learning.

The code used for this article is on GitHub.

Reinforcement Learning

Reinforcement Learning is a type of machine learning that allows you to create AI agents that learn from the environment by interacting with it. Just like how we learn to ride a bicycle, this kind of AI learns by trial and error. As seen in the picture, the brain represents the AI agent, which acts on the environment. After each action, the agent receives the feedback. The feedback consists of the reward and next state of the environment. The reward is usually defined by a human. If we use the analogy of the bicycle, we can define reward as the distance from the original starting point.

Deep Reinforcement Learning

Google’s DeepMind published its famous paper Playing Atari with Deep Reinforcement Learning, in which they introduced a new algorithm called Deep Q Network (DQN for short) in 2013. It demonstrated how an AI agent can learn to play games by just observing the screen without any prior information about those games. The result turned out to be pretty impressive. This paper opened the era of what is called ‘deep reinforcement learning’, a mix of deep learning and reinforcement learning.

Click to Watch: DeepMind’s Atari Player

In Q-Learning Algorithm, there is a function called Q Function, which is used to approximate the reward based on a state. We call it Q(s,a), where Q is a function which calculates the expected future value from state s and action a. Similarly in Deep Q Network algorithm, we use a neural network to approximate the reward based on the state. We will discuss how this works in detail.

Cartpole Game

Usually, training an agent to play an Atari game takes a while (from few hours to a day). So we will make an agent to play a simpler game called CartPole, but using the same idea used in the paper.

CartPole is one of the simplest environments in OpenAI gym (a game simulator). As you can see in the animation from the top, the goal of CartPole is to balance a pole connected with one joint on top of a moving cart. Instead of pixel information, there are 4 kinds of information given by the state, such as angle of the
================================================================================
Rank = 19; Score = 6520832.0
<|begin_of_text|>The past few years have seen rapid advances in machine learning, with dramatic improvements in technical performance—from more accurate speech recognition, to better image search, to improved translations. But we believe AI can go much further—and be more useful to all of us—if we build systems with people in mind at the start of the process.

Today we’re announcing the People + AI Research initiative (PAIR) which brings together researchers across Google to study and redesign the ways people interact with AI systems. The goal of PAIR is to focus on the "human side" of AI: the relationship between users and technology, the new applications it enables, and how to make it broadly inclusive. The goal isn’t just to publish research; we’re also releasing open source tools for researchers and other experts to use.

PAIR's research is divided into three areas, based on different user needs:

Engineers and researchers: AI is built by people. How might we make it easier for engineers to build and understand machine learning systems? What educational materials and practical tools do they need?

Domain experts: How can AI aid and augment professionals in their work? How might we support doctors, technicians, designers, farmers, and musicians as they increasingly use AI?

Everyday users: How might we ensure machine learning is inclusive, so everyone can benefit from breakthroughs in AI? Can design thinking open up entirely new AI applications? Can we democratize the technology behind AI?

We don't have all the answers—that's what makes this interesting research—but we have some ideas about where to look. One key to the puzzle is design thinking. Instead of viewing AI purely as a technology, what if we imagine it as a material to design with? Here history might serve as a guide: For instance, advances in computer graphics meant more than better ways of drawing pictures—and that led to completely new kinds of interfaces and applications. You can read more in this post on what we call human-centered machine learning (HCML).We’re open sourcing new tools, creating educational materials (such as guidelines for designing AI interfaces), and publishing research to answer these questions and spread the power of AI to as many people as possible.

Open-source tools

Today we're open sourcing two visualization tools, Facets Overview and Facets Dive. These applications are aimed at AI engineers, and address the very beginning of the machine learning process. The Facets applications give engineers a clear view of the data they use to train AI systems.

We think this is important because training data is a key ingredient in modern AI systems, but it can
================================================================================
Rank = 20; Score = 6488064.0
<|begin_of_text|>$\begingroup$

maybe all the major/preferred algorithms of interest to this audience have been mentioned at this point. however, a few more deserve mention for completeness. & some analysis of what is considered a significant algorithm is relevant here.

in CS & IT fields there seems to be a phenomenon noticed long ago in AI called "moving the goalposts". this is a psychological phenomenon where the field advances relatively quickly but people quickly mentally adjust to "the new normal" and take real or even breakthrough advances as mundane or unremarkable in retrospect, after accomplished, ie downplayed or minimized. this is highly captured in this question in the way that algorithms move from R&D into "deployment". quoting the author of the question in later comments:

In fact, a negligible fraction of all the code that gets written is implementing anything that is interesting from an algorithmic point of view.

but this is problematic and basically a TCS-centric redefinition of the word "algorithm". presumably the interesting algorithms are advanced. does that mean that if a problem is reduced to an advanced algorithm, its no longer "interesting"? and "advanced" is clearly a moving target. so there is a way to define "algorithms" narrowly, or broadly. it seems the TCS definition changes on context, but note even in TCS, there is a trend toward the broad definition eg in the so-called "algorithmic lens".

sometimes the most ubiquitous algorithms are also the most overlooked! the internet and WWW is a large environment/near-ecology for algorithms. still relatively young at only about 2 decades old (invented ~1991) it has grown massively and exponentially in a short amount of time. WWW site growth has probably even outpaced the famous exponential Moores law.

the internet/WWW are supported by many sophisticated algorithms. the internet has complex routing algorithms built into routers (again powering multi-billion-dollar corporations such as Cisco). some advanced theory is applicable there eg in routing algorithms. these algorithms were a subject of emerging, advanced/cutting edge research decades ago however a now so finetuned & well understood that its somewhat invisible.

we should not so soon forget that decades ago, leading researchers were not even sure if the internet world work or was possible (seen in early packet switching research, a radical new design pattern at the time departing from the prior circuit-switching), and even a few years ago there were fears that it would fail to scale at some point and begin to fail due to overwhelming spikes in volume.

it also uses sophisticated error detection/correction.
================================================================================
Rank = 21; Score = 6455296.0
<|begin_of_text|>Artificial intelligence (AI) may conjure up images of intelligent systems rising up to enslave us, with sadistic driverless cars taking passengers on terrifying joyrides and malevolent smart fridges refusing to order milk.

But in reality, AI is already here; many of us just do not realise it yet. Through the development of machine learning, the technology industry has created smart systems designed to help, not hinder, people.

Rather than send murderous robots back in time to destroy us, machine learning sits behind the scenes in the form of algorithms that learn from data to better deliver services offered by software applications.

This is the direction that Forrester principal analyst Diego Lo Giudice believes AI is heading: "While some AI research still tries to simulate our brain or certain regions of it - and is frankly unlikely to deliver concrete results anytime soon - most of it now leverages a less human, but more effective, approach revolving around machine learning and smart integration with other AI capabilities," he said in a blog post.

One example of this is Office Graph, created by Microsoft. Office Graph sits behind Office 356 and maps the relationships between people, content and activity across the company's productivity software suite.

Office Graph uses these relationships to serve up documents, files, email messages, and other elements in Office 365 that are relevant to the work the user is carrying out, with the goal of making the user more productive.

Another example is Microsoft's Skype Translator, which uses machine learning capabilities to improve the accuracy of its translation of spoken languages, something CEO Satya Nadella likens to "magic".

"The one fascinating feature of this is something called transfer learning. What happens is you teach it English and it learns English. Then you teach it Mandarin, and it learns Mandarin, but it becomes better at English," he said.

"Then you teach it Spanish, and it gets good at Spanish but gets great at both Mandarin and English. Quite frankly none of us knows exactly why. It's magic."

Not wanting to be left behind, Google recently announced an update to Google Translate to provide real-time translations that use machine learning to deliver better results the more it is used.

Big data analytics

But, while machine learning might improve the performance of consumer-level services, it is in the enterprise world that it really shines.

Big data is a big business, but trawling through hundreds of gigabytes of often unstructured information can be a lengthy and resource-sapping process. Add machine learning into the mix and that process becomes a lot easier.

Rather than
================================================================================
Rank = 22; Score = 5931008.0
<|begin_of_text|>Three Challenges for Artificial Intelligence in Medicine

Brandon Ballinger Blocked Unblock Follow Following Sep 19, 2016

Why is the world’s most advanced AI used for cat videos, but not to help us live longer and healthier lives? A brief history of AI in Medicine, and the factors that may help it succeed where it has failed before.

Imagine yourself as a young graduate student in Stanford’s Artificial Intelligence lab, building a system to diagnose a common infectious disease. After years of sweat and toil, the day comes for the test: a head-to-head comparison with five of the top human experts in infectious disease. Over the first expert, your system squeezes a narrow victory, winning by just 4%. It beats the second, third, and fourth doctors handily. Against the fifth, it wins by an astounding 52%.

Would you believe such a system exists already? Would you believe it existed in 1979? This was the MYCIN project, and in spite of the excellent research results, it never made its way into clinical practice. [1]

In fact, although we’re surrounded by fantastic applications of modern AI, particularly deep learning — self-driving cars, Siri, AlphaGo, Google Translate, computer vision — the effect on medicine has been nearly nonexistent. In the top cardiology journal, Circulation, the term “deep learning” appears only twice [2]. Deep learning has never been mentioned in the New England Journal of Medicine, The Lancet, BMJ, or even JAMA, where the work on MYCIN was published 37 years ago. What happened?

There are three central challenges that have plagued past efforts to use artificial intelligence in medicine: the label problem, the deployment problem, and fear around regulation. Before we get in to those, let’s take a quick look at the state of medicine today.

The Moral Case for AI in Healthcare

Medicine is life and death. With such high stakes, one could ask: should we really be rocking the boat here? Why not just stick with existing, proven, clinical-grade algorithms?

Well, consider a few examples of the status quo:

The most common prediction model for stroke risk was based on only 25 strokes. Source: Lip 2010

To be clear, none of this means medical researchers are doing a bad job. Modern medicine is a miracle; it’s just unevenly-distributed. Computer scientists can bring much to the table here. With tools like Apple’s ResearchKit and Google Fit, we can collect health data at scale
================================================================================
Rank = 23; Score = 5832704.0
<|begin_of_text|>This post is by Instacart VP Data Science Jeremy Stanley, and technical advisor and former LinkedIn data leader Daniel Tunkelang. Previously, Jeremy wrote the most comprehensive manual we’ve ever seen for hiring data scientists.

It's hard to believe that "data scientist" only became a bona fide job title in 2008. Jeff Hammerbacher at Facebook and DJ Patil at LinkedIn coined the term to capture the emerging need for interdisciplinary skills across analytics, engineering, and product. Today, the demand for data scientists has blossomed, and with it the need to better understand how to grow these teams for success.

The two of us have seen our share of the good, the bad, and the ugly, leading and advising teams at a variety of companies in different industries and at different stages of maturity. We've seen the challenges of not only hiring top data scientists, but making effective use of them and retaining them in a hyper competitive market for talent.

In this article, we've summarized the advice we give to founders who are interested in building data science teams. We explain why data science is so important for many startups, when companies should begin investing in it, where to put data science in their organization and how to build a culture where data science thrives.

First, what are you trying to achieve?

Data science serves two important but distinct sets of goals: improving the products your customers use, and improving the decisions your business makes.

Data products use data science and engineering to improve product performance, typically in the form of better search results, recommendations and automated decisions.

Decision science uses data to analyze business metrics — such as growth, engagement, profitability drivers, and user feedback — to inform strategy and key business decisions.

This distinction may sound straightforward, but it’s an important one to keep in mind as you establish and grow your data science team. Let’s take a closer look at these two areas.

Using Data Science to Build Better Products

Data products leverage data science to improve product performance. They rely on a virtuous cycle where products collect usage data that becomes the fodder for algorithms which in turn offer users a better experience.

What happens before you’ve collected that data? The first version of your product has to address what data science calls the “cold start” problem — it has to provide a "good enough" experience to initiate the virtuous cycle of data collection and data-driven improvement. It’s up to product managers and engineers to implement that good enough solution.

For example, when an Instacart user visits the site, the application
================================================================================
Rank = 24; Score = 5832704.0
<|begin_of_text|>Introduction

I’ve always thought it would be neat to create a digital version of oneself. The closest that programmers can get, is most likely the chat bot. Aside from chat bots being a goal towards beating the Turing Test, there may be an ulterior motive involved as well - to create a digital copy of one’s own personality, and thus, a pseudo form of immortality. We’ve come a long way in this progress (the chat bot part, not the immortality part), starting from the humble orgins of Eliza all the way to the highly configurable Alice chat bot.

Chat bots typically use case-based reasoning or similar techniques to map responses to certain keywords in a sentence. In this manner, a crude personality can be programmed, and a unique chat bot created. This is definitely an interesting exercise that I think all software developers should try, at least once. But, let’s try something a little different and apply a modern spin to the idea of an artificially generated digital persona.

Is it possible to apply machine learning to a Twitter user’s collection of tweets and accurately develop a model of the personality?

This article describes a program, using unsupervised learning with the K-Means algorithm, to automatically group a collection of tweets into distinct categories. Once the categories have been organized, the program will then locate new links that match the categories and recommend them for the persona. In the process, we’ll create a machine learning recommendation engine for Twitter that will recommend links, related to the user’s interests, and automatically identify new content.

Supervised vs Unsupervised Learning

There are two core methods for building a model from data with machine learning: supervised and unsupervised learning. Since we’re trying to determine a set of topics for a Twitter user’s personality, we could choose either method to build a recommendation engine. Let’s take a look at what’s involved for each learning type.

Supervised Learning

Using supervised learning, such as an SVM, a model could be constructed that maps articles a user likes vs dislikes. This is similar to book or movie rating services. We would record a set number of links that the user clicks on and an equal set of links that the user skips over (assume that we’re using an RSS feed of articles in this example). The SVM would learn to classify new articles as being liked or disliked, thus determining a general personality. One drawback to this approach is the sheer amount of rated data that is required, in addition to a logging mechanism to record the likes/dislikes. Since this kind of
================================================================================
Rank = 25; Score = 5799936.0
<|begin_of_text|>How to Grow and Prune a Classification Tree

In what follows, we will describe the work of Breiman and his colleagues as set out in their seminal book "Classification and Regression Trees." Theirs is a very rich story, and we will concentrate on only the essential ideas...

David Austin

Grand Valley State University

Email David Austin

Introduction

It's easy to collect data these days; making sense of it is more work. This article explains a construction in machine learning and data mining called a classification tree. Let's consider an example.

In the late 1970's, researchers at the University of California, San Diego Medical Center performed a study in which they monitored 215 patients following a heart attack. For each patient, 19 variables, such as age and blood pressure, were recorded. Patients were then grouped into two classes, depending on whether or not they survived more than 30 days following the heart attack.

Assuming the patients studied were representative of the more general population of heart attack patients, the researchers aimed to distill all this data into a simple test to identify new patients at risk of dying within 30 days of a heart attack.

By applying the algorithm described here, Breiman, Freidman, Olshen, and Stone, created a test consisting of only three questions---what is the patient's minimum systolic blood pressure within 24 hours of being admitted to the hospital, what is the patient's age, does the patient exhibit sinus tachycardia---to identify patients at risk. In spite of its simplicity, this test proved to be more accurate than any other known test. In addition, the importance of these three questions indicate that, among all 19 variables, these three factors play an important role in determining a patient's chance of surviving.

Besides medicine, these ideas are applicable to a wide range of problems, such as identifying which loan applicants are likely to default and which voters are likely to vote for a particular political party.

In what follows, we will describe the work of Breiman and his colleagues as set out in their seminal book Classification and Regression Trees. Theirs is a very rich story, and we will concentrate on only the essential ideas.

Classification trees

Before we start developing a general theory, let's consider an example using a much studied data set consisting of the physical dimensions and species of 150 irises. Here are the data for three irises out of the 150 in the full data set:

Sepal Length Sepal Width Petal Length Petal Width Species
================================================================================
Rank = 26; Score = 5734400.0
<|begin_of_text|>Computer Vision for Music Identification

by Yan Ke, Derek Hoiem, and Rahul Sukthankar

Abstract:

We describe how certain tasks in the audio domain can be effectively addressed using computer vision approaches. This paper focuses on the problem of music identification, where the goal is to reliably identify a song given a few seconds of noisy audio. Our approach treats the spectrogram of each music clip as a 2-D image and transforms music identification into a corrupted sub-image retrieval problem. By employing pairwise boosting on a large set of Viola-Jones features, our system learns compact, discriminative, local descriptors that are amenable to efficient indexing. During the query phase, we retrieve the set of song snippets that locally match the noisy sample and employ geometric verification in conjunction with an EM-based "occlusion" model to identify the song that is most consistent with the observed signal. We have implemented our algorithm in a practical system that can quickly and accurately recognize music from short audio samples in the presence of distortions such as poor recording quality and significant ambient noise. Our experiments demonstrate that this approach significantly outperforms the current state-of-the-art in content-based music identification.

Y. Ke, D. Hoiem, and R. Sukthankar. In Proceedings of Computer Vision and Pattern Recognition, 2005. [PDF 240KB]

Demo and Code available here:

Demonstration video: [CVPR05Video.avi 16MB]. Encoded with Indeo 5.10 and IMA ADPCM. There are both Windows and Linux codecs.

Windows/Cygwin and Linux binary demo code and sample music: [mrdemo.tgz 15.7MB]

C++ server code: [musicretr-1.0.tar.gz 87KB]

Java graphical user interface: [mrgui.tgz 21KB]<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 27; Score = 5537792.0
<|begin_of_text|>Energy lab team explores new ways of analyzing social media

A team at the Energy Department’s Pacific Northwest National Laboratory is creating an analysis tool that makes spotting trends in social media faster and easier.

The tool, called SALSA, for SociAL Sensor Analytics, takes an automated approach that can sort through billions of tweets or other posts in seconds, identifying patterns and finding key bits of information that could be useful in emergency response, public health and other areas, according to PNNL.

SALSA draws on the computing power of PNNL’s 600-node, 162-Teraflop Olympus supercomputing cluster, so the processing power is there. The challenge was finding a way to dig useful information out from all the banter on a platform that also has a non-traditional lexicon.

“The task we all face is separating out the trivia, the useless information we all are blasted with every day, from the really good stuff that helps us live better lives,” Court Corley, a PNNL data scientist who’s leading the research, said in PNNL’s report. “There's a lot of noise, but there's some very valuable information too."

Corley said the reach of social media creates value in analyzing posts to various platforms. "The world is equipped with human sensors — more than 7 billion and counting,” he said. “It's by far the most extensive sensor network on the planet. What can we learn by paying attention?"

Agencies have employed systems to monitor social media, such as the U.S. Geological Survey’s Twitter Earthquake Detector (TED). The system was created in 2009 because Twitter often spreads the word about earthquakes and other disasters faster than traditional sensor methods. But one reason TED works is that USGS taps into Twitter’s API to search for a known keyword — “earthquake.” Taking a deeper dive into analyzing social media is more of a challenge.

The Library of Congress has run into that difficulty with its Twitter archive. LOC has collected hundreds of billions of tweets for use in research, but found that available tools for sifting through the archive are inadequate. Searches take too much time (just searching the 20 billion tweets in its initial collection would take 24 hours), and as a result LOC was turning down requests from researchers to explore the archive.

And the amount of social media-generated data is only going to grow. PNNL points out that, as of mid-2012, information posted to social media outlets each hour included an average of 30 million comments,
================================================================================
Rank = 28; Score = 5505024.0
<|begin_of_text|>💪 From the big boys

Backchannel run a rare piece on how Apple uses machine learning. It states that a 200mb software package runs on the iPhone encompassing “app usage data, interactions with contacts, neural net processing, a speech modeler and a natural language event modeling system”. I’ve held the view for a while now that today’s AI techniques and infrastructure will re-open a class of historically intractable problems while also enabling us to rethink how products and features should be designed. Apple seem to think the same: “Machine learning is enabling us to say yes to some things that in past years we would have said no to. It’s becoming embedded in the process of deciding the products we’re going to do next.”

, modestly called

Salesforce announced their internal umbrella AI initiative, modestly called Einstein, which will go on to power many of the company’s cloud services, as well as expose AI tools to end users. The team of 175 data scientists includes talent from acquired startups MetaMind, PredictionIO and RelateIQ. The company’s flagship event, Dreamforce, will attract 170k people into SF next week.

Six of the most powerful technology companies have set up the have set up the Partnership on AI as a non-profit aimed at advancing public understanding of AI and formulate best practices on the challenges and opportunities within the field. An important catalyst to this end will undoubtedly be the continuation of open source technology development, which Seldon’s founder articulates in this piece.

🌎 On the importance and impact of AI on the World

Stanford’s 100 year study on AI published their first report. It finds “no cause for concern that AI is an imminent threat to humankind. No machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be developed in the near future”. From a public policy perspective, it recommends to:

Define a path toward accruing technical expertise in AI at all levels of government.

Remove the perceived and actual impediments to research on the fairness, security, privacy, and social impacts of AI systems.

Increase public and private funding for interdisciplinary studies of the societal impacts of AI.

a16z’s Chris Dixon sets out 11 reasons to be excited about the future of technology with short soundbites for each. Five of these are either directly related to or will be enabled by AI and machine learning.

👍 User-friendly AI

UC Berkeley announced a new Center for Human-Compatible AI to study how AI used for mission
================================================================================
Rank = 29; Score = 5472256.0
<|begin_of_text|>DeepMind

When DeepMind burst into prominent view in 2014 it taught its machine learning systems how to play Atari games. The system could learn to defeat the games, and score higher than humans, but not remember how it had done so.

DeepMind's AI has learnt to become 'highly aggressive' when it feels like it's going to lose DeepMind DeepMind's AI has learnt to become 'highly aggressive' when it feels like it's going to lose

Advertisement

For each of the Atari games, a separate neural network had to be created. The same system could not be used to play Space Invaders and Breakout without the information for both being given to the artificial intelligence at the same time. Now, a team of DeepMind and Imperial College London researchers have created an algorithm that allows its neural networks to learn, retain the information, and use it again.

"Previously, we had a system that could learn to play any game, but it could only learn to play one game," James Kirkpatrick, a research scientist at DeepMind and the lead author of its new research paper, tells WIRED. "Here we are demonstrating a system that can learn to play several games one after the other".

Read next DeepMind's Mustafa Suleyman: In 2018, AI will gain a moral compass DeepMind's Mustafa Suleyman: In 2018, AI will gain a moral compass

The work, published in the Proceedings of the National Academy of Sciences journal, explains how DeepMind's AI can learn in sequences using supervised learning and reinforcement learning tests. This is also explained in a blog post from the company.

Watch Google's Deep Mind complete Montezuma's Revenge Gaming Watch Google's Deep Mind complete Montezuma's Revenge

Advertisement

"The ability to learn tasks in succession without forgetting is a core component of biological and artificial intelligence," the computer scientists write in the paper. Kirkpatrick says a "significant shortcoming" in neural networks and artificial intelligence has been its inability to transfer what it has learned from one task to the next.

The group says it has been able to show "continual learning" that's based on'synaptic consolidation'. In the human brain, the process is described as "the basis of learning and memory".

The performance of DeepMind's EWC algorithm 'enhanced' neural network compared to its other nets DeepMind / PNAS

Read next DeepMind's latest AI breakthrough is its most significant yet DeepMind's latest AI breakthrough is its most significant yet
================================================================================
Rank = 30; Score = 5406720.0
<|begin_of_text|>The Bitcoin technologies, namely blockchains are sidechains are used for various other purposes than just for registering and facilitating transactions over the Bitcoin network. Some of the new companies like Astroblocks and Bitproof are building Proof-of-Existence platforms (there is also another platform by the same name) over the Bitcoin blockchain.

These Proof-of-Existence platforms provides a smart tamper-proof way of storing encrypted information on the Bitcoin blockchain. Each file stored on the blockchain will be associated with a unique irreplicable transaction hash. These transaction hashes will act as a reference to the documents stored on the blockchain. Anything once written on to the blockchain will be stored forever, without any way of deleting it and that is what makes it an ideal electronic system to certify and store documents.

We can safely say that intellectual property is the most valuable asset any individual or company can have. The process of obtaining protection for your intellectual property is still a cumbersome process. It involves filing the necessary applications, multiple times in few cases and waiting for a long time for the authorities to verify and grant protection. Blockchain based Proof-of-Existence platforms provides an alternate way for both individuals and companies alike to document their creations or discoveries and handle IP disputes more efficiently.

Whether it is a new scientific invention, design, artwork, screenplay, story or anything. The creator can record his works on these Proof-of-Existence platforms. In case of any dispute, the inventor can always contest any claims by using the transactional hash of the concerned document. The hash will also act as a timestamp that can be used to identify the time when it was uploaded onto the blockchain.

Now that anyone can make use of these Proof-of-Existence services, making it easier for them to claim and receive credit for their innovation and creation, no matter how small it is.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 31; Score = 5406720.0
<|begin_of_text|>OWASP ModSecurity Core Rule Set (CRS)

The 1st Line of Defense Against Web Application Attacks

The OWASP ModSecurity Core Rule Set (CRS) is a set of generic attack detection rules for use with ModSecurity or compatible web application firewalls. The CRS aims to protect web applications from a wide range of attacks, including the OWASP Top Ten, with a minimum of false alerts. The CRS provides protection against many common attack categories, including SQL Injection, Cross Site Scripting, Locale File Inclusion, etc. The offical website of the project can be found at https://coreruleset.org.

Getting Started / Tutorials

The following tutorials will get you started with ModSecurity and the CRS v3.

These tutorials are part of a big series of Apache / ModSecurity guides published by netnea. They are written by Christian Folini.

More Information about the rule set is available at the official website, https://coreruleset.org.

Licensing

OWASP ModSecurity CRS is free to use. It is licensed under the Apache Software License version 2 (ASLv2), so you can copy, distribute and transmit the work, and you can adapt it, and use it commercially, but all provided that you attribute the work and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one.

Reporting Issues

If you think you've found a false positive in commercially available software and want us to take a look, submit an issue on our Github

Have you found a false negative/bypass? We'd love to hear about it - please responsibly disclose it to security@coreruleset.org<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 32; Score = 5275648.0
<|begin_of_text|>In the age of ‘Big Data,’ with datasets rapidly growing in size and complexity and cloud computing becoming more pervasive, data science techniques are fast becoming core components of large-scale data processing pipelines.

Apache Spark offers analysts and engineers a powerful tool for building these pipelines, and learning to build such pipelines will soon be a lot easier. Databricks is excited to be working with professors from University of California Berkeley and University of California Los Angeles to produce two new upcoming Massive Open Online Courses (MOOCs). Both courses will be freely available on the edX MOOC platform in spring summer 2015. edX Verified Certificates are also available for a fee.

The first course, called Introduction to Big Data with Apache Spark, will teach students about Apache Spark and performing data analysis. Students will learn how to apply data science techniques using parallel programming in Spark to explore big (and small) data. The course will include hands-on programming exercises including Log Mining, Textual Entity Recognition, Collaborative Filtering that teach students how to manipulate data sets using parallel processing with PySpark (part of Apache Spark). The course is also designed to help prepare students for taking the Spark Certified Developer exam. The course is being taught by Anthony Joseph, a professor at UC Berkeley and technical advisor at Databricks, and will start on February 23rd June 1st, 2015.

The second course, called Scalable Machine Learning, introduces the underlying statistical and algorithmic principles required to develop scalable machine learning pipelines, and provides hands-on experience using PySpark. It presents an integrated view of data processing by highlighting the various components of these pipelines, including exploratory data analysis, feature extraction, supervised learning, and model evaluation. Students will use Spark to implement scalable algorithms for fundamental statistical models while tackling real-world problems from various domains. The course is being taught by Ameet Talwalkar, an assistant professor at UCLA and technical advisor at Databricks, and will start on April 14th June 29th, 2015.

Both courses are available for free on the edX website. You can sign up for them today:<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 33; Score = 5275648.0
<|begin_of_text|>* Dept of Homeland Security: Java vulnerable to hackers

* Could be used to steal identity, form malicious networks

* Applies to browsers on all major operating systems

By Jim Finkle

Jan 11 (Reuters) - The U.S. Department of Homeland Security urged computer users to disable Oracle Corp’s Java software, amplifying security experts’ prior warnings to the hundreds of millions of consumers and businesses that use it to surf the Web.

Hackers have figured out a way to exploit Java to install malicious software enabling them to commit crimes ranging from identity theft to making an infected computer part of an ad-hoc network of computers that can be used to attack websites.

“We are currently unaware of a practical solution to this problem,” the Department of Homeland Security’s Computer Emergency Readiness Team said in a posting on its website late on Thursday.

“This and previous Java vulnerabilities have been widely targeted by attackers, and new Java vulnerabilities are likely to be discovered,” the agency said. “To defend against this and future Java vulnerabilities, disable Java in Web browsers.”

Java is a computer language that enables programmers to write software utilizing just one set of code that will run on virtually any type of computer, including ones that use Microsoft Corp’s Windows, Apple Inc’s OS X and Linux, an operating system widely employed by corporations.

Computer users access Java programs through modules, or plug-ins, that run Java software on top of browsers such as Internet Explorer and Firefox.

The U.S. government’s warning on Java came after security experts earlier on Thursday warned of the newly discovered flaw.

It is relatively rare for government agencies to advise computer users to completely disable software due to a security bug, particularly in the case of widely used programs such as Java. They typically recommend taking steps to mitigate the risk of attack while manufacturers prepare an update, or hold off on publicizing the problem until an update is prepared.

In September, the German government advised the public to temporarily stop using Microsoft’s Internet Explorer browser to give it time to patch a security vulnerability that opened it to attacks.

The Department of Homeland Security said that attackers could trick targets into visiting malicious websites that would infect their PCs with software capable of exploiting the bug in Java.

It said that an attacker could also infect a legitimate website by uploading malicious software that would infect machines of computer users who trust that site because they have previously visited it without experiencing any problems.

They said developers of several popular tools known as exploit kits, which criminal hackers use to attack PCs, have added software that allows hackers to exploit the newly discovered bug in Java to attack
================================================================================
Rank = 34; Score = 5242880.0
<|begin_of_text|>Originally called “connected TVs,” and now they are called as “smart TVs”. Any television that can be connected to the Internet to access services, use apps and behave in some way as our computers with web browser. Smart TVs connect to Internet via wired Ethernet connection or Wi-Fi to connect to a home network. Smart TVs require computer chips to juggle video processing, multiple screens and an Internet connection. They also use memory to buffer streaming video and music, and need additional processing power to deal with graphics. The TVs can be controlled by voice commands and by apps running on some Smartphone.

Dan Reynolds, information security solution and training expert of International Institute of cyber security explains that these Smart TVs are not that smart and the security of software isn’t exactly perfect. Smart TVs resemble for us the Internet of things (IoT) but old vulnerabilities which were considered to have completely disappeared are new vulnerabilities again in the Internet of Things (IoT). Sometimes you can easily find a flaw that can enable you to take a variety of actions on the TV, including accessing potentially sensitive data, remote files and information, the drive image and eventually gain root access to the device.

In the article we will be covering different aspects of two most famous brands of Smart TVs Samsung and LG with the help of ethical hacking course professor of IIcybersecurity.

Understanding SAMSUNG SMART TV Operating system

Tizen is an operating system based on the Linux kernel and the GNU C Library implementing the Linux API. It targets a very wide range of devices including smart phones, tablets, in-vehicle infotainment (IVI) devices, smart TVs, PCs, smart cameras, wearable computing, Blu-ray players, printers and smart home appliances. Its purpose is to offer a consistent user experience across devices. Tizen would be implemented in Samsung TVs from 2015.

There are some online community which are working over the Samsung smart TV OS research like ( Sammygo) mentions Dan Reynolds, information security solution and training expert.

How to do analysis over Samsung Smart TV firmware

ExLink connector consist of a cable which has in one side a 3.5mm jack, like the audio ones, and on the other side an RS232 ( Serial ) DB9 connector. This cable will allow you to connect your PC computer to the TV, and enter in the Serial mode. With this you can use a serial Communications Software, like Hyperterminal, Putty from Windows or Linux.

Connecting to Samsung TV

Put the TV into Standby Mode, press [Info] then [Menu]
================================================================================
Rank = 35; Score = 5177344.0
<|begin_of_text|>Despite the events depicted in The Imitation Game, Alan Turing did not invent the machine that cracked Germany’s codes during World War II—Poland did. But the brilliant mathematician did invent something never mentioned in the film: a mathematical tool for judging the reliability of information. His tool sped up the work of deciphering encoded messages using improved versions of the Polish machines.

Now researchers studying rhesus monkeys have found that the brain also uses this mathematical tool, not for decoding messages, but for piecing together unreliable evidence to make simple decisions. For Columbia University neuroscientist Michael Shadlen and his team, the finding supports a larger idea that all the decisions we make—even seemingly irrational ones—can be broken down into rational stastical operations. “We think the brain is fundamentally rational,” says Shadlen.

Invented in 1918, the German Enigma machine created a substitution cipher by swapping the original letters in a message for new ones, producing what seemed like pure gibberish. To make the cipher more complicated, the device had rotating disks inside that swiveled each time a key was pressed, changing the encoding with each keystroke. The process was so complex that even with an Enigma machine in hand, the Germans could decipher a message only by knowing the initial settings of those encryption dials.

Turing created an algorithm that cut down the number of possible settings the British decryption machines, called bombes, had to test each day. Working at the secret Bletchley Park facility in the U.K., Turning realized that it was possible to figure out if two messages had come from machines with rotors that started in the same positions—a key piece of information for figuring out those positions. Line up two encoded messages, one on top of the other, and the chance that any two letters will be the same is slightly greater if both messages came from machines with the same initial settings. This is because in German, as in English, certain letters tend to be more common, and the encryption process preserved this pattern.

Turing’s algorithm essentially added up the probabilities of those clues being useful. It also indicated when the cumulative odds were good enough to either accept or reject that the two messages being compared came from machines with the same rotor states. This statistical tool, called the sequential probability ratio test, proved to be the optimal solution to the problem. It saved time by allowing the Bletchley codebreakers to decide whether two messages were useful while looking at the fewest number of letters possible. Turning wasn
================================================================================
Rank = 36; Score = 5144576.0
<|begin_of_text|>Let be a natural number. A basic operation in the topology of oriented, connected, compact, -dimensional manifolds (hereby referred to simply as manifolds for short) is that of connected sum: given two manifolds, the connected sum is formed by removing a small ball from each manifold and then gluing the boundary together (in the orientation-preserving manner). This gives another oriented, connected, compact manifold, and the exact nature of the balls removed and their gluing is not relevant for topological purposes (any two such procedures give homeomorphic manifolds). It is easy to see that this operation is associative and commutative up to homeomorphism, thus and, where we use to denote the assertion that is homeomorphic to.

(It is important that the orientation is preserved; if, for instance,, and is a chiral 3-manifold which is chiral (thus, where is the orientation reversal of ), then the connect sum of with itself is also chiral (by the prime decomposition; in fact one does not even need the irreducibility hypothesis for this claim), but is not. A typical example of an irreducible chiral manifold is the complement of a trefoil knot. Thanks to Danny Calegari for this example.)

The -dimensional sphere is an identity (up to homeomorphism) of connect sum: for any. A basic result in the subject is that the sphere is itself irreducible:

Theorem 1 (Irreducibility of the sphere) If, then.

For (curves), this theorem is trivial because the only connected -manifolds are homeomorphic to circles. For (surfaces), the theorem is also easy by considering the genus of. For the result follows from the prime decomposition. But for higher, these ad hoc methods no longer work. Nevertheless, there is an elegant proof of Theorem 1, due to Mazur, and known as Mazur’s swindle. The reason for this name should become clear when one sees the proof, which I reproduce below.

Suppose. Now consider the infinite connected sum

This is an infinite connected sum of spheres, and can thus be viewed as a half-open cylinder, which is topologically equivalent to a sphere with a small ball removed; alternatively, one can contract the boundary at infinity to a point to recover the sphere. On the other hand, by using the associativity of connected sum (which will still work for the infinite connected sum, if one thinks about it carefully
================================================================================
Rank = 37; Score = 5111808.0
<|begin_of_text|>Just a few years ago the common perception was that we were in an AI winter.

Although there were lots of narrow AI applications running in the background of our daily lives, there wasn’t much enthusiasm.

But quietly in the background, a revolution was building thanks to progress across a few key areas. These areas would soon converge to produce breakthrough after breakthrough and put us on the verge of what many believe to be the most important event in human history.

Key Advance 1) More Data

Andrew Ng of Baidu explains that a massive amount of data is needed. If you put 10x the data in many of these algorithms they work, put 1/10th in and they don’t.

Previously it was very difficult to get hold of the massive amounts of data required to feed the AI systems, but thanks to the internet researchers have tonnes of data to train their neural nets.

Key Advance 2) More Computing Power

If you only have the computational power to build a small neural network it doesn’t work. But computer power has continued to increase and prices have dropped.

Moore’s Law may be stalling, but the Law of Accelerating Returns is not.

The Law of Accelerating Returns. What Steve Jurvetson calls “the most important graph ever”.

GPUs are much better for training neural networks than CPUs and have provided the computer power needed for these algorithms to function.

Infrastructure has also improved. Today it’s possible for anyone to rent a massive amount of GPU power on cloud computing platforms (AWS, Microsoft Azure, Google Cloud, IBM Cloud).

Key Advance 3) Better Algorithms

Neural networks have been known about for decades, but most researchers had given up on them

Geoffrey Hinton of Google is one of the few who stuck with it. Despite his peers calling it a dead end, he believed that it was the right approach. It turned out he was right.

Hinton learned how to stack neural networks dozens of layers deep (deep learning) which enabled vastly more calculations and now he is considered “the godfather of neural networks”.

With these 3 breakthroughs in place neural networks finally began to work. And they worked better than almost anyone expected.

The Tipping Point: ImageNet 2012

The ImageNet project was created in 2009 to judge how well computers can see.

In 2011 computers had a 26% error rate when trying to label images. Humans only had a 5% error rate.

But in 2012, Hinton’s team made a breakthrough
================================================================================
Rank = 38; Score = 5046272.0
<|begin_of_text|>We live in a world of pain and suffering. There is no one who is not affected by the harsh realities of life, and the question “why do bad things happen to good people?” is one of the most difficult questions in all of theology. God is sovereign, so all that happens must have at least been allowed by Him, if not directly caused by Him. At the outset, we must acknowledge that human beings, who are not eternal, infinite, or omniscient, cannot expect to fully understand God’s purposes and ways.The book of Job deals with the issue of why God allows bad things to happen to good people. Job was a righteous man (Job 1:1), yet he suffered in ways that are almost beyond belief. God allowed Satan to do everything he wanted to Job except kill him, and Satan did his worst. What was Job’s reaction? “Though he slay me, yet will I hope in him” (Job 13:15). “The LORD gave and the LORD has taken away; may the name of the LORD be praised” (Job 1:21). Job did not understand why God had allowed the things He did, but he knew God was good and therefore continued to trust in Him. Ultimately, that should be our reaction as well.Why do bad things happen to good people? As hard as it is to acknowledge, we must remember that there are no “good” people, in the absolute sense of the word. All of us are tainted by and infected with sin (Ecclesiastes 7:20; Romans 3:23; 1 John 1:8). As Jesus said, “No one is good—except God alone” (Luke 18:19). All of us feel the effects of sin in one way or another. Sometimes it’s our own personal sin; other times, it’s the sins of others. We live in a fallen world, and we experience the effects of the fall. One of those effects is injustice and seemingly senseless suffering.When wondering why God would allow bad things to happen to good people, it’s also good to consider these four things about the bad things that happen:1) Bad things may happen to good people in this world, but this world is not the end. Christians have an eternal perspective: “We do not lose heart. Though outwardly we are wasting away, yet inwardly we are being renewed day by day. For our light and momentary troubles are achieving for us an eternal glory that far
================================================================================
Rank = 39; Score = 4685824.0
<|begin_of_text|>Eigenfaces is the name given to a set of eigenvectors when they are used in the computer vision problem of human face recognition.[1] The approach of using eigenfaces for recognition was developed by Sirovich and Kirby (1987) and used by Matthew Turk and Alex Pentland in face classification.[2] The eigenvectors are derived from the covariance matrix of the probability distribution over the high-dimensional vector space of face images. The eigenfaces themselves form a basis set of all images used to construct the covariance matrix. This produces dimension reduction by allowing the smaller set of basis images to represent the original training images. Classification can be achieved by comparing how faces are represented by the basis set.

History [ edit ]

The eigenface approach began with a search for a low-dimensional representation of face images. Sirovich and Kirby (1987) showed that principal component analysis could be used on a collection of face images to form a set of basis features. These basis images, known as eigenpictures, could be linearly combined to reconstruct images in the original training set. If the training set consists of M images, principal component analysis could form a basis set of N images, where N < M. The reconstruction error is reduced by increasing the number of eigenpictures, however the number needed is always chosen less than M. For example, if you need to generate a number of N eigenfaces for a training set of M face images, you can say that each face image can be made up of "proportions" of all this K "features" or eigenfaces : Face image 1 = (23% of E 1 ) + (2% of E 2 ) + (51% of E 3 ) +... + (1% E n ).

In 1991 M. Turk and A. Pentland expanded these results and presented the eigenface method of face recognition.[3] In addition to designing a system for automated face recognition using eigenfaces, they showed a way of calculating the eigenvectors of a covariance matrix in such a way as to make it possible for computers at that time to perform eigen-decomposition on a large number of face images. Face images usually occupy a high-dimensional space and conventional principal component analysis was intractable on such data sets. Turk and Pentland's paper demonstrated ways to extract the eigenvectors based on matrices sized by the number of images rather than the number of pixels.

Once established, the eigenface method was expanded to include methods of preprocessing to improve accuracy.[4
================================================================================
Rank = 40; Score = 4685824.0
<|begin_of_text|>Quality of life (QOL) is an overarching term for the quality of the various domains in life. It is a standard level that consists of the expectations of an individual or society for a good life. These expectations are guided by the values, goals and socio-cultural context in which an individual lives. It is a subjective, multidimensional concept that defines a standard level for emotional, physical, material and social well-being. It serves as a reference against which an individual or society can measure the different domains of one’s own life. The extent to which one's own life coincides with this desired standard level, put differently, the degree to which these domains give satisfaction and as such contribute to one's subjective well-being, is called life satisfaction.

Overview [ edit ]

Quality of life is the general well-being of individuals and societies, outlining negative and positive features of life. It observes life satisfaction, including everything from physical health, family, education, employment, wealth, safety, security to freedom, religious beliefs, and the environment.[1] QOL has a wide range of contexts, including the fields of international development, healthcare, politics and employment. It is important not to mix up the concept of QOL with a more recent growing area of health related QOL (HRQOL[2]). An assessment of HRQOL is effectively an evaluation of QOL and its relationship with health.

Quality of life should not be confused with the concept of standard of living, which is based primarily on income.

Standard indicators of the quality of life include not only wealth and employment but also the built environment, physical and mental health, education, recreation and leisure time, and social belonging.[3][4] According to the World Health Organization (WHO), quality of life is defined as “the individual’s perception of their position in life in the context of the culture and value systems in which they live and in relation to their goals.” In comparison to WHO's definitions, the Wong-Baker Faces Pain Rating Scale defines quality of life as “life quality (in this case, physical pain) at a precise moment in time.”[5]

According to ecological economist Robert Costanza:

While Quality of Life (QOL) has long been an explicit or implicit policy goal, adequate definition and measurement have been elusive. Diverse "objective" and "subjective" indicators across a range of disciplines and scales, and recent work on subjective well-being (SWB) surveys and the psychology of happiness have spurred renewed interest.[6]

One approach, called engaged theory, outlined
================================================================================
Rank = 41; Score = 4685824.0
<|begin_of_text|>BY: Follow @LizWFB

Researchers at the National Institutes of Health (NIH) have developed a system that can predict the "psychological status" of users with smartphones and hope to private companies to bring the invention to the market.

The technology appeared on a list of NIH inventions published in the Federal Register that are now available to be licensed by private companies. The government allows companies to license inventions resulting from federal research in order to expedite their arrival on the marketplace.

The system uses smartphones to ask people how they are doing mentally during the day and based on the results can "deliver an automated intervention" if necessary.

"The NIH inventors have developed a mobile health technology to monitor and predict a user's psychological status and to deliver an automated intervention when needed," according to the notice published Wednesday. "The technology uses smartphones to monitor the user's location and ask questions about psychological status throughout the day."

"Continuously collected ambulatory psychological data are fused with data on location and responses to questions," the NIH said. "The mobile data are combined with geospatial risk maps to quantify exposure to risk and predict a future psychological state. The future predictions are used to warn the user when he or she is at especially high risk of experiencing a negative event that might lead to an unwanted outcome (e.g., lapse to drug use in a recovering addict)."

The NIH said the technology has potential commercial applications for "real-time behavior monitoring" and "therapeutic delivery of an intervention via a mobile device."

Researchers developed the system from a project that tracked the mood and cravings of drug users in Baltimore. The $8.9 million federal study sought to develop algorithms that could "automatically detect behavioral events (such as episodes of drug use or stress) without requiring self-report."

The NIH said the app is currently being used for drug addiction interventions, but that the "inventors are also seeking to test the technology for other health applications."

Request for comment from the NIH was not returned by press time.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 42; Score = 4620288.0
<|begin_of_text|>Fraser Smith is a psychology research assistant and seminar tutor who is also working toward his doctorate in counseling psychology. Fraser took time out of his busy schedule to speak with us about his love for psychology, his ongoing projects and his desire to encourage collaboration in the psychology field.

Smith credits his mother for inspiring his career in psychology, as she was the first person in his family to receive a degree in the field. Fraser knew he wanted to emulate his mother and go on to college after high school. He also noted, “I have always been fascinated by people and the way they behave, both individually and in group settings. I think this played a part in my passion for psychology as well.”

Even with this inspiration, Fraser took a unique path to end up in his current role. His first exposure to counseling came when he began an evening job working behind the reception desk of a relationship counseling center. The company then offered to send him on a free therapy training course, which sparked a love for the direct practice aspects of psychology. This led him to pursue a psychology degree while working as a research assistant and after graduation to continue on toward a doctorate degree.

Currently, Smith is working as a qualitative researcher in a lab investigating antimicrobial resistance. Participants are interviewed about their perceptions of antibiotics and their behaviors about taking them. The hope is to increase the knowledge surrounding how people think about antibiotics to better educate them about taking them. The research is still in its early stages, but Smith is excited to see results and gain a deeper understanding of human behavior.

When working in the lab, Fraser spends a lot of time analyzing interview transcripts and conducting thematic analyses of these transcripts. He also works with a team to conduct interviews and to bring their findings together to create publishable results. Smith uses printed transcripts and a highlighter when analyzing, as he finds this helps him with his analysis. He then uploads his results into an online program to ensure his data is secure. One of the program’s current flaws is that it is difficult to communicate his results with fellow researchers. Fraser likes the features of Conseris because it safely stores data while also making it easy to identify patterns and share those trends with fellow researchers. Smith emphasized the importance of collaboration in this area of research.

When not in the lab, Fraser is working on the early stages of his doctorate in counseling psychology. He will soon begin practical training and will recieve his placement for direct practice later this year. Smith hopes to learn more about the power of therapy so he can enact his passion for helping people
================================================================================
Rank = 43; Score = 4554752.0
<|begin_of_text|>OctaEdit has been built in a modular fashion, where each module can be used for various tasks, and each module can interact with the other modules in various ways.

OctaEdit has eleven (11) Modules: Project, Samples, Sequencer, Manager, Chainer, Arp Designer, LFO Designer, Library, Analytics, Options and Support.

OctaEdit LE is restricted to five (5) Modules: Manager, Chainer, Analytics, Options and Support.

The Manual is available to download for further details.

OctaEdit YouTube Channel

Please visit our Forum for detailed information and FAQ’s.

The Project Module

The Project module provides access to all the settings of the currently loaded Project, includes integration with the Library module and copy/paste functionality.

The Samples Module.

The Samples module allows the ability to manage all sample slots within a project. Features include Sample Slicing/Editing, drag/drop from file browser or audio browser, sample remapping, sample playback, sample editing, renaming and analysis amongst others.

The Sequencer Module.

The Sequencer module provides full access to every element of the Octatrack, from entire configurations down to a parameter locks on a single step.

The Sequencer module is fully integrated with the Arp and LFO Designer modules allowing quick and easy modifications on each track.

The Sequencer module is also fully integrated with the Library module allowing the ability to Save/Load any element of your setup, from your Parts, Audio/Midi/Recoder Machine configurations, Scenes through to FX Chains, Amp configurations and much, much more…

The Manager module.

The Manager module provides the ability to copy “Elements” from a Source Set/Project to a Target Set/Project.

Elements can be copied at the Project, Bank, Pattern, Track, Part or Scene level, from any Project in any Set to any other Project in any other Set, and includes the ability to remap sample assignments on the fly.

Boasting a feature set that is way too long to list, with just a couple of clicks entire sets can be merged together seamlessly, meaning you never have to worry about the Octatrack structure ever again.

The Chainer Module.

The Chainer module allows the ability to create Sample Chains on the fly from various audio files. Features include drag/drop from file browser and integration with OctaEdit’s other modules like the ‘Samples’ module, randomisation and so on.

The Arp Designer Module.

The Arp Designer module provides the ability to graphically design arpeggios
================================================================================
Rank = 44; Score = 4554752.0
<|begin_of_text|>InfluxDB : InfluxDB is an open-source time series database written in Go that has been built to work best with metrics, events, and analytics. Using InfluxDB, you can easily store system and application performance data and manage any time series data.

Grafana : Grafana is an open-source, general purpose dashboard that is used for visualizing time series data for Internet infrastructure and application analytics. Grafana supports graphite, influxdb or opentsdb as backends and runs as a web application.

In this tutorial, we will learn how to create and run Grafana and InfluxDB Docker containers in Ubuntu 14.04.

Requirements

A server running Ubuntu-14.04 with Docker installed.

A non-root user with sudo privileges setup on server.

Creating The Dockerfile

First, you will need to create the Docker file to install all requisite software. Create docker file inside your home directory using the following command:

sudo nano Dockerfile

Add the following lines with all requisite software:

FROM ubuntu MAINTAINER Hitesh Jethva (hitjethva@gmail.com) RUN apt-get update && apt-get -y --no-install-recommends install ca-certificates software-properties-common python-django-tagging python-simplejson python-memcache python-ldap python-cairo python-pysqlite2 python-support python-pip gunicorn supervisor nginx-light nodejs git curl openjdk-7-jre build-essential python-dev

Add the following lines to install Grafana, InfluxDB, and do some basic configuration:

WORKDIR /opt RUN curl -s -o /opt/grafana-1.8.1.tar.gz http://grafanarel.s3.amazonaws.com/grafana-1.8.1.tar.gz && curl -s -o /opt/influxdb_latest_amd64.deb http://s3.amazonaws.com/influxdb/influxdb_latest_amd64.deb && mkdir /opt/grafana && tar -xzvf grafana-1.8.1.tar.gz --directory /opt/grafana --strip-components=1 && dpkg -i influxdb_latest_amd64.deb && echo "influxdb soft nofile unlimited" >> /etc/security/limits.conf && echo "influxdb hard nofile unlimited" >> /etc/security/limits.conf

Next, copy some configuration files:

ADD config.js /opt/grafana/config.js ADD nginx.conf /etc/nginx/nginx.conf ADD supervisord.conf /etc/supervisor/conf.d/superv
================================================================================
Rank = 45; Score = 4358144.0
<|begin_of_text|>JREF Swift Blog

The Lurking Pornographer: Why Your Brain Turns Bubbles Into Nude Bodies

There is a pornographer lurking in some corner of your mind. He peeks out from behind the curtains of your consciousness without warning, and almost never at an acceptable time.

The lurking pornographer in your brain is ever vigilant, looking for patterns, for signs of nudity, and sometimes generating them out of nowhere. He is exceedingly good at what he does, and isn’t afraid to prove his power over your perception. Just like that, he can take a picture of Daniel Craig in a bathing suit and turn it obscene.

If anything, Craig is more covered than he was before, but still he must be nude in the new picture, or so the pornographer would have you believe. The pornographer is sly. He takes advantage in the slightest slip in shapes and curves to insert his nudity. One of his favorite techniques is called “bubbling,” a technique that reveals how our brains actually “see.”.

Breasts and Blind Spots

Stifled by the pornography-restricting tenets of his religion, a young Mormon took to Photoshop, or so the story goes. His attempt to fool God and circumvent his law resulted in “bubbling,” a trick clever enough that how it works hasn’t yet been answered.

You eye doesn’t see everything. Right now, there are innumerable photons hitting the photoreceptors all over your retina, except in the place where your optic nerve connects to it. This area is your blind spot, and it should show up as a rather large black dot in your vision, but it doesn’t. Why not?

As your brain matures, it learns from the world. Neuronal connections are formed and broken in accordance with the deluge of information your brain receives. Over time, your brain becomes adept at predicting the world, so much so that much of our conscious lives are spent only noticing when things aren’t going as predicted. For example, there was probably a time when you got out of the car and realized you have almost no recollection of the drive you just took. It seemed automatic because it was. Consciousness didn’t need to intrude during something so routine, so it didn’t. However, introduce a near-collision into your daily commute, and consciousness quickly steps up to handle the situation.

Based on all the shapes and colors and lines and lighting schemes that your brain has encountered, your cognition makes predictions about how things will look. The surprising
================================================================================
Rank = 46; Score = 4325376.0
<|begin_of_text|>Crop insurance is purchased by agricultural producers, and subsidized by the federal government, to protect against either the loss of their crops due to natural disasters, such as hail, drought, and floods, or the loss of revenue due to declines in the prices of agricultural commodities. The two general categories of crop insurance are called crop-yield insurance and crop-revenue insurance. On average, the federal government subsidizes 62 percent of the premium. In 2014, crop insurance policies covered 294 million acres. Major crops are insurable in most counties where they are grown, and approximately 83% of U.S. crop acreage is insured under the federal crop insurance program. Four crops—corn, cotton, soybeans, and wheat— typically account for more than 70% of total enrolled acres. For these major crops, a large share of plantings is covered by crop insurance. In 2014, the portion of total corn acreage covered by federal crop insurance was 87%; cotton, 96%; soybeans, 88%; and wheat, 84%.

Specialty crops [ edit ]

A farmer or grower may desire to grow a crop associated with a particular defined attribute that potentially qualifies for a premium over similar commodity crops, agricultural products, or derivatives thereof. The particular attribute may be associated with the genetic composition of the crop, certain management practices of the grower, or both. However, many standard crop insurance policies do not differentiate between commodity crops and crops associated with particular attributes. Accordingly, farmers have a need for crop insurance to cover the risk of growing crops associated with particular attributes.[1]

United States [ edit ]

In the United States, a subsidized multi-peril federal insurance program, administered by the Risk Management Agency, is available to most farmers. The program is authorized by the Federal Crop Insurance Act (which is actually title V of the Agricultural Adjustment Act of 1938, P.L. 75-430), as amended. Federal crop insurance is available for more than 100 different crops, although not all insurable crops are covered in every county. With the amendments to the Federal Crop Insurance Act made by the Federal Crop Insurance Reform Act of 1994 (P.L. 103-354, Title I) and the Agriculture Risk Protection Act of 2000 (P.L. 106-224), USDA is authorized to offer basically free catastrophic (CAT) coverage to producers who grow an insurable crop. For a premium, farmers can buy additional coverage beyond the CAT level. Crops for which insurance
================================================================================
Rank = 47; Score = 4292608.0
<|begin_of_text|>A cinematographer is also known as a director of photography. They’re the guys that make the movies we watch look how they look. It’s their photographic eye that we see. And they don’t get too much recognition for the work they do, with most of the attention going towards the director and actor already. I wanted to write about a few good ones and see if it can become a weekly thing if you guys are into it. You probably know the work these guys have done, so I’ll cover what they did to get the shots that we see on the big screen.

If this is going to be the first out of more to come, I’ll start it off with a bang by focusing it on Roger Deakins.

Roger Deakins

You’d probably have called him a purist; Roger Deakins’ preferred medium was film, until around 2011, when he stepped into the digital world when the Alexa cameras caught his eye.

“I prefer Super 35 because it allows you to use short focal-length lenses. I also like the scale of that format — the intimacy — and the texture of the film grain. In some cases I find anamorphic to be almost too clean, too grain-free and pristine.” (AC Magazine, October 2007)

Regardless of what he uses, he uses it to its fullest potential. If you’ve seen The Shawnshank Redemption, No Country for Old Men, The Assassination of Jesse James, Skyfall or even How to Train Your Dragon, you’ve seen his work. The guy’s got a resume, and it’s a wonder that none of his work has placed him an oscar yet. Here’s how a few of his most iconic shots were done:

The Assassination of Jesse James by the Coward Robert Ford

The Assassination of Jesse James opens with a train robbery. This scene was being filmed entirely at night, in a town called Edmonton. The train they had was supposedly a bit small (the director started calling it Thomas the Tank Engine), and they didn’t have the budget to bring a bigger train. Deakins assured the crew that with the right camera work, it wouldn’t be a big deal at all. After attempting to calm the crew down, Deakins gave the set another scare with his approach to how he’d film the scene: with just one source of light coming from a lamp set on the train.

Considering this was being done with a night shot, everyone was rightfully pretty uneasy; what resulted, however, was possibly
================================================================================
Rank = 48; Score = 4194304.0
<|begin_of_text|>Internet pornography is any pornography that is accessible over the Internet, primarily via websites, peer-to-peer file sharing, or Usenet newsgroups. The availability of widespread public access to the World Wide Web in late 1990s led to the growth of Internet pornography.

A 2015 study finds "a big jump"[1] in pornography viewing over the past few decades, with the largest increase occurring between people born in the 1970s and those born in the 1980s. While the study's authors note this increase is "smaller than conventional wisdom might predict," it's still quite significant. Children born in the 1980s onward are also the first to grow up in a world where they have access to the Internet beginning in their teenage years, and this early exposure and access to Internet pornography may be the primary driver of the increase.[1] The sex and tech conference series Arse Elektronika dedicated their 2007 conference to what they call pr0nnovation. The con presented a keynote by culture theorist Mark Dery and published a reader about the subject.

As of 2018, a single company, MindGeek, owns and operates many popular[2] pornographic websites, including video sharing services Pornhub, RedTube, and YouPorn, as well as adult film producers Brazzers, Digital Playground, Men.com, Reality Kings, and Sean Cody, among others.[3] It has been alleged to be a monopoly.[3]

History and methods of distribution

Before the World Wide Web

Pornography is regarded by some as one of the driving forces behind the expansion of the World Wide Web, like the camcorder VCR and cable television before it.[4] Pornographic images had been transmitted over the Internet as ASCII porn but to send images over network needed computers with graphics capability and also higher network bandwidth. This was possible in the late 1980s and early 1990s through the use of anonymous FTP servers and through the Gopher protocol. At this time the internet was mainly an academic and military network and there was not widespread use of the internet. One of the early Gopher/FTP sites was at tudelft and was called the Digital Archive on the 17th Floor (List of websites founded before 1995). This small image archive contained some low quality scanned pornographic images that were initially available to anyone anonymously, but the site soon became restricted to Netherlands only access.

Usenet groups

Usenet newsgroups provided an early way of sharing
================================================================================
Rank = 49; Score = 4096000.0
<|begin_of_text|>Researchers at the University of Liverpool have developed a set of algorithms that will help teach computers to process and understand human languages.

Whilst mastering natural language is easy for humans, it is something that computers have not yet been able to achieve. Humans understand language through a variety of ways for example this might be through looking up it in a dictionary, or by associating it with words in the same sentence in a meaningful way.

The algorithms will enable a computer to act in much the same way as a human would when encountered with an unknown word. When the computer encounters a word it doesn’t recognise or understand, the algorithms mean it will look up the word in a dictionary (such as the WordNet), and tries to guess what other words should appear with this unknown word in the text.

Semantic representation

It gives the computer a semantic representation for a word that is both consistent with the dictionary as well as with the context in which it appears in the text. In order to know whether the algorithm has provided the computer with an accurate representation of a word it compares similarity scores produced using the word representations learnt by the computer algorithm against human rated similarities.

Liverpool computer scientist, Dr Danushka Bollegala, said: “Learning accurate word representations is the first step towards teaching languages to computers.”

“If we can represent the meaning for a word in a way a computer could understand, then the computer will be able to read texts on behalf of humans and perform potentially useful tasks such as translating a text written in a foreign language, summarising a lengthy article, or find similar other documents from the Internet.

“We are excitingly waiting to see the immense possibilities that will be brought about when such accurate semantic representations are used in various language processing tasks by the computers.”

The research was presented at the Association for Advancement of Artificial Intelligence Conference (AAAI-2016) held in Arizona, USA.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 50; Score = 3964928.0
<|begin_of_text|>The business of male escort services in India is often complex and dangerous.

Last week, the police in New Delhi uncovered a racket after rescuing an aspiring escort who was kidnapped by a fake agency and held for ransom in Hapur in the northern state of Uttar Pradesh. The 26-year-old was allegedly lured by the promise of earning between Rs15,000 and Rs25,000 a day, much more than what he made as a salesman selling mobile phones at a store in the national capital.

The incident comes in the wake of the rapid proliferation of escort services across India, taking advantage of the internet, social media networks, and technology like WhatsApp to make transactions quick and smooth. Over the years, both male and female escort services have become well-versed in techniques such as Search Engine Optimisation (SEO) to make sure their websites are easily visible to those seeking company or a little more online. And these methods have paid off: one anonymous escort agency owner told The Times of India that the industry has a daily turnover of around Rs10 crore in Mumbai and Rs50 crore in New Delhi.

For some young Indians, among them struggling actors or models, signing up as an escort is a way to make some serious money, anywhere between Rs20,000 to Rs40,000 a day, by one estimate. And while many escort agencies advertise their services in PG terms like “friendship,” there’s a whole range of options available, including the “girlfriend experience” that blends transactional sex with the intimacy of a relationship, albeit one that is paid for.

But as the Hapur kidnapping shows, the shadowy workings of the industry have left a lot of escorts and aspiring escorts vulnerable. In July, India banned 240 escort websites in a bid to stem the spread of these services but the move was widely criticised as ineffective. After all, many of the websites could simply switch to a new domain and resume business as usual.

So far, India’s laws have mainly concerned themselves with prostitution and human trafficking, with a focus on women and children, but there’s still a lack of comprehensive regulation. Under the Immoral Traffic Prevention Act of 1956, it’s soliciting sex that’s a crime, besides running a brothel or forcing individuals to engage in prostitution. So technically, prostitution itself isn’t illegal. But in reality, the laws are interpreted in line with the society’s moral code that stigmatises sex workers and keeps them working in the shadows, vulnerable to exploitation.

The situation is more complicated when it comes to
================================================================================
Rank = 51; Score = 3932160.0
<|begin_of_text|>Nobody wants a laptop computer that stops working when a cloud passes by. Storing sunlight as fuel that can be later used to drive fuel cells requires new materials. Scientists demonstrated such a material. They combined two oxides on the atomic scale. The interface between the oxide materials, one containing strontium and titanium and one containing lanthanum and chromium, absorbs visible light, producing electrons and holes that might be useful for catalyzing reactions, such as producing hydrogen fuel. However, if there is nothing to pull those electrons and holes apart, they will quickly annihilate one another without doing anything useful. By carefully synthesizing this material as a series of alternating layers, the international team created a built-in electric field that could help separate the excited electrons and holes and improve the material's performance as a catalyst.

This material opens up new scientific frontiers to solve a persistent energy challenge: storing solar energy for later use. Fuel cells capable of running on hydrogen fuel created by solar energy could allow people to heat their homes and run their computers on solar energy even in the dark of night.

By depositing thin layers of SrTiO 3 and LaCrO 3 on a crystalline substrate, the investigators at the Pacific Northwest National Laboratory, Argonne National Laboratory, SuperSTEM, and the University of Oxford controlled how the ions at the interfaces bond to each other. This allowed them to engineer an electric field in the material that can be used to separate electrons and holes. Using X-ray spectroscopy techniques, they confirmed that the electric field was present and that the ions in the material shift positions as a result. Ultra-high resolution measurements using an electron microscope helped confirm this ionic shift.

All of these experimental results were backed up by computational modeling of the materials, which predicted behavior in excellent agreement with what was observed.

The electric field in these materials is present because the researchers were able to precisely control the growth process so that each interface has either a positive or negative charge. Alternating positively and negatively charged interfaces in the material produce nanoscale electric fields that can interact with the electrons and holes that are excited by solar energy. Electrons may then be driven to the surface, where they could interact with water molecules to break their bonds and produce hydrogen fuel.

Researchers are continuing to explore the properties of these superlattices using cutting-edge X-ray measurements at synchrotrons around the world and using other advanced microscopy techniques to look at the chemical makeup of the interfaces.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 52; Score = 3932160.0
<|begin_of_text|>Email Address

First Name

Last Name

Phone Number

Job Title

Company

Terms and Conditions

This is a legal agreement between you, the end-user (“User”), and PRRI. By downloading the survey data from the PRRI web site (“Data”) you are agreeing to be bound by the terms and conditions of this agreement. If you do not agree to be bound by these terms, do not download or use the Data.

PRRI hereby grants to the User a non-exclusive, revocable, limited, non-transferable license to use the Data solely for (1) research, scholarly or academic purposes, (2) the internal use of your business, or (3) your own personal non-commercial use. You may not reproduce, sell, rent, lease, loan, distribute, or sublicense or otherwise transfer any Data, in whole or in part, to any other party, or use the Data to create any derived product for resale, lease or license. Notwithstanding the foregoing, you may incorporate limited portions of the Data in scholarly, research, or academic publications or for the purposes of news reporting, provided you acknowledge the source of the Data (with express references to PRRI, as well as the complete title of the report) and include the following legend:

PRRI bears no responsibility for the analyses or interpretations of the data presented here.

The Data is provided “as is” without any warranty of any kind, either express or implied, arising by law or otherwise, including but not limited to warranties of completeness, non-infringement, accuracy, merchantability, or fitness for a particular purpose. The User assumes all risk associated with use of the data and agrees that in no event shall the center be liable to you or any third party for any indirect, special, incidental, punitive, or consequential damages including, but not limited to, damages for the inability to use equipment or access data, loss of business, loss of revenue or profits, business interruptions, loss of information or data, or other financial loss, arising out of the use of, or inability to use, the data based on any theory of liability including, but not limited to, breach of contract, breach of warranty, tort (including negligence), or otherwise, even if User has been advised of the possibility of such damages.

PRRI has taken measures to ensure that the Data is devoid of information that could be used to identify individuals (e.g., names, telephone numbers, email addresses, social security numbers) who participated in or who were the subject of
================================================================================
Rank = 53; Score = 3915776.0
<|begin_of_text|>Recently, the operation entity of Baoquan.com——Shuqin Technology Co.,Ltd.(hereinafter referred to as Shuqin) has signed a further strategic partnership agreement with Factom to promote the cross-border interation between both two sides. As early as in 2016, Baoquan.com and Factom have reached a strategic cooperation intention to jointly promote the development of Blockcchian underlying technology. And for this time, both parties will have a deep learning and exploring in application layer of Baoquan.com and the bottom products of Factom, then to further form the co-operation with business interconnection and combination technique.

Factom also said that Shuqin is the customer who uploading the maximum amount of data on Factom chain except itself. Relying on the technical support of Factom, Shuqin has completed the key projects like China local stock exchange, big data, etc. And cooperated with Qianmai for the judicial identication, which means Blockchain electronic data attestation won the first approval of justice on judicature layer. On the one hand, Factom will apply a more comprehensive identity management and on-chain data feedback service to perfect the underlying technology service. On the other hand, Shuqin will also provide the business practice to support the upgrading of basic technology and the deploying of demestic alliance nodes.

Factom is an infrastructure service provider for recording, verifying and auditing based on Blockchain, and stores the world’s data on a decentralized system. Using blockchain technology for smart contracts, digital assets and database integrity. The core technology that enables company to interact smoothly with data, improve efficiency, and make better decisions. And Factom software can plug into the existing systems with proven technology that can be deployed quickly and adapt to any domain. The application of Factom has far exceeded the scope of recording and management, and is being ued in copyright, education, taxation, government services and some other fields.

As for a China local blockchain enterprise, Shuqin is devoting to solve the practical problem by blockchain technology with the core business on the financial service. Baoquan.com as the first product in electronic data attestation, applied the distributed and decentralized characteristics of blockchian technology to provide the one-stop service for enterprises and personal. Big data exchange centre as the core project, using blockchain technology to ensure the authenticity and safety of data sources, and achieve the transparency of chain-trading, then to realize the goal of real-time traceability for the data sources
================================================================================
Rank = 54; Score = 3883008.0
<|begin_of_text|>Growth for the sake of growth is the ideology of the cancer cell –-Edward Abbey

Capitalism can be defined as a system under which industries, trade and the means of production are largely or wholly privately owned and operated for profit. Following the end of feudalism, it dominated the Western world, and thanks to imperialism this domination extended to the global economic system by the end of the 19th century. Entering the 21st century, it continues to reign unchallenged as the world’s pre-eminent economic doctrine.

The world’s richest person (Bill Gates) has a personal wealth of $78.7 billion. This is higher than the (nominal) GDP of 130 countries, including Uruguay (population 3.4m), Ecuador (population 15.9m), Bulgaria (population 7.2m) and Croatia (population 4.3m). The wealth of the top ten richest people combined is $544 billion — higher than the GDP of 172 of the 194 nations for which UN data is available, including Thailand (population 65m), South Africa (population 54m), Egypt (population 88m), Portugal (population 10.4m) and Czech Republic (population 10.5m).

Almost half the world’s population, over 3 billion people, live on less than $2.50 a day. According to UNICEF, 22,000 children die EACH DAY due to poverty. They “die quietly in some of the poorest villages on earth, far removed from the scrutiny and the conscience of the world. Being meek and weak in life makes these dying multitudes even more invisible in death.” Nearly a billion people entered the 21st century unable to read a book or sign their names. For every $1 in aid a developing country receives, over $25 is spent on debt repayment. Less than one per cent of what the world spent every year on weapons was needed to put every child into school by the year 2000. [Sources. (Note: site last updated in January 2013. Some data is a few years out of date, meaning it is likely to be worse now as global inequality has widened.)]

The modern form capitalism has taken is complex, with close relationships and revolving doors between politics and the multinational corporations and banks that act as the main players commonplace. Directors of corporations are under severe pressure to increase profits each quarter, partly to increase the size, market share, wealth and power of the company
================================================================================
Rank = 55; Score = 3866624.0
<|begin_of_text|>I created a model that learns how to turn outlines into stylish and colorful icons. Icon and logo design is difficult — expert human designers choose from line weights, colors, textures and shapes to create beautiful icons such as these (I’m a big fan). But there seems to be a pattern to how each designer make their choices. So, I decided it would be interesting to try to train a model that learns a designer’s style, and then takes any freely available icon outlines (e.g. from the Noun Project ), and color and style them exactly as how a designer would have completely automatically. I built this as part of my Stanford CS229 final project.

I’ve made my code and pre-trained model weights available at https://github.com/mosessoh/iconcolor.

How I envision this being used is that someone starts a side project, and needs an logo/icon for branding. Let’s say her side project is about sharing beautiful royalty free images (e.g. Unsplash). She grabs an icon she likes (e.g. this camera icon from the IconBros icon pack that went viral on Product Hunt), and submits it to the model, and voila — a fully colored and stylized icon. Note how the model uses some darker shades of orange at the sides to give it some visual depth, and adds a splash of green to really make it pop.

Why I think this might be useful

Beautiful icon outlines are easy to find online, relative to colored icons. This can help someone generate an original & unique colored icon for whatever project they need. It might not be as good as Yoga Perdana’s or Ivan Bobrov’s work, but I think it’s much better than using a generic solid icon.

How it works

There are more details in the poster below (I made that for CS229), but at a high level, this is modelled as a supervised learning problem. I take in a 1 x 128 x 128 grayscale icon and produce a 3 x 128 x 128 RGB icon which is then compared to a true RGB icon using some loss function during training. The model is a Convolutional Neural Network called a U-Net which I trained on an icon set from Smashicons (I’m a premium subscriber — there isn’t a more complete set of ). I taught the model to convert outlines to yellow style icons, but this model can learn to convert between arbitrary styles since nothing is hard-coded.

Poster for my CS229 final project

The top 3 things I learned from my
================================================================================
Rank = 56; Score = 3833856.0
<|begin_of_text|>Genetically modified animals are animals that have been genetically modified for a variety of purposes including producing drugs, enhancing yields, increase resistance to disease, etc. The vast majority of genetically modified animals are at the research stage with the number close to entering the market remains small.[1]

Production [ edit ]

The process of genetically engineering mammals is a slow, tedious, and expensive process.[2] As with other genetically modified organisms (GMOs), first genetic engineers must isolated the gene they wish to insert into the host organism. This can be taken from a cell containing the gene[3] or artificially synthesised.[4] If the chosen gene or the donor organism's genome has been well studied it may already be accessible from a genetic library. The gene is then combined with other genetic elements, including a promoter and terminator region and usually a selectable marker.[5]

A number of techniques are available for inserting the isolated gene into the host genome. With animals DNA is generally inserted into using microinjection, where it can be injected through the cell's nuclear envelope directly into the nucleus, or through the use of viral vectors.[6] The first transgenic animals were produced by injecting viral DNA into embryos and then implanting the embryos in females.[7] It is necessary to ensure that the inserted DNA is present in the embryonic stem cells.[8] The embryo would develop and it would be hoped that some of the genetic material would be incorporated into the reproductive cells. Then researchers would have to wait until the animal reached breeding age and then offspring would be screened for presence of the gene in every cell, using PCR, Southern hybridization, and DNA sequencing.[9]

New technologies are making genetic modifications easier and more precise.[2] Gene targeting techniques, which creates double-stranded breaks and takes advantage on the cells natural homologous recombination repair systems, have been developed to target insertion to exact locations. Genome editing uses artificially engineered nucleases that create breaks at specific points. There are four families of engineered nucleases: meganucleases,[10][11] zinc finger nucleases,[12][13] transcription activator-like effector nucleases (TALENs),[14][15] and the Cas9-guideRNA system (adapted from CRISPR).[16][17] TALEN and CRISPR are the two most commonly used and each has its own advantages.[18] TALENs have greater target specificity, while CRISPR is easier to design and more efficient.[18] The development of the CRISPR-C
================================================================================
Rank = 57; Score = 3817472.0
<|begin_of_text|>Researchers at North Carolina State University have developed techniques that can be used to create ideal geometric phase holograms for any kind of optical pattern -- a significant advance over the limitations of previous techniques. The holograms can be used to create new types of displays, imaging systems, telecommunications technology and astronomical instruments.

A geometric phase hologram is a thin film that manipulates light. Light moves as a wave, with peaks and troughs. When the light passes through a geometric phase hologram, the relationship between those peaks and troughs is changed. By controlling those changes, the hologram can focus, disperse, reorient or otherwise modify the light.

An ideal geometric phase hologram modifies the light very efficiently, meaning that little of the light is wasted. But ideal geometric phase holograms can also produce three different, well-defined "wavefronts" -- or transformed versions of the light that passes through the thin film.

"We can direct light into any one or more of those three wavefronts, which allows us to use a single ideal geometric phase hologram in many different ways," says Michael Escuti, a professor of electrical and computer engineering at NC State and corresponding author of a paper on the work.

Previously, researchers were only able to make ideal geometric phase holograms in a limited set of simple patterns, curtailing their usefulness for new applications. This is because making these holograms involves orienting molecules or structures at a scale smaller than the wavelength of light.

"We've come up with two ways of making ideal geometric phase holograms that are relatively simple but allow us to control the orientation of the molecules that ultimately manipulate the light," Escuti says.

First, the researchers use lasers to create a high-fidelity light pattern, either by taking advantage of how waves of light interfere with each other, or by using a tightly focused laser to scan through a pattern -- much like a laser printer.

A photoreactive substrate records the light pattern, with each molecule in the substrate orienting itself depending on the polarization of the light it was exposed to. To understand this, think of a beam of light as a wavy string, traveling from left to right. That string is also vibrating up and down -- creating wiggles are that are perpendicular to the direction the string is traveling. Controlling the orientation angle of the light's linear polarization just means controlling the direction that the wave is wiggling.

The pattern that is recorded on the substrate then serves as a template for a liquid crystal layer that forms the finished hologram.

"Using these techniques,
================================================================================
Rank = 58; Score = 3751936.0
<|begin_of_text|>Definition from: All About Philosophy - Existentialism Existentialism is a 20th century termed philosophy concerned with human existence, finding self, and the meaning of life through free will, choice, and personal responsibility. The belief is that people are searching to find out who and what they are throughout life as they make choices based on their experiences, beliefs, and outlook. And personal choices become unique without the necessity of an objective form of truth. An existentialist believes that a person should be forced to choose and be responsible without the help of laws, ethnic rules, or traditions.Listed below are a number of existentially-themed films which I feel address the human condition in a profound, and ultimately, rewarding way. Gaining insight into ourselves and being able to express this with others through these films.In reverse chronological order and purely subjective.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 59; Score = 3751936.0
<|begin_of_text|>We think our destiny is to journey to Mars and beyond. Yet as we build our spacecraft, we're about to be broadsided - from a different direction - by the most explosive event in history.

Sometime in the future science will be able to create realities that we can't even begin to imagine. As we evolve, we'll be able to construct other information systems that correspond to other realities, universes based on logic completely different from ours and not based on space and time.

Immanuel Kant declared in 1781 that space and time were real, but only indeed as properties of the mind. These algorithms are not only the key to consciousness, but why space and time − indeed the properties of matter itself - are relative to the observer. But a new theory called biocentrism suggests that space and time may not be the only tools that can be used to construct reality. At present, our destiny is to live and die in the everyday world of up and down. But what if, for example, we changed the algorithms so that instead of time being linear, it was 3-dimensional like space? Consciousness would move through the multiverse. We'd be able to walk through time just like we walk through space. And after creeping along for 4 billion years, life would finally figure out how to escape from its corporeal cage. Our destiny would lie in realities that exist outside of the known physical universe.

Even science fiction is struggling with the implications. In "Avatar," human consciousness is infused into blue aliens that inhabit a wondrous world. However, according to biocentrism, replicating human intelligence or consciousness will require the same kind of algorithms for employing time and space that we enjoy. Everything we experience is a whirl of information occurring in our heads. Time is simply the summation of spatial states - much like the frames in a film - occurring inside the mind. It's just our way of making sense of things. There's also a peculiar intangibility to space. We can't pick it up and bring it to the laboratory. Like time, space isn't an external object. It's part of the mental software that molds information into multidimensional objects.

We take for granted how our mind puts everything together. When I woke up this morning, I was in the middle of a dream that seemed as real as everyday life. I remember looking out over a crowded port with people in the foreground. Further out, there were ships engaged in battle. And still further out to sea was a battleship with
================================================================================
Rank = 60; Score = 3735552.0
<|begin_of_text|>Sometimes bad things happen, and you must recover your desktop or server from an error.

For this delicate work there are some specialised Linux Live Distributions that once booted allow you to make a lot of tasks, like mounting and repairing disk partitions and file system, for this it’s important that the live Distro has support for many different File System type and LVM, it should also have a good support for the network, it’s possible that you need to download or move something on the computer, and so it’s also important to have a wide range of network support, wi-fi too maybe.

I’ve know Finnix because it’s the recovery solution choosed by my the hosting company that i use, Linode.

Finnix is a self-contained, bootable Linux CD distribution (“LiveCD”) for system administrators, based on Debian. You can mount and manipulate hard drives and partitions, monitor networks, rebuild boot records, install other operating systems, and much more. Finnix includes the latest technology for system administrators, with Linux kernel 3.0, x86 and PowerPC support, hundreds of sysadmin-geared packages, and much more. And above all, Finnix is small; currently the entire distribution is over 300MiB, but is dynamically compressed into a small bootable image. Finnix is not intended for the average desktop user, and does not include any desktops, productivity tools, or sound support, in order to keep distribution size low.

New versions of Finnix are released every 3 months on average, with updated software from the Debian “testing” tree, along with new Finnix-specific functionality, the latest release it’s version 102 released on 23 July 2011.

Features

This is a list of packages and features available on release 102

Linux 3.0 as kernel

Based on Debian testing (2011-07-21)

Support to many file systems: ext3/4, btrfs, Macintosh HFS, cifs, JFS, NTFS, NTFS-3G and XFS

Support to LVM and EVMS

A lot of network tools: tcpdump, ethtool, snmp tools, wirelesstools

A lot of other tools like wipe (to delete securely files)

And if you need for a specific tool or program you can check the online package list

GRML Grml is a bootable live system (Live-CD) based on Debian. Grml includes a collection of GNU/Linux software
================================================================================
Rank = 61; Score = 3702784.0
<|begin_of_text|>Image copyright Reuters Image caption Chinese Go player Ke Jie has lost two games to AlphaGo

Google's DeepMind AlphaGo artificial intelligence has defeated the world's number one Go player Ke Jie.

AlphaGo secured the victory after winning the second game in a three-part match.

DeepMind founder Demis Hassabis said Ke Jie had played "perfectly" and "pushed AlphaGo right to the limit".

Following the defeat, Ke Jie told reporters: "I'm a little bit sad, it's a bit of a regret because I think I played pretty well."

Image copyright Reuters Image caption Ke Jie eventually resigned

In Go, players take turns placing stones on a 19-by-19 grid, competing to take control of the most territory.

It is considered to be one of the world's most complex games, and is much more challenging for computers than chess.

Tea-making

AlphaGo has built up its expertise by studying older matches and playing thousands of games against itself.

The company says the eventual plan is to deploy its artificial intelligence "in areas of medicine and science".

Prof Noel Sharkey, a computer scientist at Sheffield University, said it is still a long way from creating a general intelligence.

"It is an incredible achievement and most experts thought an AI winning at Go was 20 years away so DeepMind is leading the field but this AI doesn't have general intelligence. It doesn't know that is playing a game and it can't make you a cup of tea afterwards."

Prof Nello Cristianini, from Bristol University, added: "This is machine learning in action and it proves that machines are very capable but it is not general intelligence. No-one has built that yet."

The types of intelligence exhibited by machines that are good at playing games are seen as very narrow. While they may produce algorithms that are useful in other fields, few think they are close to the all-purpose problem solving abilities of humans that can come up with good solutions to almost any problem they encounter.

Prof Cristianini added that while competition at a gaming level is fine, it should not govern how we view our relationship with intelligent machines going forward.

"We should focus on the good things that we can get out of them and be careful not to create situations in which we put ourselves in direct competition with machines."

Both experts agreed that such algorithms could be adapted to other fields, such as health care.

DeepMind has already begun working with the UK's national health service to develop apps and other tools for diagnosis.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 62; Score = 3702784.0
<|begin_of_text|>An example of the "ghosting" one sees when looking through the Eidos glasses. Photo: Team Eidos The Eidos glasses allow wearers to see the ghostly trail of objects in motion, which has applications ranging from education to entertainment. Photo: Team Eidos Eidos brings a theatrical flair to the world of wearable computing. Photo: Team Eidos The Eidos face mask allows users to focus on and amplify a single audio signal. Photo: Team Eidos Despite originating at an art school, the Eidos products feature interesting technical ideas like conducting sound through bone. Photo: Team Eidos Early experiments were focused on functionality at the expense of aesthetics, but do hint at the mask's faceted appearance. Photo: Team Eidos The functionality of the Eidos system could be expanded by developing new apps and could be incorporated into other devices. Photo: Team Eidos

Wearable computing is exploding. Google Glass has geeks atwitter, Oculus Rift is garnering great reviews from fanboys, and gadget blogs and Wall Street alike are waiting for another aluminum-clad icon to emerge from Apple HQ. However, innovation in this area isn't confined to billion-dollar companies or VC-funded startups A team of designers from the Royal College of Art are trying to put a new face on wearable computing—literally.

Their creation, Eidos, is a pair of masks that use off-the-shelf sensors and custom software to augment the vision and hearing of their wearers, conferring superhero-ish powers in the process. Technology in the face mask lets users isolate and amplify audio signals of their choosing, the glasses display "ghosted" images of objects in motion, and the unique look of the products will irresistibly draw the attention of any passersby.

>The glasses display "ghosted" images of objects in motion.

The look of these devices has a distinct art school feel, but Team Eigos—comprised of Tim Bouckley, Millie Clive-Smith, Mi Eun Kim and Yuta Sugawara, sees killer apps for many different types of users. "The vision device gives a similar effect to long-exposure photography, revealing otherwise hidden traces of movement, but it does this live," says Clive-Smith. "This is remarkable at revealing patterns and allows you to pick out the details of motion you wouldn't normally realize." Coaches could use this technology to help players improve their form in real time rather than waiting until after a game to correct bad habits. Choreographers could start adding special effects to their performances
================================================================================
Rank = 63; Score = 3670016.0
<|begin_of_text|>Renting Dumpsters Posted on December 5, 2018 | By STEVE Whenever people have a task, it is easy to generate waste and piling up garbage can be a main source of worry. This is not only with regards to taking up space but also in conditions of polluting the environment unnecessarily. Therefore, how perform these worries are taken by you aside? This can be done by opting to use 20 yard dumpsters philadelphia simply. These are offered by local rental businesses and are designed to alleviate the worries of coping with trash from your brain. There are many of these rental companies in the marketplace and as such, it ought not to be difficult so that you can get the same. The local rental businesses will make sure that you recycle your waste in an eco-friendly method and at the same period, make the process easy. By using these eco-friendly dumpsters, you not really just ensure that your encircling is clean and gives you the ideal life-style but it will also make sure that you enjoy great wellness. As a result, this will give you great endurance and boost your probabilities of taking pleasure in a treatment free of charge way of living. These can be utilized in different fronts and places such as churches, house owners, areas and additional businesses. For this good reason, they are known to provide all round services to all these combined groups. There are instances when you might have dangerous waste products and this provides the ideal chance that you can ensure that you get rid of the same in a secure way. Dumpsters may end up being availed in various designs seeing that good as sizes and this makes it all easy for individuals to help to make buys based on their requirements. Another main benefit connected with this type of waste materials disposal is usually the truth that the consumer can place it in an area of their choice. This makes it simple to make sure that the clean up process is simple and effective on your part as well. It is normally essential to take note that the rental companies are also accountable for making sure that they are eliminated from your property once you are completed using the same and they can adhere to your time routine. What is more, in many situations, these ongoing companies will adhere to certain safety procedures when they are putting the same on site. At this true point, it is important to note that dumpsters that are provided by professional local rental businesses are also the perfect way to eliminate waste that is produced on building sites whether it is green backyard, commercial or home keep waste. What is usually even more, this provides you
================================================================================
Rank = 64; Score = 3670016.0
<|begin_of_text|>PYNQ is an open-source project that makes it easy for you to design embedded systems using the Xilinx Zynq-7000 SoC using the Python language, associated libraries, and the Jupyter Notebook, which is a pretty nice, collaborative learning and development environment for many programming languages including Python. PYNQ allows you to exploit the benefits of programmable logic used together with microprocessors to build more capable embedded systems with superior performance when performing embedded tasks such as:

High frame-rate video processing

Hardware-accelerated algorithms

Real-time signal processing

High-bandwidth I/O

Low-latency control

Nearly every embedded system needs to run one or more such tasks. The programmable hardware on the Zynq SoC just makes this job a lot easier.

The PYNQ-Z1 based on the Xilinx Zynq Z-7020 SoC is the first dev board to support PYNQ and it just showed up on the Digilent Web site, listing for $229. (Digilent’s academic price for the PYNQ-Z1 is only $65!)

Digilent PYNQ-Z1 Dev Board

Here’s what’s on the PYNQ-Z1 board:

Xilinx Zynq Z-7020 SoC with a dual-core ARM Cortex-A9 MPCore processor running at 650MHz

512Mbytes of DDR3 SDRAM running at 1050MHz

16Mbytes of Quad-SPI Flash memory

A MicroSD slot for the PYNQ environment

USB OTG host port

USB programming port

Gigabit Ethernet port

Microphone

Mono audio output jack

HDMI input port

HDMI output port

Two Digilent PMOD ports

Arduino-format header for Arduino shields

Pushbuttons, switches, and LEDs

That’s a lot of board for $229—and it’s pink!

Here’s what PYNQ is, really: It’s for software developers and students who want to take advantage of the improved embedded performance made possible by the Zynq SoC’s programmable hardware without having to use ASIC-style (HDL) design tools to design hardware.

For even better performance, you can also program the PYNQ-Z1 with C or C++ using the Xilinx SDK software development environment, available in the no-cost Xilinx Vivado HL Design Suite WebPACK.

The PYNQ-Z1 and the PYNQ project make it possible to create a lot of really interesting systems, so just what are you waiting for?


================================================================================
Rank = 65; Score = 3670016.0
<|begin_of_text|>Campus-based technology courses can be expensive, which is why many people are choosing to take free technology courses online. Some of the best universities in the world offer free technology courses. Examples include Stanford University, the Massachusetts Institute of Technology and the Tokyo Institute of Technology.

1. The Massachusetts Institute of Technology

The Massachusetts Institute of Technology OpenCourseWare site is undoubtedly the best place to find free technology courses online. There are hundreds of courses to choose from, which means you can find a course on nearly any technology topic imaginable.

2. Delft University of Technology

The Delft University of Technology offers a number of free technology courses. Most of Delft's courses cover advanced topics that would be of interest to students in engineering or related fields. New courses are constantly being added to the site.

3. University of California, Berkeley

If you're looking for free technology courses that you can download to your MP3 player or computer, you'll definitely want to check out UC Berkeley on iTunes. Self-learners can gain access to a wide range of technology courses, lectures and events. Additional courses can be found though webcast.berkeley.edu.

4. Stanford University

Berkeley isn't the only university that makes free technology courses available through iTunes. Stanford University has a similar set-up through their Stanford iTunes project. There are quite a few technology courses and lectures that can be accessed for free already, and more are added on a weekly basis.

5. Rice University

Connexions, a Rice University organization, hosts a wide variety of scholarly content. The site offers hundreds of free technology courses. Some courses consist of small 'knowledge chunks.' Other courses include multiple modules, books and course notes.

6. The Open University

There are more than 50 free technology courses that can be taken through The Open University's online LearningSpace. Courses range from the introductory level to the advanced level and take anywhere from two hours to two weeks to complete.

7. Utah State University

Self-learners around the world can choose from more than a dozen free technology courses when they visit Utah State University's OpenCourseWare site. Technology topics include, but are not limited to: online communities, instructional technology, interactive learning and computer engineering.

8. University of Southern Queensland

The University of Southern Queensland offers two different courses that may be of interest to technology buffs. Both of the text-based courses are free and include materials that are practical for self-learners.

9. Dixie State College of Utah

The CIT Department at the
================================================================================
Rank = 66; Score = 3620864.0
<|begin_of_text|>More Info

Windows 10

Microsoft Windows 10 combines the best elements of the Windows you already know, like the Start menu, with new features like space to pin your favorite apps and easy navigation so you'll feel right at home. Getting set up is faster and easier so you can get right to work or exploring the Windows App Store games or a new version of Office. InstantGo quickly boots up or resumes Windows while enhancements help balance memory and processor resources for maximum efficiency and Battery Saver automatically conserves power. Windows 10 also offers more built-in security features improving protection against viruses, phishing, and malware.

Solid State Drive

More reliable and significantly faster than traditional spinning-platter hardrives, solid state drives work more like a large flash drive giving you quick access to your data. With no moving parts generating heat, solid state drives use less power and keep your system cooler which helps reduce component failure. Light weight and durable, solid state drives are often found in portable devices since they are less prone to travel damage and accidents like being dropped.

1TB Hard Drive

A terabyte equals one trillion bytes. To put that in perspective, that's room enough for about 250,000 MP3 files, or according to Ron White, in his book, "How Computers Work", 20,000 four-drawer filing cabinets filled with text. In addition, this hard drive operates at a variable RPM which means you've got the best of both worlds, full speed for demanding loads and energy conservation during minimal access.

Smart Cache

With Intel Smart Cache you benefit from increased data access because the cache is shared between the cores from a single access point and optimized by workload demand. That means your system is making maximum use of its resources, enhancing multi-threading, and reducing storage redundancy.

Intel® Quick Sync Video

Built into every 2nd generation Intel Core™ processor, Quick Sync Video enables users to quickly create, edit, synchronize, and share video files from home or make them available online without the need for extra hardware. In addition, media conversion between devices takes just minutes thanks to hardware acceleration streamlining this process at incredible speed. That includes creating DVDs, Blu-ray™ discs, or editing and converting HD videos for portable media devices or uploading to your favorite social media sites.

Intel InTrue™ 3D Technology

The perfect way to watch 3D movies. InTrue 3D technology delivers 1080p, hi-def resolution 3D action to your TV using HDMI 1.4. With Blu
================================================================================
Rank = 67; Score = 3604480.0
<|begin_of_text|>Guidelines for a better Web User Experience

Although a user’s experience on a website is largely subjective, there are several things you can do to ensure that visitors return to your site.

A great user experience can result in increased customer loyalty, higher rates of conversion, and a decrease in abandon rates and customer service requests. It improves the quality of a user’s interactions with your products and services, and it enhances their perception of your organization. A good user experience will ensure that users find and recognize the value in what they are being offered and provided.

To ensure that visitors get the best experience possible, it’s necessary to understand their needs and values, and the abilities and limitations your site gives them as users. You can learn this by working closely with your clients to gain a better understanding of their business environment. When you understand your client’s business, you can come to know what type of users your website or app should cater to.

Very responsive, delivered what was asked of them. Great communication. I am happy that I picked AllianceTek! They produce. - Jason Corbin (Co-Founder)

strApp in Apps, LLC

New York, NY

Call Christopher Dabhi at 484-892-5713 now to schedule your free consultation with AllianceTek.

The design of your website is the most important factor to a user. A solid design incorporates the principles of human-computer interaction (HCI) and facilitates easy and intuitive user navigation by employing the– your website or app should provide iconic representation of its facets and use white space efficiently for readability and a comfortable aesthetic.The design should be. In an era where new devices and operating systems seem to be released every day, a single site with multiple capabilities to function on desktops, tablets, mobile phones, and other web-accessible devices is sorely needed. The most popular framework for developing responsive websites is Bootstrap.is also key to a good user experience. For a user to feel engaged, your website or app should feature calls to action, search prompts, and easy navigation. Interactivity can be enhanced through audio or video media, hover states, scroll events, and sliding interfaces.It’s also important to keep user clicks to a. You should cut down on tasks or actions required by users to create a simplified and streamlined experience that lets them find the content they need without spending excess time clicking through other pages to get there.that you provide on your website is also very important. A web page should not be verbose. Think of each page as a paragraph in an essay, where each section
================================================================================
Rank = 68; Score = 3604480.0
<|begin_of_text|>But more important for the students, the various A-Lab projects served as a concrete learning experience on what data science is all about, - how to leverage messy, incomplete, real-world data to shed light on a complex and not-so-well-defined problem.

A-Lab received over 20 project proposals from different companies, of which 13 were selected by the students. Each project team was assigned a research mentor to provide guidance as appropriate. I mentored a 3-student team that worked on a project sponsored by MasterCard. The students explored the possibility of improving on predictions of the economic performance of emerging markets by coupling existing economic indicators data with consumer behavior based on MasterCard’s transaction data. This is a particularly interesting project because economic data in emerging markets is often not as reliable as the data in more advanced markets.

This past semester I was involved in an interesting course at MIT’s Sloan School of Management, - Analytics Labs (A-Lab). A-Lab’s objective is to teach students how to use data sets and analytics to address real-world business problems. Companies submit project proposals prior to the start of the class, including the business problem to be addressed and the data on which the project will be based. Students are then matched with the project they’re most interested in and grouped into teams of 3-4 students.

A-Lab is taught by professors Erik Brynjolfsson and Sinan Aral. The course was first given in 2014, so this is only its second year. The 2015 Syllabus offers a good overview of the class, including the various companies that submitted project proposals.

Projects are considered confidential unless the companies involved give permission to talk about them publicly,

as several did in 2014.

Amazon, for example, sponsored a project on how to raise the share of wallet of Amazon Prime customers, based on the analysis of over 200 million anonymized data points. And IBM sponsored a project to uncover a potential Watson application. The students recommended using Watson as a kind-of regulatory analyst assistance, to help financial institutions better understand how to comply with the over 1700 pages of regulations in the Dodd-Frank legislation.

The 2015 A-Lab class culminated with short presentations of each of the student projects before an audience that included company sponsors and mentors. As I listened to the 13 presentations, I was impressed by the potential of applying big data and analytics to all kinds of business problems, from the tactical decisions every business makes as part of its normal operations to the strategic
================================================================================
Rank = 69; Score = 3588096.0
<|begin_of_text|>Fetch is the native AJAX API to replace jQuery.get()

As a term AJAX has been around for over a decade now. While many still relate to it as rich and fluid interfaces, the real deal is the possibility of making asynchronous requests to the server from the browser.

As browsers evolved the XMLHttpRequest API used for AJAX did not not evolve. This is why many developers still rely on including the whole jQuery library just to abstract asynchronous HTTP Request. jQuery has a great and simple to use API for many things, but including it just for AJAX is overkill.

Finally the Web API has a new native alternative available. This is known as the Fetch API and offers a very simple way of getting content from the server with JavaScript in the browser. The API is a high level one that has a generic definition of HTTP Request and Response objects.

To anyone used to working with the HTTP, the concepts are immediately familiar:

GlobalFetch : Contains the fetch() method used to fetch a resource.

: Contains the fetch() method used to fetch a resource. Headers : Represents response/request headers, allowing you to query them and take different actions depending on the results.

: Represents response/request headers, allowing you to query them and take different actions depending on the results. Request : Represents a resource request.

: Represents a resource request. Response: Represents the response to a request.

In addition to standard features developers would expect, the Fetch API definition handles closely related concepts such as CORS (Cross-origin resource sharing) and HTTP origin headers. The fetch resouce is available in the browser's standard window object and has only a single mandatory argument - the request URL.

The fetch method returns a JavaScript promise that makes it convenient to work with the asynchronous nature of HTTP requests in the browser context. Once the response is received, there are a number of methods available in the object.

Browser support for the Fetch API is very good with evergreen browsers such as Chrome, Edge, Firefox and Opera already supporting the feature. Internet Explorer does not support the API, but there is a fetch API polyfill that can be used for it.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 70; Score = 3555328.0
<|begin_of_text|>“There will come a time when it isn't ‘They’re spying on me through my phone’ anymore. Eventually, it will be ‘My phone is spying on me.’” ― Philip K. Dick

If ever Americans sell their birthright, it will be for the promise of expediency and comfort delivered by way of blazingly fast Internet, cell phone signals that never drop a call, thermostats that keep us at the perfect temperature without our having to raise a finger, and entertainment that can be simultaneously streamed to our TVs, tablets and cell phones.

Likewise, if ever we find ourselves in bondage, we will have only ourselves to blame for having forged the chains through our own lassitude, laziness and abject reliance on internet-connected gadgets and gizmos that render us wholly irrelevant.

Indeed, while most of us are consumed with our selfies and trying to keep up with what our so-called friends are posting on Facebook, the megacorporation Google has been busily partnering with the National Security Agency (NSA), the Pentagon, and other governmental agencies to develop a new “human” species, so to speak.

In other words, Google—a neural network that approximates a global brain—is fusing with the human mind in a phenomenon that is called “singularity,” and they’ve hired transhumanist scientist Ray Kurzweil to do just that. Google will know the answer to your question before you have asked it, Kurzweil said. “ It will have read every email you will ever have written, every document, every idle thought you’ve ever tapped into a search-engine box. It will know you better than your intimate partner does. Better, perhaps, than even yourself.”

But here’s the catch: the NSA and all other government agencies will also know you better than yourself. As William Binney, one of the highest-level whistleblowers to ever emerge from the NSA said, “ The ultimate goal of the NSA is total population control.”

Science fiction, thus, has become fact.

We’re fast approaching Philip K. Dick’s vision of the future as depicted in the film Minority Report. There, police agencies apprehend criminals before they can commit a crime, driverless cars populate the highways, and a person’s biometrics are constantly scanned and used to track their movements, target them for advertising, and keep them under perpetual surveillance.

Cue the dawning of the Age of the Internet of Things, in which internet-connected “things” will monitor your home, your health and your habits
================================================================================
Rank = 71; Score = 3555328.0
<|begin_of_text|>Radical Servers

Anti-capitalist, anti-hierarchy, autonomous, feminist, or radical server projects, revolutionary collectives which provide free or mutual aid services to radical and grassroots activists.

Want to update your collective’s information or want to be listed here? Edit our repo.

World Wide

Hackerspaces are community-operated physical places, where people can meet and work on their projects. This website is for Anyone and Everyone who wants to share their hackerspace stories and questions with the global hackerspaces community.

indymedia.org is a decentralized global network of media activists.

News feeds

Tachanka is the idea of providing technical services to emancipatory projects and groups of political change. It is also the groups of people behind this idea. There are many overlapping collectives that help to maintain the projects, but Tachanka operates within many shared principles, forming a supportive network to enable shared aims. The collectives are international, seeking to enhance global solidarity. The foremost guiding principles of the Tachanka project are the Hallmarks of People’s Global Action and the Debian Social Contract.

Mailing lists

Virtual servers

DNS caching

caching Drupal farms

English / French / Portuguese language

Take Back The Tech!

Take Back The Tech! is a global campaign that connects the issue of violence against women and information and communications technology (ICT). It aims to raise awareness on the way violence against women is occurring on ICT platforms such as the Internet and mobile phones, and to call for people to use ICT in activism to end violence against women. It was initiated by the Association for Progressive Communications (apc.org), Women’s Networking Support Programme, in 2006. Since then, the campaign has been taken up and organised by individuals, collectives and non-governmental organizations in at least 24 countries. More at Wikipedia

Telecentre

Telecentre is a widely adopted concept to provide free access to the internet and other services at puplic spaces with computers and other digital technologies that enable them to gather information, create, learn, and communicate with others while they develop essential digital skills. Guidebook for Managing Telecentre Networks

South America

Codigosur

Codigosur is a collective of activists from different social movements in Latin America for collaboration and the development of communication tools, culture and free technology

Hosting

Email

Lists

Mumble

Streaming

TLS

Etherpad

DNS and more

Colnodo

Colnodo is a Colombian non-profit organization that aims to facilitate communication and the exchange of information, experience, and
================================================================================
Rank = 72; Score = 3489792.0
<|begin_of_text|>network scanner

Nmap (Network Mapper) is a free and open-source network scanner created by Gordon Lyon (also known by his pseudonym Fyodor Vaskovich).[3] Nmap is used to discover hosts and services on a computer network by sending packets and analyzing the responses.

Nmap provides a number of features for probing computer networks, including host discovery and service and operating system detection. These features are extensible by scripts that provide more advanced service detection,[4] vulnerability detection,[4] and other features. Nmap can adapt to network conditions including latency and congestion during a scan.

Nmap started as a Linux utility[5] and was ported to other systems including Windows, macOS, and BSD.[6] Linux is the most popular platform, followed by Windows.[7]

Features [ edit ]

Nmap features include:

Host discovery – Identifying hosts on a network. For example, listing the hosts that respond to TCP and/or ICMP requests or have a particular port open.

Port scanning – Enumerating the open ports on target hosts.

Version detection – Interrogating network services on remote devices to determine application name and version number. [8]

OS detection – Determining the operating system and hardware characteristics of network devices.

Scriptable interaction with the target – using Nmap Scripting Engine[9] (NSE) and Lua programming language.

Nmap can provide further information on targets, including reverse DNS names, device types, and MAC addresses.[10]

Typical uses of Nmap:

Auditing the security of a device or firewall by identifying the network connections which can be made to, or through it. [11]

Identifying open ports on a target host in preparation for auditing. [12]

Network inventory, network mapping, maintenance and asset management.

Auditing the security of a network by identifying new servers. [13]

Generating traffic to hosts on a network, response analysis and response time measurement. [14]

Finding and exploiting vulnerabilities in a network. [15]

DNS queries and subdomain search

User interfaces [ edit ]

NmapFE, originally written by Zach Smith, was Nmap's official GUI for Nmap versions 2.2 to 4.22.[16] For Nmap 4.50 (originally in the 4.22SOC development series) NmapFE was replaced with Zenmap, a new official graphical user interface based on UMIT, developed by Adriano Monteiro Marques.

Various web-based interfaces allow controlling Nmap or analysing Nmap results from a
================================================================================
Rank = 73; Score = 3489792.0
<|begin_of_text|>Your face is quickly becoming a key to the digital world. Computers, phones, and even online stores are starting to use your face as a password. But new research from Carnegie Mellon University shows that facial recognition software is far from secure.

In a paper (pdf) presented at a security conference on Oct. 28, researchers showed they could trick AI facial recognition systems into misidentifying faces—making someone caught on camera appear to be someone else, or even unrecognizable as human. With a special pair of eyeglass frames, the team forced commercial-grade facial recognition software into identifying the wrong person with up to 100% success rates. Researchers had the same success tricking software touted by Chinese e-commerce giant Alibaba for use in their “smile-to-pay” feature.

Modern facial recognition software relies on deep neural networks, a flavor of artificial intelligence that learns patterns from thousands and millions of pieces of information. When shown millions of faces, the software learns the idea of a face, and how to tell different ones apart.

Carnegie Mellon University Top images show the real person, and the bottom images show the identity chosen by facial recognition.

As the software learns what a face looks like, it leans heavily on certain details—like the shape of the nose and eyebrows. The Carnegie Mellon glasses don’t just cover those facial features, but instead are printed with a pattern that is perceived by the computer as facial details of another person.

In a test where researchers built a state-of-the-art facial recognition system, a white male test subject wearing the glasses appeared as actress Milla Jovovich with 87.87% accuracy. An Asian female wearing the glasses tricked the algorithm into seeing a Middle Eastern man with the same accuracy. Other notable figures whose faces were stolen include Carson Daly, Colin Powell, and John Malkovich. Researchers used about 40 images of each person to generate the glasses used to identify as them.

The test wasn’t theoretical—the CMU printed out the glasses on glossy photo paper and wore them in front of a camera in a scenario meant to simulate accessing a building guarded by facial recognition. The glasses cost $.22 per pair to make. When researchers tested their glasses design against a commercial facial recognition system, Face++, who has corporate partners like Lenovo and Intel and is used by Alibaba for secure payments, they were able to generate glasses that successfully impersonated someone in 100% of tests. However, this was tested digitally—the researchers edited the glasses onto a picture, so in the real world the success rate could be less
================================================================================
Rank = 74; Score = 3457024.0
<|begin_of_text|>Contractors' All Risks (CAR) Insurance Definition

What is 'Contractors' All Risks (CAR) Insurance'

Contractors' All Risks (CAR) insurance is an insurance policy that provides coverage for both damage to a property and third-party injury or damage claims.

Types of Risk in Construction Project

Damage to the property

Third-parties Damage

Why Contractors' All Risks (CAR) Insurance

Contractors' All Risks Insurance Coverage

Tags:

contractors liability insurance engineer insurance, engineering insurance, professional liability insurance cost, professional liability insurance, contractor insurance, contractor liability insurance, contractors all risk insurance, engineers insurance, insurance for engineers, professional engineer insurance, professional indemnity insurance, professional insurance, contractors insurance

In other words Contractors' All Risks (CAR) insurance is a property insurance by which any building or civil engineering project under construction is protected against accidents which may lead to physical damage to or the destruction of works in progress or materials brought to the site.Contractors' all risk (CAR) insurance policies are considered non-standard insurance policies.Construction projects typically involve two primary types of risk: damage to the property, and third-party claims of injury or damage.Damage to the property could include the structure as a result of poor construction, or damage during a renovation.Third-parties, including injuries subtend by subcontractor while working in the engineering project site. Contractors' all risk (CAR) insurance bridges these two risks into a common policy, and helps cover the gap between exclusions that would otherwise exist when using separate policies.BREAKING DOWN 'Contractors' All Risks (CAR) Insurance' CAR insurance is typically taken out jointly by both the contractor and the employer, with other parties such as financing companies having the option of being named to the policy. Because multiple parties are included in the policy they each retain the right to file a claim against the insurer, although all parties also have the duty of informing the insurer of any injuries and damages that may result in a claim.Aim and objective of CAR insurance policy is to ensure the protection of all parties involves in a project, regardless of the type of damage to the property or who caused the damage. Insurers who underwrite this type of policy lose the right to subrogation, meaning that if it pays out funds to one party in the contract then it cannot seek to recover those funds from another party in the contract. For example, if the owner of a large building and the contractor working on the building are on the same CAR policy, any costs
================================================================================
Rank = 75; Score = 3457024.0
<|begin_of_text|>EDMONTON - Press "enter," dealer — scientists have taught a computer how to play unbeatable poker.

While the news may sadden the hearts of rec-room card sharps everywhere, the winners in this game are programmers trying to do everything from improve public security to help doctors treat patients with diabetes.

"We should be able to use these algorithms in any well-defined problem," said Michael Bowling, the University of Alberta computer scientist who co-authored a paper in the journal Science that details how the program for two-handed, fixed-bet Texas Hold 'Em can't do worse than break even.

Scientists in the field of game theory long ago taught computers to play games such as checkers and chess. But poker has remained elusive because it's a so-called "imperfect information" game. A player has to make decisions without knowing all the data such as what the other player is holding.

"This game has been, historically, an important challenge problem," Bowling said. "Poker is one of the games that really motivated the whole founding of the field of game theory back in the '20s."

Bowling's team made its breakthrough by refining a previously developed technique called counterfactual regret minimization that allows a computer to look back at previous hands and learn from its mistakes. Although that sounds similar to how humans improve, the computer used here became a one-player Las Vegas.

"It spent two months playing billions and billions of hands of poker against itself to find the perfect strategy," said Bowling. "The strategy is 1,000 times larger than all the English-language Wikipedia."

It's unlikely to be of much use at anyone's Saturday night game.

"You have to memorize a 10-terabyte table of probabilities."

A terabyte is one byte followed by 12 zeros.

But the point was never to become an unbeatable online poker star. The same process that taught the computer when to hold 'em and when to fold 'em can be transferred to any problem with well-defined rules and outcomes, many options and imperfect information — terrorist security, for example.

"We run patrols, we do searches — we have these tools at our disposal, but how do we deploy them? We want to find a strategy that's unbeatable.

"What we've done is shown that we can do these game theoretic analyses at a scale that hasn't been done before — at a really enormous complexity. That means that we can start looking at problems in that security sphere."

Game theory is already being used to help schedule air marshals on commercial flights in the
================================================================================
Rank = 76; Score = 3424256.0
<|begin_of_text|>ARMONK, NY - 28 Aug 2012: IBM (NYSE: IBM) today announced the zEnterprise® EC12 mainframe server, the most powerful and technologically advanced version of an IBM system that has been the linchpin of enterprise computing for 48 years. The new enterprise system features technologies that demonstrate IBM’s ongoing commitment to meet the growing need to secure and manage critical information with the System z mainframe.

Mainframes support significant portions of the data environment at most large enterprises. As these enterprises grapple with the well-documented growth of data, they are looking for new ways to secure and gain insights from such critical information as financial, customer and enterprise resource data that will enable them to provide their clients with new services. The new zEC12 offers industry-leading security and robust support for operational analytics that can help clients efficiently sift through large volumes of raw data and transform it to gain knowledge that can be used for competitive advantage. For example, a retailer managing online transactions on zEC12 can gain insights from client information that will enable it to provide clients with a more customized shopping experience.

The IBM zEC12 enterprise system is the result of an investment by IBM Systems and Technology Group of more than $1 billion in IBM research and development primarily in Poughkeepsie, New York as well as 17 other IBM labs around the world and in collaboration with some of IBM's top clients.

The new IBM mainframe is one of the most secure enterprise systems ever (1), with built-in security features designed to meet the security and compliance requirements of different industries. With operational analytics and near real-time workload monitoring and analysis, clients can use the new zEC12 for a variety of workloads, including hybrid clouds that can take advantage of the System's 25% more performance per core and 50% greater total system capacity than its predecessor as well as the world’s fastest chip running at 5.5 GHz (2).

Ultimate Security for Critical Information

IBM System z is a leading platform for secure data serving and the only commercial server to achieve Common Criteria Evaluation Assurance Level 5+ security classification, providing clients the confidence to run many different applications containing confidential data on a single mainframe. The new zEC12 builds on this with innovative security and privacy features to help protect data at rest or in flight -- a critical capability in the age of Internet banking and mobile devices.

zEC12 includes a state-of-the-art, tamper-resistant cryptographic co-processor called Crypto Express4S that provides privacy for
================================================================================
Rank = 77; Score = 3407872.0
<|begin_of_text|>The first piece of software to show the potential of quantum computing has finally been run on a real machine, 20 years after it was initially dreamed up. Although it doesn’t do anything useful on its own, implementing the algorithm could lead to more practical computers powered by the strange properties of quantum mechanics.

Quantum computers should be much faster than ordinary ones, but only at tasks for which there is a quantum algorithm – software that takes advantage of the computer’s quantum nature. Without these algorithms, quantum computers are just regular computers that are much harder to build.

One of the best-known pieces of quantum software is Shor’s algorithm, which factorises large numbers into their prime components – a notoriously slow and difficult problem to solve classically. Shor’s algorithm has been run in a limited way using photons sent through the air and on silicon chips – but a full-blown quantum computer capable of running it could threaten online encryption, which relies on large primes.

Designing an algorithm that takes advantage of a quantum computer is tricky, so there aren’t many around. In 1994, Daniel Simon, then at the University of Montreal, Canada, came up with one of the earliest examples. Crucially, his was the first that showed a quantum computer could solve a problem exponentially faster than an ordinary computer. Previous algorithms had only shown a slight speed boost, or none at all.

Advertisement

Sceptical

Simon was a quantum computing sceptic, but in attempting to prove they would never be useful, he stumbled across a problem that showed the exact opposite. Imagine you feed a string of bits, like 0101, into a black box and get another string, like 1100, out in return. There are a finite number of possible outputs, but you don’t know how the black box produces them. Simon’s problem asks: does the black box give a unique output for every possible input, or do some inputs give a common output? The problem doesn’t show up in any real-world applications, but Simon’s algorithm for solving it inspired the more useful Shor’s algorithm and the field of quantum computing as a whole.

“It has a kind of special place in the history of the development of quantum algorithms,” says Mark Tame at the University of KwaZulu-Natal in Durban, South Africa. “However, despite being the first to show that an exponential gap exists, it was surprisingly never experimentally realised in all the years since.”

That’s why Tame and his colleagues have now run Simon’s algorithm for the first
================================================================================
Rank = 78; Score = 3407872.0
<|begin_of_text|>For a broader coverage of forks, see Fork (blockchain)

Bitcoin forks are defined variantly as changes in the protocol of the bitcoin network or as the situations that occur "when two or more blocks have the same block height".[1] A fork influences the validity of the rules. Forks are typically conducted in order to add new features to a blockchain, to reverse the effects of hacking or catastrophic bugs. Forks require consensus to be resolved or else a permanent split emerges.

Forks of the client software

The following are forks of the software client for the bitcoin network:

All three software clients attempt to increase transaction capacity of the network. None achieved a majority of the hash power.[2]

Intended hard forks splitting the cryptocurrency

Hard forks splitting bitcoin (aka "split coins") are created via changes of the blockchain rules and sharing a transaction history with bitcoin up to a certain time and date. The first hard fork splitting bitcoin happened on 1 August 2017, resulting in the creation of Bitcoin Cash.

The following is a list of hard forks splitting bitcoin by date and/or block:

Bitcoin Cash: Forked at block 478558, 1 August 2017, for each bitcoin (BTC), an owner got 1 Bitcoin Cash (BCH)

Bitcoin Gold: Forked at block 491407, 24 October 2017, for each BTC, an owner got 1 Bitcoin Gold (BTG)

Bitcoin SV: Forked at block 556766, 15 November 2018, for each Bitcoin Cash (BCH), an owner got 1 Bitcoin SV (BSV).

Intended soft forks splitting from not-most-work block

The fork fixing the value overflow incident was controversial because it was announced after the exploit was mined.

Unintended hard forks

Two hard forks were created by "protocol change" definition:

March 2013 Chain Fork (migration from BerkeleyDB to LevelDB caused a chain split) [3]

CVE-2018-17144 (Bitcoin 0.15 allowed double spending certain inputs in the same block. Not exploited)<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 79; Score = 3407872.0
<|begin_of_text|>Our next workshops will be in Auckland 23rd to 25th September 2016. It will be run by Leilani Smiler and Mariano Perdroza. Please visit our workshops page for more information.

this link to David Berceli’s website for an excellent summary of how TRE has helped others. Followto David Berceli’s website for an excellent summary of how TRE has helped others.

CTV interview with Dr Berceli in Christchurch:

Click on the link to see an interview with Dr Berceli at workshops held in Christchurch (Dec 2011).

Below is a 6 minute overview of TRE from a documentary shown on South African Medical TV.

What is TRE?

TRE® is a purely body based, physical process. There is no counseling, or ‘talk therapy’ aspect to it.

TRE® (Tension & Trauma Release Exercises) is series of 7 exercises that assist the body in releasing deep muscular patterns of tension. Created by Dr. David Berceli, PhD, TRE® safely activates a natural reflex mechanism of shaking or vibrating that releases muscular tension, calming down the nervous system.

TRE®’s reflexive muscle vibrations generally feel pleasant and soothing. After doing TRE®, many people report feelings of peace and well-being.

An explanation of TRE by Dr David Berceli

TRE NZ is a Franchise of TRE for All, a Not for Profit Organisation, based in the USA and founded by Dr David Berceli, PhD. All Providers of TRE listed on this website have completed the Certification Process designed and delivered through TRE for All.

This technology is not intended to diagnose, treat, cure, or prevent any disease.

Medical advice must only be obtained from a physician or qualified health practitioner.

Results may vary between individuals.

There are no guarantees, expressed, or implied.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 80; Score = 3325952.0
<|begin_of_text|>Best Wild Game Cookbooks – Venison, Turkey, Moose, Elk, Fish, and other Wild Game Cookbooks

Cooking wild game after a successful hunt is not always the easiest thing to master. The most common mistake is simply treating wild game like we would beef, chicken, or pork, which are so prevalent in our diets. Wild game needs special care, but when you take a little time to learn how to prepare, the rewards are well worth it.

Wild Game Cookbooks

Buck, Buck, Moose: Recipes and Techniques for Cooking Deer, Elk, Moose, Antelope and Other Antlered Things

This is not your father’s venison cookbook. Buck, Buck, Moose is the first comprehensive, lushly photographed, full-color guide to working with and cooking all forms of venison, including deer, elk, moose, antelope and caribou. Buck, Buck, Moose will take you around the world, from nose to tail. The book features more than 100 recipes ranging from traditional dishes from six continents to original recipes never before seen. You’ll also get thorough instructions on how to butcher, age and store your venison, as well as how to use virtually every part of the animal. Buck, Buck, Moose also includes a lengthy section on curing venison and sausage-making. Peppered throughout are stories of the hunt and essays on why venison holds such a special place in human society. Venison is far more than mere food. It is, in many ways, what made us human.

The Complete Guide to Hunting, Butchering, and Cooking Wild Game: Volume 1: Big Game

This invaluable book includes

• recommendations on what equipment you will need—and what you can do without—from clothing to cutlery to camping gear to weapons

• basic and advanced hunting strategies, including spot-and-stalk hunting, ambush hunting, still hunting, drive hunting, and backpack hunting

• how to effectively use decoys and calling for big game

• how to find hunting locations, on both public and private land, and how to locate areas that other hunters aren’t using

• how and when to scout hunting locations for maximum effectiveness

• basic information on procuring hunting tags, including limited-entry “draw” tags

• a species-by-species description of fourteen big-game animals, from their mating rituals and preferred habitats to the best hunting techniques—both firearm and archery—for each species

• how to plan and pack for backcountry hunts

• instructions on how to break
================================================================================
Rank = 81; Score = 3309568.0
<|begin_of_text|>The speed of sound is the distance travelled per unit time by a sound wave as it propagates through an elastic medium. At 20 °C (68 °F), the speed of sound in air is about 343 meters per second (1,234.8 km/h; 1,125 ft/s; 767 mph; 667 kn), or a kilometre in 2.9 s or a mile in 4.7 s. It depends strongly on temperature, but also varies by several meters per second, depending on which gases exist in the medium through which a soundwave is propagating.

The speed of sound in an ideal gas depends only on its temperature and composition. The speed has a weak dependence on frequency and pressure in ordinary air, deviating slightly from ideal behavior.

In common everyday speech, speed of sound refers to the speed of sound waves in air. However, the speed of sound varies from substance to substance: sound travels most slowly in gases; it travels faster in liquids; and faster still in solids. For example, (as noted above), sound travels at 343 m/s in air; it travels at 1,480 m/s in water (4.3 times as fast as in air); and at 5,120 m/s in iron (about 15 times as fast as in air). In an exceptionally stiff material such as diamond, sound travels at 12,000 metres per second (27,000 mph);[1] (about 35 times as fast as in air) which is around the maximum speed that sound will travel under normal conditions.

Sound waves in solids are composed of compression waves (just as in gases and liquids), and a different type of sound wave called a shear wave, which occurs only in solids. Shear waves in solids usually travel at different speeds, as exhibited in seismology. The speed of compression waves in solids is determined by the medium's compressibility, shear modulus and density. The speed of shear waves is determined only by the solid material's shear modulus and density.

In fluid dynamics, the speed of sound in a fluid medium (gas or liquid) is used as a relative measure for the speed of an object moving through the medium. The ratio of the speed of an object to the speed of sound in the fluid is called the object's Mach number. Objects moving at speeds greater than Mach1 are said to be traveling at supersonic speeds.

History [ edit ]

Sir Isaac Newton computed the speed of sound in air as 979 feet per second
================================================================================
Rank = 82; Score = 3309568.0
<|begin_of_text|>The November/December issue of acmqueue is out now

Subscribers and ACM Professional members login here

PDF

May 11, 2012

Volume 10, issue 5

Modeling People and Places with Internet Photo Collections

Understanding the world from the sea of online photos

David Crandall, School of Informatics and Computing, Indiana University

Noah Snavely, Department of Computer Science, Cornell University

Computational photography often considers sets of photos taken by a single user in a single setting, but the popularity of online social media sites has created a social aspect to photo collections as well. Photo-sharing sites such as Flickr and Facebook contain vast amounts of latent information about our world and human behavior. Our recent work has involved building automatic algorithms that analyze large collections of imagery in order to understand and model people and places at a global scale. Geotagged photographs can be used to identify the most photographed places on Earth, as well as to infer the names and visual representations of these places. At a local scale, we can build detailed three-dimensional models of a scene by combining information from thousands of two-dimensional photographs taken by different people and from different vantage points. One key representation for many of these tasks is a network: a graph linking photos by visual similarity or other measures.

This article describes our work in using online photo collections to reconstruct information about the world and its inhabitants at both global and local scales. This work has been driven by the dramatic growth of social content-sharing Web sites, which have created immense online collections of user-generated visual data. Flickr.com alone currently hosts more than 6 billion images taken by more than 40 million unique users,11 while Facebook.com has said it grows by nearly 250 million photos every day.20

While users of these sites are primarily motivated by a desire to share photos with family and friends, collectively they are generating vast repositories of online information about the world and its people. Each of these photos is a visual observation of what a small part of the world looked like at a particular point in time and space. It is also a record of where a particular person (the photographer) was at a moment in time and what he or she was paying attention to.

In aggregate, and in combination with nonvisual metadata available on photo-sharing sites (including photo timestamps, geotags, captions, user profiles, and social contacts), these billions of photos present a rich source of information about the state of the world and the behavior of its people. We can thus imagine extending the
================================================================================
Rank = 83; Score = 3293184.0
<|begin_of_text|>"MOND" redirects here. For other uses, see Mond

Modified Newtonian dynamics (MOND) is a theory that proposes a modification of Newton's laws to account for observed properties of galaxies. It is an alternative to the theory of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.

Created in 1982 and first published in 1983 by Israeli physicist Mordehai Milgrom,[1] the theory's original motivation was to explain that the velocities of stars in galaxies were observed to be larger than expected based on Newtonian mechanics. Milgrom noted that this discrepancy could be resolved if the gravitational force experienced by a star in the outer regions of a galaxy was proportional to the square of its centripetal acceleration (as opposed to the centripetal acceleration itself, as in Newton's second law), or alternatively if gravitational force came to vary inversely with radius (as opposed to the inverse square of the radius, as in Newton's law of gravity). In MOND, violation of Newton's laws occurs at extremely small accelerations, characteristic of galaxies yet far below anything typically encountered in the Solar System or on Earth.

Unsolved problem in physics:

What is the nature of dark matter? Is it a particle, or do the phenomena attributed to dark matter actually require a modification of the laws of gravity? (more unsolved problems in physics)

MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, MOND has successfully predicted a variety of galactic phenomena that are difficult to understand from a dark matter perspective.[2][3] However, MOND and its generalisations do not adequately account for observed properties of galaxy clusters, and no satisfactory cosmological model has been constructed from the theory.

The accurate measurement of the speed of gravitational waves compared to the speed of light in 2017 ruled out many theories which used modified gravity to explain dark matter.[4] However, both Milgrom’s bi-metric formulation of MOND and nonlocal MOND are not ruled out according to the same study.

Overview [ edit ]

[5] Comparison of the observed and expected rotation curves of the typical spiral galaxy M33

Several independent observations point to the fact that the visible mass in galaxies and galaxy clusters is insufficient to account for their dynamics, when analysed using Newton's laws. This discrepancy – known as
================================================================================
Rank = 84; Score = 3276800.0
<|begin_of_text|>What do the San Francisco Giants, Cryptolocker and nuclear war all have in common? They all involve conflicts in which incentives, payouts and winning strategies can be analyzed with game theory. Game theory is a branch of mathematics that models conflict and cooperation between parties and is used in many real-world decision making scenarios, inside and outside the Information Security field. Game theory is particularly useful in analyzing the extortionist / victim dynamic present in ransomware infection scenarios.

Ransomware comes in many varieties and works in different ways, but the basic setting is the same: cybercriminals infect a computer with malicious software that blocks access to the system or important files until the ransom is paid.

The conventional wisdom in information security regarding ransomware is to never pay. But, why? The answer is a little more nuanced than “never pay” or “always pay.” The decision is a complex scenario of incentives and payoffs. Who stands to gain when ransomware is paid? Who gains when it is not paid?

This talk will use the familiar topic of ransomware to introduce participants to game theory concepts like rational decision-making, zero-sum games, incentives, utility and Nash Equilibrium – all important tools that can help solve security problems. By analyzing ransomware decision-making with a game theory mindset, participants will learn a new set of skills and a new way of incentive-driven thinking.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 85; Score = 3260416.0
<|begin_of_text|>I have a well-referenced book in my library at work, Wayne Eckerson's "Performance Dashboards." Although it is now about five years old – and aging, given how quickly the business intelligence industry is advancing – I continue to use it and discover new ideas.

If you've got his book, check out chapter 6. It classifies three types of performance dashboards: Operational, Tactical, and Strategic.

In a nutshell, Wayne defines Operational Dashboards as being focused on exception alerting, based on real-time or transactional data. It's up to the user or a script to then act upon this opportunity or issue. OK, I'm with him so far.

Tactical Dashboards display data that is not quite as real-time as operational dashboards, and are generally not evaluated against absolute conditions. Contextual information, and the ability to explore the data, tends to guide users to the decision process.

Strategic Dashboards, according to Wayne, track performance metrics against high-level objectives. As a result, these dashboards tend to summarize performance over the past month, quarter, or year. Strategic objectives are usually also the result of many underlying metrics, and require social analysis to digest properly.

At Klipfolio, we tend to use the terms operational and tactical interchangeably. If, instead of using the naming conventions, we simply arrange these dashboards on a time continuum - real-time, daily, and monthly - this aligns better with what's actually happening. (For the sake of definition, let's ignore how frequently the data gets refreshed, because after all, each of these dashboards can accept real-time data.)

The business dashboard is likely to be quite volatile, its data changing frequently throughout the day, but it can be fully trusted to present an accurate snapshot of what is happening right now. This is your "speedometer." It's also one of the most easily understood dashboards, applicable to a wide range of employees for various tasks. The logic for this type of a dashboard is simple. But don't mistake simple and easy for less value. Would you want to drive without your speedometer?

The daily dashboard settles down considerably. We're now looking at an aggregated, summarized, or averaged view of data. Because this presentation tends to smooth the outliers, we can start comparing it against historical values, benchmarks, and goals without having a panic attack every time the data refreshes. This would be your "average trip fuel consumption" metric. The daily dashboard allows you to make informed decisions. Because it's
================================================================================
Rank = 86; Score = 3260416.0
<|begin_of_text|>Merkabah

Merkabah, also spelled Merkaba, is the divine light vehicle allegedly used by ascended masters to connect with and reach those in tune with the higher realms. "Mer" means Light. "Ka" means Spirit. "Ba" means Body. Mer-Ka-Ba means the spirit/body surrounded by counter-rotating fields of light, (wheels within wheels), spirals of energy as in DNA, which transports spirit/body from one dimension to another.

Merkabah Mysticism Chariot of the Gods

The word Merkaba or Merkava - Hebrew 'Chariot' or 'to ride an animal, in a chariot' - is used in the Bible (Ezekiel 1:4-26) to refer to the throne-chariot of God, the four-wheeled vehicle driven by four Cherubim, each of which has four wings and four faces (of a man, lion, ox, and eagle). Merkabah/Merkavah Mysticism (or Chariot mysticism) is a school of early Jewish mysticism, c. 100 BCE-1000 BCE, centered on visions such as those found in the Book of Ezekiel chapter 1, or in the hekhalot ("palaces") literature, concerning stories of ascents to the heavenly palaces and the Throne of God. The main corpus of the Merkabah literature was composed in Israel in the period 200-700 CE, although later references to the Chariot tradition can also be found in the literature of the Chassidei Ashkenaz in the Middle Ages. A major text in this tradition is the Maaseh Merkabah (Works of the Chariot). In Ancient Alien Theory the Chariot is a UFO.

Archaeologists describe how Ezekiel's Wheel helped turn African Americans into Christians PhysOrg - November 9, 2016

At the site of a plantation where abolitionist Frederick Douglass once lived archaeologists have uncovered striking evidence of how African and Christian religious beliefs blended and merged in the 19th century. The team dug up an intact set of objects that they interpret as religious symbols - traditional ones from Africa, but mixed with what they believe to be a Biblical image: a representation of Ezekiel's Wheel.

In medieval Judaism, the beginning of the book of Ezekiel was regarded as the most mystical passage in the Bible, and its study was discouraged, except by mature individuals with an extensive grounding in the
================================================================================
Rank = 87; Score = 3260416.0
<|begin_of_text|>Warren Berger believes questions are more important than answers. A best-selling author and self-proclaimed “questionologist,” Warren is on a mission to help all leaders learn how to use questioning to support innovative, resilient and adaptive organizations.

We caught up with Warren recently to ask him about why it’s more important than ever to lean into questioning and curiosity.

Lisa Kay Solomon: Your recent book is called A More Beautiful Question. Can you talk about what a “beautiful question” is and why they are so important to ask?

Warren Berger: I am particularly interested in questions that lead to innovation. So, I studied a lot of those kinds of questions to see what they had in common and arrived at this definition: A beautiful question is an ambitious yet actionable question that can shift the way we think about something and may serve as a catalyst for change.

Each aspect is important. “Ambitious” because we have to ask bold questions to innovate. “Actionable” because big questions we can’t do anything about don’t lead to change. Critically, the question has to cause a mental shift—it makes you step back and say, “Hmmm, that’s interesting. I hadn’t really thought about this question before, and I want to explore it further.”

These kinds of questions are the starting point of innovation and growth, which is why every leader should be asking them and encouraging others to ask them.

LKS: How does questioning help us become better innovators?

WB: Questioning helps us become more comfortable and proactive in dealing with the unknown, which is critical for innovators. Innovators often operate in a realm where the solution to a particular problem is unknown or may even seem impossible.

The innovator’s job is to explore that unknown and arrive at a completely new and original answer. Questions can help the innovator keep moving forward; you start with one question, then that can lead you to a better, deeper question, and so on. You could almost think of questioning as “an app for the unknown.”

When you finally arrive at the answer—the innovation—it may seem the questioning is over. But it isn’t really. Soon, the innovator is wondering how to make their creation even better, more affordable—how they can expand upon it.

For true innovators, the cycle of questioning never ends.

LKS: Are you seeing technology change the type of questions we should be asking?

WB: Technology has thrown everything open to question because everything we thought we knew how to do can now be done differently. The shoe-ret
================================================================================
Rank = 88; Score = 3211264.0
<|begin_of_text|>For any business to grow, it is crucial to keep records accurately. These records are always useful when planning for future operations. They also are used to find out how the company has been performing, especially when it comes to profitability. Therefore, records are not among the things that you can take lightly. If you cannot keep them on your own, it would be good to find the services of a bookkeeping company. This gives you the assurance that as you focus on the other important aspects that can grow your business, your records will be properly kept, and you can access them anytime you want. When looking for such a company, you have to be careful with the choices that you make. Smart organization managers do not just hire any company that they come across. They always look at the following factors before making any choice.

Modern bookkeeping practices

How will your records be stored? The best Bookkeeping Company in Melbourne, VIC has invested in modern record storage techniques. Instead of the old files, they prefer to turn your documents into soft copies that can be stored more securely. This means that they can store them in the cloud, and the chances of losing any of your records will be minimal. In addition to that, they ensure that everything is securely stored so that none of your information is accessed by third parties. Remember that some of the documents may crucial to your operations, and leaking them can be costly.

Experience in bookkeeping

Finding bookkeepers that have a lot of experience is one of the things that can guarantee better services. It is because, over the years, those that have been handling a lot of records are known to offer better services. It seems that they get better with every task that they are assigned because there are unique skills that they acquire. This is the reason they know all the challenges that organization go through when it comes to managing records. They also have perfect solutions to these problems.

Accessibility

You should be thinking of how fast you will get any of your records whenever you need them. If you have an emergency and you have to produce a specific document, you will not have the luxury of having to wait for too long for the company to provide it. For instance, if auditors demanded some proof of transactions, you should have a reliable company that can retrieve it from their systems at once. Unfortunately, this is not what you will find from all the companies out there and therefore, you have to be careful with the bookkeepers that you hire.

If there is no reliable bookkeeper that you know of, you may want
================================================================================
Rank = 89; Score = 3178496.0
<|begin_of_text|>Inkscape just released version 0.91 of their Open Source vector graphics editor, and the new package will soon be available in the stable repositories for Fedora 21. Inkscape 0.91 is the first major release of Inkscape for a few years, and it has many bugfixes and new features compared to the previous Inkscape 0.48 release.

For those not familiar with Inkscape, It is a versatile, feature rich vector graphics editor that can be used for a wide range of tasks, including UI mockups, icons, logos, digital illustration:

Inkscape uses the Scalable Vector Graphics (SVG) format as the primary source filetype, which is getting more and more popular as a format for vector graphics on the web. Inkscape can also export to a wide range of different formats, including PNG and PDF.

Inkscape 0.91 is a huge improvement over the previous 0.48 release, with the inkscape developers fixing hundreds of bugs, introducing a wide range of new tools and features, and also making Inkscape perform much, much better.

Stability and Performance

The Inkscape developers did a lot of work in this release to improve the performance and stability of Inkscape. I have been using development versions of Inkscape 0.91 for over a year now, and noticed a huge reduction in the number of times Inkscape stalled or crashed during use.

Additionally, Inkscape 0.91 includes a new renderer based on Cairo, which is noticeably faster at rendering complex drawings with lots of SVG filters like blurs, and also caches parts of complex drawings to improve rendering times when editing.

Inkscape now uses threading when rendering filter effects, so users with newer machines with multiple processing cores will notice a difference when rendering complex images. Inkscape also now uses up to 25% less memory on some complex drawings.

New Features

Inkscape 0.91 delivers a wide range of features that improve and simplify drawing and illustrating in Inkscape.

New measurement tool

0.91 introduces a new tool that allows users to measure the distances and angles between objects in an inkscape drawing by drawing a “ruler” over the objects. Using this tool is super simple, just draw a live over your objects, and the distances and angles will live-update on canvas as you move your mouse

Font and Text Improvements

The inkscape developers did a lot of work in the 0.
================================================================================
Rank = 90; Score = 3162112.0
<|begin_of_text|>This article is Part VI in a series looking at data science and machine learning by walking through a Kaggle competition. If you have not done so already, you are strongly encouraged to go back and read the earlier parts – (Part I, Part II, Part III, Part IV and Part V).

Continuing on the walkthrough, in this part we build the model that will predict the first booking destination country for each user based on the dataset created in the earlier parts.

Choosing an Algorithm

The first step to building a model is to decide what type of algorithm to use. Below we look at some of the options.

Decision Tree

Arguably the most well known algorithm, and one of the simplest conceptually. The decision tree works in a similar manner to the decision tree that you might create when trying to understand which decision to make based on a range of variables.

The goal of the decision tree algorithm used for classification problems (like the one we are looking at) is to create one of these decision trees to classify records into a set number of categories. To do this, it starts with all the records in the training dataset and looks through all the features until it finds the one that allows it to most ‘cleanly’ split the records according to their categories. For example, if you are using daily weather data to try and determine whether it will rain the following day (i.e. there are two categories, ‘it does rain’ and ‘it does not rain’), the algorithm will look for a feature that best splits the records (in this case representing days) into those two categories. When it finds that feature, and the value to split on, it creates one point (‘decision node’) on the decision tree. It then takes each subpopulation and does the same thing again, building up a tree until either all the records are correctly classified, or the number in each subpopulation becomes too small to split. Below is an example decision tree using the described weather data to predict if it will rain tomorrow or not (thanks to Graham Williams’ excellent Rattle package for R):

The way to interpret the above tree is to start at the top. The first criteria the algorithm splits on is the humidity at 3pm. Starting with 100% of the records, if the the humidity at 3pm is less than 71, as it is the case for 93% of the records, we move to the left and find the next decision node. If the humidity at 3pm is greater than or equal to
================================================================================
Rank = 91; Score = 3145728.0
<|begin_of_text|>Russian president Vladimir Putin has joined the war of words concerning the international race to develop artificial intelligence. Speaking to students last Friday, Putin predicted that whichever country leads the way in AI research will come to dominate global affairs.

“Artificial intelligence is the future, not only for Russia, but for all humankind,” said Putin, reports RT. “It comes with colossal opportunities, but also threats that are difficult to predict. Whoever becomes the leader in this sphere will become the ruler of the world.”

“It comes with colossal opportunities, but also threats.”

The development of artificial intelligence has increasingly become a national security concern in recent years. It is China and the US (not Russia) which are seen as the two frontrunners, with China recently announcing its ambition to become the global leader in AI research by 2030. Many analysts warn that America is in danger of falling behind, especially as the Trump administration prepares to cut funding for basic science and technology research.

Although it’s thought that artificial intelligence will help boost countries’ economies in a number of areas, from heavy industry to medical research, AI technology will also be useful in warfare. Artificial intelligence can be used to develop cyber weapons, and control autonomous tools like drone swarms — fleets of low-cost quadcopters with a shared ‘brain’ that can be used for surveillance as well as attacking opponents.

Both China and the US are currently researching this technology, and in his speech on Friday, Putin predicted that future wars would be fought by countries using drones. “When one party's drones are destroyed by drones of another, it will have no other choice but to surrender,” said the Russian president, according to the Associated Press.

Recently, Elon Musk and 116 other technology leaders sent a petition to the United Nations calling for new regulations on how such AI weapons are developed. The group stated that the introduction of autonomous technology would be tantamount to a “third revolution in warfare,” following the development of gunpowder and nuclear weapons.

An AI arms race doesn’t necessarily have to be a winner-takes-all scenario, though. Putin noted that Russia did not want to see any one country “monopolize” the field, and said instead: “If we become leaders in this area, we will share this know-how with entire world, the same way we share our nuclear technologies today.”

Update September 4th, 5:40AM ET: Elon Musk offers his cheery perspective on Twitter:<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 92; Score = 3112960.0
<|begin_of_text|>“Stereotypes” have a bad name, and everybody hates stereotypes. But what exactly is a?

What people call “stereotypes” are what scientists call “empirical generalizations,” and they are the foundation of scientific theory. That’s what scientists do; they make generalizations. Many stereotypes are empirical generalizations with a statistical basis and thus on average tend to be true. If they are not true, they wouldn’t be stereotypes. The only problem with stereotypes and empirical generalizations is that they are not always true for all individual cases. They are generalizations, not invariant laws. There are always individual exceptions to stereotypes and empirical generalizations. The danger lies in applying the empirical generalizations to individual cases, which may or may not be exceptions. But these individual exceptions do not invalidate the generalizations.

An observation, if true, becomes an empirical generalization until someone objects to it, and then it becomes a stereotype. For example, the statement “Men are taller than women” is an empirical generalization. It is in general true, but there are individual exceptions. There are many men who are shorter than the average woman, and there are many women who are taller than the average man, but these exceptions do not make the generalization untrue. Men on average are taller than women in every human society (and, by the way, there are evolutionary psychological explanations for this phenomenon, known as the sexual dimorphism in size, but that’s perhaps for a future post). Everybody knows this, but nobody calls it a stereotype because it is not unkind to anybody. Men in general like being taller than women, and women in general like being shorter than men.

However, as soon as one turns this around and makes a slightly different, yet equally true, observation that “Women are fatter than men,” it becomes a stereotype because nobody, least of all women, wants to be considered fat. But it is true nonetheless; women have a higher percentage of body fat than men throughout the life course (and there are evolutionary reasons for this as well). Once again, there are numerous individual exceptions, but the generalization still holds true at the population level.

Stereotypes and empirical generalizations are neither good nor bad, desirable nor undesirable, nor immoral. They just are. Stereotypes do not tell us how to behave or treat other people (or groups of people). Stereotypes are observations about the empirical world, not behavioral prescriptions. One may not infer how to treat people from empirical observations about them. Stereotypes
================================================================================
Rank = 93; Score = 3096576.0
<|begin_of_text|>By Tanja Babic, PhD

In the modern world, success of corporations is often driven by their employees and teams. Understanding how human behaviors affect the workplace is the main objective of industrial and organizational psychology, also known as I/O psychology. I/O psychologists study factors that promote motivation, team work and productivity, as well as management practices that improve employee’s performance. There are many industrial and organizational psychologists working towards improving working conditions, however, the following 30 people were chosen based on the following criteria:

1. Publications: Majority of the individuals on this list have outstanding publication records. These publications include articles in scientific journals, books and book chapters.

2. Impact on industrial and organizational practices: Although academic research is an important aspect of psychology, its influence can only be assessed once ideas are put into practice and applied in a workplace. Men and women on this list have made a significant impact on modern practices and policies in a workplace.

3. Influence on future research directions: While many researchers have published important papers in the field of industrial and organizational psychology, priority was given to those who have made a substantial change in the way in which certain theories are viewed, or those who have led the way into novel research areas.

4. Awards and recognitions: Most psychologists on this list have been recognized for their achievements by various international professional societies, foundations and even Queen Elizabeth II

1. Stanley Silverman

Dr. Silverman is the dean of the Summit College and University College at the University of Akron. His area of expertise is the effect that arrogant bosses have on a workplace. He devised a new measure of arrogance called “Workplace Arrogance Scale,” which is used to determine whether managers possess arrogant tendencies.

2. Anita Woolley

Just like individuals, teams possess a certain level of intelligence. This concept of “collective intelligence” is the focus of Dr. Woolley’s research. She is an assistant professor of organizational behavior and theory at the Tepper School of Business at Carnegie Mellon University. One of the most interesting findings of her recent research is that team intelligence is positively correlated with the number of women on the team. This study was published in the prestigious journal Science in 2010.

3. David Rock

The Neuroleadership Institute was founded in order to bring together neuroscience and leadership experts and to enhance the science of leadership. The institute’s founder, Dr. David Rock, is the author of two business best-sellers and is a blogger for the Harvard Business Review, Fortune Magazine, Psychology Today and the Huffington Post.


================================================================================
Rank = 94; Score = 3080192.0
<|begin_of_text|>Ajax a short form of Asynchronous JavaScript and XML is a set of techniques used by many modern and popularÂ web sites. This technique allows portions of a Web page to be updated without refreshing the entire page, a significant advance in the development of websites and web-based applications. For example, a site might allow you to rate aÂ product or itemÂ by clicking on a star image. With Ajax, your vote could be registered without having to load the entire page again. In this post, we are listing down 15 Beautiful Web Designs Empowered With Ajax Techniques for your inspiration. If you like this post so please spread the word as much as possible via twitter.

You are welcome if you want to share moreÂ web designs empowered with AjaxÂ techniques that we have missed here and you think our readers/viewers may like. Do you want to be the first one to know the latest happenings at SmashingApps.com just subscribe to our rss feed and you can follow us on twitter as well.

I will appreciate if you can spread the word via Digg, Stumbleupon and other social media websites, Thank you.

BBC

MSNBC

White House

Ajax Whois

iGoogle

NetvibesÂ

Pageflakes

Protopage

My Live

My Yahoo

eskobo

Symbaloo

Pingle

Inbox

Shelfari<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 95; Score = 3047424.0
<|begin_of_text|>ABSTRACT: While much has been written about how the Federal Reserve benefits certain private parties and how it generates money out of thin air, this paper delves deeper into the operation of the Federal Reserve as an instrument of war, and its historical role as a primary enabler of armed international conflict. A comparison is done with an analogous monetary institution, the Bank of Canada, to show that the Federal Reserve is being operated differently, with war being its chief historical focus.

The above image is licensed under a Creative Commons License. Feel free to use it. You can get a high quality image for printing here.

This paper resulted from attempts to derive the logic of mechanisms that govern money supply, with focus on the United States. I will also assess whether or not instruments of money supply have stayed true to the logic behind their existence. And if not, I will try to derive the rationale behind their present operation.

An Introduction to Juris Naturalis, an organic view of economics

My perspective may be categorised as that of the Austrian School of Economics, Objectivist and Libertarian. But I would like to emphasize the role of Richard Maybury’s naturalist philosophical viewpoints in shaping my perspective. Maybury attempts to derive human laws from Judeo-Christian moral beliefs. While this is a broad subject, Maybury has distilled the essence of these moral beliefs into two laws[1] that are very relevant to inter-human relations:

1. Do all you have agreed to do. 2. Do not encroach on other persons or their property.

Maybury regards these laws with the same certainty as scientific principles, a concept he refers to as juris naturalis. Maybury demonstrates that adherence to or deviation from these laws results in predictable economic outcomes.[2]

These laws are more relevant to understanding money supply than economics, mainly on the basis of what came first. Maybury demonstrates that these laws were taken for granted by the founding fathers of America, when they were drafting laws that lead to the creation of the largest “free market.” On the other hand, modern day economics is a relatively new discipline that attempts to identify patterns using a rigid but limited framework. To quote Alfred Marshall in The Principles of Economics:

The forces to be dealt with are…so numerous, that it is best to take a few at a time…Thus we begin by isolating the primary relations of supply and demand.[3]

Why Establishment Economics Fails

When Alfred Marshall published his conclusions in 1890, supply and demand diagrams appeared only in a footnote,[4] despite his being an
================================================================================
Rank = 96; Score = 3047424.0
<|begin_of_text|>Modern computer chips handle data at the mind-blowing rate of some 10^13 bits per second. Neurons, by comparison, fire at a rate of around 100 times per second or so. And yet the brain outperforms the best computers in numerous tasks.

One reason for this is way computations take place. In computers, calculations occur in strict pipelines, one at a time.

In the brain, however, many calculations take place at once. Each neuron communicates with up to 1000 other neurons at any one time. And since the brain consists of billions neurons, the potential for parallel calculating is clearly huge.

Computer scientists are well aware of this difference and have tried in many ways to mimic the brain’s massively parallel capabilities. But success has been hard to come by.

Today, Anirban Bandyopadhyay at National Institute for Materials Science in Tsukuba, Japan, unveil a promising new approach. At the heart of their experiment is a ring-like molecule called 2,3-dichloro-5,6-dicyano-p-benzoquinone, or DDQ.

This has an unusual property: it can exist in four different conducting states, depending on the location of trapped electrons around the ring. What’s more, it’s possible to switch the molecule from one to state to another by zapping it with voltages of various different strengths using the tip of a scanning tunnelling microscope. It’s even possible to bias the possible states that can form by placing the molecule in an electric field

Place two DDQ molecules next to each other and it’s possible to make them connect. In fact, a single DDQ molecule can connect with between 2 and 6 neighbours, depending on its conducting state and theirs. When one molecule changes its state, the change in configuration ripples from one molecule to the next, forming and reforming circuits as it travels.

Given all this, it’s not hard to imagine how a layer of DDQ molecules can act like a cellular automaton, with each molecule as a cell in the automaton. Roughly speaking, the rules for flipping cells from one state to another are set by the bias on the molecules and the starting state is programmed by the scanning tunnelling microscope.

And that’s exactly what these guys have done. They’ve laid down 300 DDQ molecules on a gold substrate, setting them up as a cellular automaton. More impressive still, they’ve then initialised the system so that it “calculates” the way
================================================================================
Rank = 97; Score = 3031040.0
<|begin_of_text|>In the 28 years since Super Mario Bros. was released it's obviously been comprehensiely beaten, thoroughly, many thousands of times in that time by players around the world.

But have you ever made the game beat itself?

Advertisement

That's what computer scientist Tom Murphy has done. At SigBovik 2013 he presented a program that "solves" how to play Super Mario Bros., or any other NES game, like it's just another kind of mathematical problem. And for those who know that SigBovik is an annual computer science conference dedicated to spoof research, hosted on 1 April every year, Murphy stresses that this is "100 percent real".

He outlines his method in a paper,"The First Level of Super Mario Bros. is Easy with Lexicographic Orderings and Time Travel... after that it gets a little tricky", but he also presented the results in the video you can see with this story.

Read next Super Smash Bros. Ultimate review: the best Smash ever Super Smash Bros. Ultimate review: the best Smash ever

Lexicographic ordering is a pretty simple mathematical technique used to determine the best order a set of values should come in.

It's most commonly used in libraries or dictionaries for arranging books and words, for instance, with the alphabet determining the order of the letters.

Advertisement

The NES puts out 60 frames of 2048 bytes per second, and each of these was fed into LearnFun. Everything in the NES's memory -- the buttons being pressed, the number of lives left, the score, the locations of enemies, Mario's position as coordinates, and so on -- is taken in by the LearnFun algorithm.

PlayFun then plays the game, and uses the knowledge from LearnFun to try and increase the values it knows it has to increase, which mainly means Mario's score and how far scrolled the right Mario is in the level. "It's trying to find the sequence of inputs to make those values go up in the RAM," Murphy explains in the video.

The results are impressive. After some tweaking, Mario plays the first level just like a real person, jumping on enemies like Goombas and hitting boxes for coins. The program even learns how to take advantage of bugs and glitches, like timing jumps so that Mario begins falling again at the exact time that he makes contact with a Goomba. Mario's invincible when he's falling, so the touch kills the Goomba, not Mario, and it gives him a further jump boost.

Read next These
================================================================================
Rank = 98; Score = 3031040.0
<|begin_of_text|>The Great Math Mystery

PBS Airdate: April 15, 2015

NARRATOR: We live in an age of astonishing advances: engineers can land a car-sized rover on Mars, physicists probe the essence of all matter, while we communicate wirelessly on a vast worldwide network. But underlying all of these modern wonders is something deep and mysteriously powerful.

It's been called the language of the universe, and perhaps it's civilization's greatest achievement. Its name? Mathematics.

But where does math come from? And why, in science, does it work so well?

MARIO LIVIO (Author, Is God a Mathematician?): Albert Einstein wondered, “How is it possible that mathematics does so well in explaining the universe as we see it?”

NARRATOR: Is mathematics even human?

ELIZABETH BRANNON (Duke University, Center for Cognitive Neuroscience): There doesn't really seem to be an upper limit to the numerical abilities of animals.

NARRATOR: And is it the key to the cosmos?

MAX TEGMARK (Massachusetts Institute of Technology/Author, Our Mathematical Universe): Our physical world doesn't just have some mathematical properties; it has only mathematical properties.

NARRATOR: The Great Math Mystery, next, on NOVA!

Human beings have always looked at nature and searched for patterns. Eons ago we gazed at the stars and discovered patterns we call constellations, even coming to believe they might control our destiny.

We've watched the days turn to night and back to day, and seasons, as they come and go, and called that pattern “time.” We see symmetrical patterns in the human body and the tiger's stripes, and build those patterns into what we create, from art to our cities.

But what do patterns tell us? Why should the spiral shape of the nautilus shell be so similar to the spiral of a galaxy or the spiral found in a sliced open head of cabbage?

When scientists seek to understand the patterns of our world, they often turn to a powerful tool: mathematics. They quantify their observations and use mathematical techniques to examine them, hoping to discover the underlying causes of nature's rhythms and regularities.

And it's worked, revealing the secrets behind the elliptical orbits of the planets to the electromagnetic waves that connect our cell phones. Mathematics has even guided the way, leading us right down to the sub-atomic building blocks of matter, which raises the question, “Why does it work at all? Is there an inherent mathematical nature to reality
================================================================================
Rank = 99; Score = 3031040.0
<|begin_of_text|>-Fixed a bug that caused small dogs to start barking and never stop.

-Small dogs are no longer supported, and will self-terminate within two months.

-St. Bernard size increased 100%.

-St. Bernard's cask will no longer contain brandy. Now contains one of four dipping sauces (ranch dressing, sweet and sour, honey mustard and barbecue).

-Improved saliva production in certain breeds by as much as 300%.

-Removed a rare glitch that would cause a dog's head to rotate 360 degrees when looking inquisitively at operator.

-Dogs now regard vacuum cleaners as friends and will attempt to operate them.

-Fixed a bug that caused dogs to incorrectly regard fecal matter as food.

-Fixed an error that caused social interaction between dogs to occur on the wrong end.

-Fixed an issue where some breeds would grow curly hair, making them appear effeminate and prissy.

-Improved dog pathfinding, allowing them to better navigate to operators located at higher altitudes or behind complicated obstacles.

-Dramatically reduced instances of autoerotic behavior in public.

-Addressed a logic error that caused some dogs to hysterically chomp at bees.

-Fixed a rare glitch where a dog would regard its own tail as an enemy.

-Removed an exploit that would allow operators to duplicate their dogs endlessly.

-Placing a small ball or toy in a dog's open mouth while it is yawning will no longer cause it to shut down.

-Reduced bloodhound "droopiness" by 25%.

-Fixed issues of dogs turning hostile when spoken to in a funny or scary voice.

-Introduced fixes for three common benign situations that would cause a dog to become sexually aroused.

-Fixed error of dog incompatibility with chocolate cake. Should greatly improve experience of being dog.

-Fixed a bug that would cause floppy ears to flip over the wrong way, requiring an operator to reset them to their default position.

-Dogs should no longer drag butts across carpeting in presence of operators.

-Fixed rendering bug that caused deformed geometry on pug faces.

-Investigated issue of pit bulls entering rings of screaming men, biting each other to death; confirmed cause as operator error.

-Fixed vocalization issue in huskies which would cause them to utter "I love you" when trying to say "stop tormenting me."

-Added a routine to flush loyalty cache of dogs upon death of operators, preventing documented issues of dogs waiting at train stations for 15 years.

-Fixed a